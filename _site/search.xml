<?xml version="1.0" encoding="utf-8"?>
<search>
  
    <entry>
      <title><![CDATA[第四课 无模型的预测]]></title>
      <url>/2019/03/11/%E7%AC%AC%E5%9B%9B%E8%AF%BE-%E6%97%A0%E6%A8%A1%E5%9E%8B%E7%9A%84%E9%A2%84%E6%B5%8B/</url>
      <content type="text"><![CDATA[---layout: posttitle: "第四课 无模型的预测"date: 2019-03-11categories: ReinforceLearningtags: ["ReinforceLearning", "强化学习"]---									第四课 无模型的预测	这一课帅小哥主要讲的内容是预测的部分，在第五课会加入控制的部分。其中预测的部分主要是两个相似的算法，一个为 Monte-Carlo（MC），另一个为 Temporal-Difference（TD）。两者的区别主要在于，MC 为需要在出现终止状态后，才能得到 Reward，而 TD 则是实时的。Monte-CarloMonte-Carlo强化学习指的是，在不清楚 MDP 状态转移和奖励方案的情况下，直接通过经历完整的 episode 来学习状态的 value。一般而言，一个状态的 value 为，其在多个 episode 下的 value 的平均值。注：episode 指的是不定的起始状态开始，直到某一特定的终止状态结束。其评价每个状态的 value 的主要算法如下两种：	首次访问 Monte-Carlo 策略评估		首先，固定一个策略 $\pi$ ，之后使用这个策略进行多个完整的 episode。对于每个 episode，当且仅当状态第一次出现时才列入计算：N(s) = N(s) + 1 # 状态计数 +1S(s) = S(s) + G_t # 总收获更新V(s) = V(s) / N(s) # 状态的 value 更新		注：当 N 很大时， $V(s)$ 就是我们所求估计	每次访问 Monte-Carlo 策略评估		同首次访问 MC 算法一致，但是这个算法中，每次出现在状态转移链中的状态都会更新。Temporal-Difference首先，在开始 TD 算法前，小哥补充了一个 baby math，即一个求平均值的操作其实是可以写成一种迭代的样式如下：$$\begin{split}\mu_k &= \frac{1}{k} \sum_{j=1}^k x_j  \\&= \mu_{k-1} + \frac{1}{k}(x_k-\mu_{k-1})\end{split}$$再抽象点就得到了以下公式：$$\begin{split}V(S_t) &= V(S_t) + \frac{1}{N(S_t)}(G_t - V(S_t)) \\& = V(S_t) + \alpha(G_t - V(S_t))\end{split}  $$而 TD 算法就是从这里开始的。TD 算法和 MC 算法不同的地方在于，其不用完成整个 episode 才得到状态的 value，即它可以学到一个不完整的 episode，通过自身的引导（bootstrapping）来猜测结果。其公式如下：$$V(S_T) = V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1}) - V(S_t))$$注：其中 $R_{t+1}$ 为离开该状态的时候的即刻奖励， $S_{t+1}$ 为下一状态的预估状态价值。TD 与 MC 的对比	TD 可以在知道最终结果前就可以学习，MC 必须在 episode 结束后才知道。		注：小哥举了个例子，说你不能在被车撞死后再重来。	MC 是基于某一个策略的无偏估计，而 TD 则是有偏估计。（毕竟 TD 瞎猜了）	MC 没有 bias，但是有较高的变异性（Variance），对初值不敏感，而 TD 有 bias，低变异性，但效率高。注：MDP、TD 和 MC 都是计算状态 value 的方案参考资料	Joey 老师的教程 04 	David Silver 的强化学习课系列]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第二课 马尔可夫决策过程 MDP]]></title>
      <url>/reinforcelearning/2019/03/11/%E7%AC%AC%E4%BA%8C%E8%AF%BE-%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B-MDP/</url>
      <content type="text"><![CDATA[									第二课 马尔可夫决策过程 MDP	Markov Decision Process 是强化学习的核心，帅气的 David 说所有的强化学习问题都可以转化为 MDP，即就像 RBM 是深度学习的发源地一样，MDP 是整个强化学习的基础。而和名字一样，我们需要首先理解 Markov 和 Decision（Reward），接下来会从 Markov 过程到 Markov 过程加上 Reward 之后的马尔可夫奖励过程，最后引入 Bellman 方程，通过解 Bellman 方程的方式深入了解到底何为决策。马尔可夫过程 Markov Process谈到 Markov 过程，就需要提到 Markov 的一个前提条件，即 Markov Process 认为下一个 t+1 时刻的状态，仅同当前 t 时刻的状态有关。David 帅大大，对此的解释是认为其当前的状态其实已经包含了之前的状态的信息了。所以就可以用下面的公式表达状态转移概率了：$$P_{ss'} = P[S_{t+1} = s' | S_t = s]$$接下来我们联想当年求最短路径的算法 Floyd 的那个矩阵，我们只需要用矩阵 $S_t$ 表示当前的状态，再使用矩阵 $P$ 作为代表不同状态之间的转移概率，两者相乘就能得到下一时刻的状态矩阵 $S_{t+1}$ 。而 Markov Process 就是这个当前状态 $S$ 和概率转移矩阵 $P$ 不停相乘的过程，简单的计为 $$ 。$$P = \left[ \begin{array}{cc}        P_{11} & P_{12} & P_{13} & ... & P_{1n} \\         P_{21} & P_{22} & P_{23} & ... & P_{2n} \\		... & ... & ... & ... & ... \\		P_{n1} & P_{n2} & P_{n3} & ... & P_{nn}         \end{array} \right]$$注：其中由于 $P$ 中每一行代表着行号表示的状态转移到另一个状态的所有情况，所以每一行的概率和一定等于 1 马尔可夫奖励过程 Markov Reward Process上一部分我们讲的 Markov Process 它还是没有奖励的，也就是说没有一个评判的依据，当我们把奖励引入的话就成了 MRP 了。在引入的过程中，我们用 $\gamma$ 作为衰减系数，表达对未来的收益的看重程度。这样我们对 MRP 就记为 $$ 。接下来，我们进入细节的部分。为了更方便说明价值函数 $v(s)$，我们引入一个叫做收益 Return 的符号，其代表了当前状态，结合一定的未来情况能拿到的奖励，记做 $G_t$。$$G_t = R_{t+1} + \gamma R_{t+2} + ... = \sum_{k=0}^{\inf}\gamma^k R_{t+k+1}$$现在就可以讲述价值函数是什么了，其衡量了当前的状态的长期价值，即马尔科夫奖励过程中，从该状态开始的马尔科夫的收益的期望：$$v(s) = E[G_t|S_t = s]$$注：聪明的朋友已经发现了，这里的 $v(s)$ 的值和 $G_t$ 的定义很相似，之后在 Bellman 方程中就会体现。 Bellman 方程现在们有了上面的价值函数的表达式了，但是似乎还是看不出什么好玩的地方，接下来我们把它展开，进行推导，会得到很有趣的结果：\begin{split}v(s) &= E[G_t|S_t = s] \\&= E[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + ... | S_t = s] \\&= E[R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + ...) | S_t =s] \\&= E[R_{t+1} + \gamma G_{t+1} | S_t = s] \\&= E[R_{t+1} + \gamma v(S_{t+1}) | S_t = s] \\&= R_s + \gamma \sum_{s' \in \mathcal{S}} \mathcal{p}_{ss'} v(s') \end{split}接下来换成矩阵的形式为：$$v = \mathcal{R} + \gamma \mathcal{P}v  $$把矩阵写开为：$$\left[ \begin{array}{cc}        v(1) \\         ... \\		v(n)        \end{array} \right] =\left[ \begin{array}{cc}        R_1 \\         ... \\		R_n        \end{array} \right] +\gamma\left[ \begin{array}{cc}        P_{11}& ... & P_{1n} \\ 		... & ... & ... \\		P_{n1} & ... & P_{nn}         \end{array} \right]\left[ \begin{array}{cc}        v(1) \\         ... \\		v(n)        \end{array} \right]$$这样我们就可以直接求解：$$v = (1 - \gamma \mathcal{P})^{-1} \mathcal{R}  $$但是遗憾的是，计算复杂度为 $O(n^3)$ ，该算法的时间复杂度很高。对此，大规模MRP的求解通常使用迭代法。常用的迭代方法有：	动态规划 Dynamic Programming	蒙特卡洛评估 Monte-Carlo evaluation	时序差分学习 Temporal-Difference马尔可夫决策过程 Markov Decision Process在 MRP 中我们还没有引入 Action，而我们 MDP 的目的则是找出最佳的 Action，于是 MDP 便引入了有限行为集合 $A$ ，记做 $$ 。接下来，我们就引入了第一课中提到的策略 $\pi$ , 策略是概率的集合或分布，代表着当前状态 $s$ 采取行为 $s$ 的概率，用 $\pi(a|s)$ 表示。这样从状态 $s$ 到下一个状态 $s'$ 就需要考虑到动作的因素，于是在策略 $\pi$ 下，又 $s$ 转移到 $s'$ 的转移概率为：$$\mathcal{P}_{s,s'}^{\pi} = \sum_{a\in\mathcal{A}} \pi(a|s)\mathcal{R}_{ss'}^a  $$同时奖励函数变为：$$\mathcal{R}_s^{\pi} = \sum_{a\in A} \pi(a|s)\mathcal{R}_s^a  $$这样基于策略 $\pi$ 的价值函数为：$$\begin{split}v_{\pi}(s) &= E_{\pi}[G_t | S_t = s] \\ &= E_{\pi}[R_{t+1} + \gamma v_{\pi}(S_{t+1}) | S_t = s]\end{split}$$行为价值函数 $q_{\pi}(s, a)$ 则为：$$\begin{split}q_{\pi}(s, a)&=E_{\pi}[G_t | S_t = s， A_t = a] \\&=E_{\pi}[R_{t+1} + \gamma q_{\pi}(S_{t+1}, A_{t+1}) | S_t = s, A_t = a]\end{split}$$接下来我们在看 Bellman 方程，接下来我们做的是看如何把两个价值函数连在一起，他们之前的关系如下：接下来把他们合起来，如下图所示，则公式为：$$v_{\pi}(s) = \sum_{a \in A} \pi(a|s) (\mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} v_{\pi}(s'))  $$$$q_{\pi}(s,a)=\mathcal{R}_s^{a} + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a (\sum_{a' \in \mathcal{A}} \pi(a'|s')q_{\pi}(s',a'))$$决策公式写完了，我们接下来就是要进入决策过程，分为两个一个是价值函数最优。简单的来说，就是选一个策略让 $v(s)$ 和 $q(s,a)$ 最大，公式如下：$$v_* = max_{\pi} v_{\pi}(s)$$$$q_* = max_{\pi} q_{\pi}(s,a)$$这个解是否存在呢，David 帅哥说存在的，对于任何MDP，下面几点成立：	存在一个最优策略，比任何其他策略更好或至少相等	所有的最优策略有相同的最优价值函数	所有的最优策略具有相同的行为价值函数以上定理奠定了我们理论上能找到最优策略。那么既然存在我们应该怎么找呢，当然是用 Bellman 最优方程了，遗憾的是，这个同深度学习一样，函数是非线性的所以不能直接求解，需要通过迭代的方式。Bellman 最优方程如下：$$v_*(s) = max_a \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a v_*(s')  $$$$q_*(s,a) = \mathcal{R}_s^a + \gamma \sum_{s' \in \mathcal{S}} \mathcal{P}_{ss'}^a max_{a'}q_*(s', a') $$注：解的迭代方法有价值迭代、策略迭代、Q学习、Sarsa等参考链接	Joey 老师的教程 02	David Silver 的强化学习课系列]]></content>
      <categories>
        
          <category> ReinforceLearning </category>
        
      </categories>
      <tags>
        
          <tag> ReinforceLearning </tag>
        
          <tag> 强化学习 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第三课 动态规划寻找最优策略]]></title>
      <url>/2019/03/11/%E7%AC%AC%E4%B8%89%E8%AF%BE-%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E5%AF%BB%E6%89%BE%E6%9C%80%E4%BC%98%E7%AD%96%E7%95%A5/</url>
      <content type="text"><![CDATA[---layout: posttitle: "第三课 动态规划寻找最优策略P"date: 2019-03-11categories: ReinforceLearningtags: ["ReinforceLearning", "强化学习"]---									第三课 动态规划寻找最优策略	这节课是接着第二节课的，个人对这节课的总结只有一句话对 Bellman 方程多次迭代能得到最优策略和最大价值。课程开始的时候，David 大佬答大体讲了下什么是动态规划，这个想必大家都很熟悉了，就不赘述了。我们仔细想 Bellman 方程其实是完美的复合了动态规划的要求的条件的。所以我们就有了以下的内容。Iterative Policy Evaluation简单的来说就是重复迭代上述的过程，最终 $v(s)$ 会收敛到最大值，这样子我们就能评估当前选择的 Policy $\pi$ 好不好了。下图为算法收敛的过程。How to Improve a Policy有了 Policy 的评估之后，直觉上我们就有了下面的算法，就是每个步骤都看看当前策略候选集中的最优策略是什么，并选择最优的策略。由于整个算法最终会收敛到唯一的最优策略和最大 value，所以我们就不停的迭代上述步骤就好啦。注：收敛是因为每次贪婪的选择最优策略一定会导致下一步的结果更好，同时 MDP 保证了其最优策略等价于最优 value。总结可以这么理解，Value Iteration 是为了评估目前可选的 Policy，Policy Iteration 就是根据评估找出当前最好的 Policy。之后重复上述两个步骤就能得到最优 Policy。注：之后其实还讲了异步动态规划、 采样更新、近似动态规划，但是我们实际很少使用这些，所以就不在这里提了。参考文献	Joey 老师的教程 03	David Silver 的强化学习课系列]]></content>
      <categories>
        
      </categories>
      <tags>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[第一课 强化学习简介]]></title>
      <url>/reinforcelearning/2019/03/11/%E7%AC%AC%E4%B8%80%E8%AF%BE_%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%80/</url>
      <content type="text"><![CDATA[									第一课 强化学习简介	强化学习是什么强化学习在不同领域有不同的表现形式：神经科学、心理学、计算机科学、工程领域、数学、经济学等有不同的称呼。而强化学习是单独的一个机器学习的分支，他不属于监督学习，也不属于无监督学习。他的特点如下：	没有监督数据、只有奖励信号	奖励信号不一定是实时的，很可能会延后很多	时间（序列）是一个关键因素	当前的行为会影响后续的数据注：之前的深度学习，机器学习这些是基于数据的，而强化学习则是基于模拟实验的。强化学习问题的组成奖励 Reward $R_t$ 是一个信号的反馈，是一个标量，而个体的工作就是最大化奖励总和（长期收益最大）。小哥说这个奖励用标量就已经足够了。 Agent &amp; Environment首先要注意的是智能体是不能直接得到环境的信息的。只能通过观察得到 $t$ 时间的观察评估 $O_t$ ，之后根据观察的结果选择行为 $A_t$，最终环境给智能体一个奖励信号 $R_{t+1}$。而环境则可以接受智能体的动作，并以此更新环境。同时也能反馈个诶智能体奖励信号 $R_t$ 状态和历史历史是一个观测、行为、奖励的序列，这个序列如果全部记录下来的话太耗资源了。所以希望使用状态来表示已有的信息。这个是通过 Markov 的性质实现的。环境状态是环境的私有呈现，包括环境用来决定下一个观测/奖励的所有数据，通常对个体并不完全可见，也就是个体有时候并不知道环境状态的所有细节。即使有时候环境状态对个体可以是完全可见的，这些信息也可能包含着一些无关信息。而环境是否可观测，则是区分强化学习算法的一种分类方式：	完全可观测环境		这种问题可以看做一个 Markov Decision Process。	部分可观测环境					用个体已知的概率分布来预测			使用 RNN		智能体状态包含智能体可以使用的、决定策略使用的所有信息。一般是一个历史的函数 $S_t^a = f(H_t)$ 信息状态包括历史上所有的有用的信息，个人直观的感觉为是用在在智能体状态的一部分。强化学习智能体主要成分	策略 Policy		通过当前状态决定选择哪个动作，即输入是状态，输出是动作。	价值函数 Value Function		对未来的奖励的预期，用于评价当前状态的好坏程度。	模型 Model		用于对环境的建模强化学习智能体的分类	仅基于价值函数的 Value Based：在这样的个体中，有对状态的价值估计函数，但是没有直接的策略函数，策略函数由价值函数间接得到。	仅直接基于策略的 Policy Based：这样的个体中行为直接由策略函数产生，个体并不维护一个对各状态价值的估计函数。	演员-评判家形式 Actor-Critic：个体既有价值函数、也有策略函数。两者相互结合解决问题。此外，根据个体在解决强化学习问题时是否建立一个对环境动力学的模型，将其分为两大类：	不基于模型的个体: 这类个体并不视图了解环境如何工作，而仅聚焦于价值和/或策略函数。	基于模型的个体：个体尝试建立一个描述环境运作过程的模型，以此来指导价值或策略函数的更新。强化学习的矛盾探索 Exploration 和利用 Explotiation，简单的来说就是我们已有一个最优策略，如果不改变的话收益是已知的，而探索的结果是不确定的收益可能增加也可能减少。就和我们人类做决策一样，如何平衡这种矛盾也是强化学习中的一个很有趣的问题。参考	官方课件	大佬的详细笔记 （个人的笔记大多是选择里面个人认为重要的部分）]]></content>
      <categories>
        
          <category> ReinforceLearning </category>
        
      </categories>
      <tags>
        
          <tag> ReinforceLearning </tag>
        
          <tag> 强化学习 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[深度学习自动调参之NNI样例分析]]></title>
      <url>/nni/2018/12/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E8%B0%83%E5%8F%82%E4%B9%8BNNI%E6%A0%B7%E4%BE%8B%E5%88%86%E6%9E%90/</url>
      <content type="text"><![CDATA[									深度学习自动调参之NNI样例分析	在之前的博文中介绍了 NNI 与其他自动机器学习工具的比较，NNI 的安装和使用等内容，这篇文章你将看到：	如何修改 NNI 官方的 mnist-annotation 例子的配置文件；	官方例子支持的 Tuner 介绍；	各个 Tuner 的训练结果以及结果分析。一、配置文件将 NNI 项目 clone 到本地，进入到目录 ~/nni/examples/trials/mnist-annotation ，NNI 有两种配置方式，分别为 Annotation 和 Assessor，nni 官方给的例子是用 Annotation 的配置方式（Assessor 可参见官方 experiment 配置参考文档），配置文件 config.yml 默认参数配置如下：authorName: default# authorName 是创建 Experiment 的作者。（你自己的名字 o(*￣▽￣*) ブ）experimentName: example_mnist# experimentName 是 Experiment 的名称。trialConcurrency: 1#    **trialConcurrency** 定义了并行运行的 trails 的数量。#    注意：如果 trialGpuNum 大于空闲的 GPU 数量，Trial 任务会被放入队列，等待分配 GPU 资源。maxExecDuration: 1h# maxExecDuration 定义 Experiment 执行的最长时间。时间单位：{**s**, **m**, **h**, **d**}，分别代表：{*seconds*, *minutes*, *hours*, *days*}。#    注意：maxExecDuration 设置的是 Experiment 执行的时间，不是 Trial 的。 如果 Experiment 达到了设置的最大时间，Experiment 不会停止，但不会再启动新的 Trial 作业。maxTrialNum: 10# maxTrialNum 定义了你此次 Experiment 总共想要 NNI 跑多少 Trial。trainingServicePlatform: local#choice: local, remote, pai# trainingServicePlatform 定义运行 Experiment 的平台# local：在本机的 ubuntu 上运行 Experiment。# remote：将任务提交到远程的 Ubuntu 上，必须用 **machineList** 来指定远程的 SSH 连接信息。# pai：提交任务到微软开源的 OpenPAI 上。# kubeflow 提交任务至 Kubeflow NNI 支持基于 Kubeflow 的 Kubenetes，以及 Azure KubernetesuseAnnotation: true#choice: true, false#定义使用标记来分析代码并生成搜索空间。（官方例子使用的是 Annotation，所以 useAnnotation = true）tuner:  builtinTunerName: TPE  #builtinTunerName 指定了系统 Tuner 的名字，NNI SDK 提供了多种 Tuner，如：{TPE, Random, Anneal, Evolution, BatchTuner, GridSearch}。  #choice: TPE, Random, Anneal, Evolution, BatchTuner  #SMAC (SMAC should be installed through nnictl)  classArgs:   #classArgs** 指定了 Tuner 算法的参数。 如果 builtinTunerName 是{TPE, Random, Anneal, Evolution}，用户需要设置 optimize_mode。    #choice: maximize, minimize    optimize_mode: maximizetrial:  command: python3 mnist.py  codeDir: .  gpuNum: 0二、tuner 对比实验2.1、Random建议场景在每个 Trial 运行时间不长（例如，能够非常快的完成，或者很快的被 Assessor 终止），并有充足计算资源的情况下。 或者需要均匀的探索搜索空间。 随机搜索可作为搜索算法的基准线。参数	optimize_mode (maximize 或 minimize，可选，默认值为 maximize) - 如果为 &#39;maximize&#39;，Tuner 会给出有可能产生较大值的参数组合。 如果为 &#39;minimize&#39;，Tuner 会给出有可能产生较小值的参数组合。使用样例：# config.ymltuner:  builtinTunerName: Random  classArgs:    optimize_mode: maximize训练结果：以下为 Tuner 为 Random，TrialNum 为 30 时的训练结果，从下图右下角可以直观的得出，最大正确率为 98.28%，展开后可看到对应的超参值，在 Trails Detail 能够看到所有 Trails 在不同超参选择上的分布，便于分析。卷积核大小：7×7隐藏层：512学习率：0.0018762964666695628激活函数：ReLU池化层：最大池化batch size：32dropout rate：0.5Random.pngRandom2.png结果分析：正确率低于 30% 的 trails 隐藏层多数为 1024，学习率绝大多数低于 0.001，激活函数多数为 sigmoid。于此同时，正确率高于 90% 的 trails 卷积核大小大部分为 7×7，学习率主要分布在 0.001 以下。根据以上对结果的分析，可以合理猜测，此模型下设置卷积核大小为 7×7，学习率低于 0.001，激活函数选用 relu 或 tanh，就能获得比较理想的正确率。2.2、TPE建议场景TPE 是一种黑盒优化方法，可以使用在各种场景中，通常情况下都能得到较好的结果。 特别是在计算资源有限，只能运行少量 Trial 的情况。 大量的实验表明，TPE 的性能远远优于随机搜索。参数	optimize_mode (maximize 或 minimize，可选，默认值为 maximize) - 如果为 &#39;maximize&#39;，Tuner 会给出有可能产生较大值的参数组合。 如果为 &#39;minimize&#39;，Tuner 会给出有可能产生较小值的参数组合。使用样例：# config.ymltuner:  builtinTunerName: TPE  classArgs:    optimize_mode: maximize训练结果：以下为 tuner 为 TPE，TrialNum 为 30 时的训练结果，从下图右下角可以直观的得出，最大正确率为 98.13%，展开后可看到对应的超参值：卷积核大小：7×7隐藏层：1024学习率：0.0005779853380708741激活函数：ReLU池化层：最大池化batch size：16dropout rate：0.5TPE.pngTPE2.png结果分析：正确率前 50% 的 trails 隐藏层多数为 1024，学习率全部低于 0.001，激活函数多数为 relu 和 tanh，卷积核大小大部分为 7×7 和 5×5。根据以上对结果的分析，可以合理猜测，此模型下设置卷积核大小为 7×7 或 5×5，学习率低于 0.001，激活函数选用 relu 或 tanh，就能获得比较理想的正确率。2.3、Anneal建议场景当每个 Trial 的时间不长，并且有足够的计算资源时使用（与随机搜索基本相同）。 或者搜索空间的变量能从一些先验分布中采样。参数	optimize_mode (maximize 或 minimize，可选，默认值为 maximize) - 如果为 &#39;maximize&#39;，Tuner 会给出有可能产生较大值的参数组合。 如果为 &#39;minimize&#39;，Tuner 会给出有可能产生较小值的参数组合。使用样例：# config.ymltuner:  builtinTunerName: Anneal  classArgs:    optimize_mode: maximize训练结果：以下为 tuner 为 Anneal，TrialNum 为 100 时的训练结果，从下图右下角可以直观的得出，最大正确率为 98.89%，展开后可看到对应的超参值：卷积核大小：7×7隐藏层：512学习率：0.0010559236204399935激活函数：ReLU池化层：最大池化batch size：32dropout rate：0.5Anneal.pngAnneal2.png结果分析：正确率前 20% 的 trails 隐藏层基本分布于 512 和 1024，学习率分布在 0.001 左右，激活函数为 relu，卷积核大小大部分为 5×5。根据以上对结果的分析，可以合理猜测，此模型下设置卷积核大小为 5×5，学习率在 0.001 左右，激活函数选用 relu，隐藏层为 1024 或 512，就能获得比较理想的正确率。2.4、Evolution建议场景此算法对计算资源的需求相对较高。 需要非常大的初始种群，以免落入局部最优中。 如果 Trial 时间很短，或者利用了 Assessor，就非常适合此算法。 如果 Trial 代码支持权重迁移，即每次 Trial 会从上一轮继承已经收敛的权重，建议使用此算法。 这会大大提高训练速度。参数	optimize_mode (maximize 或 minimize，可选，默认值为 maximize) - 如果为 &#39;maximize&#39;，Tuner 会给出有可能产生较大值的参数组合。 如果为 &#39;minimize&#39;，Tuner 会给出有可能产生较小值的参数组合。使用样例：# config.ymltuner:  builtinTunerName: Evolution  classArgs:    optimize_mode: maximize训练结果：以下为 Tuner 为 Evolution，TrialNum 为 30 时的训练结果，从下图右下角可以直观的得出，最大正确率为 98.69%，展开后可看到对应的超参值：卷积核大小：5×5隐藏层：512学习率：0.0008152180302834592激活函数：tanh池化层：最大池化batch size：32dropout rate：0.5Evolution.pngEvolution2.png结果分析：正确率前 20% 的 trails 隐藏层多数分布于 512，学习率分布在 0.001 左右较为集中，激活函数为 tanh 较为集中，卷积核大小大部分为 5×5 或 3×3。根据以上对结果的分析，可以合理猜测，此模型下设置卷积核大小为 5×5，学习率在 0.001 左右，激活函数选用 tanh，隐藏层为 512，就能获得比较理想的正确率。三、总结综合对比不同 Tuner 的实验结果，发现不同的 Tuner 算法得出的超参分布存在一定差异性，如在使用 Anneal 时准确率前 20% 的 trails 采用的激活函数都为 relu，而 Evolution 的实验中，这部分 trails 却是 tanh 居多。需要思考一下神经网络模型相同的情况下，是什么导致的这些差异性。同样，我们在对比中也能发现许多一致性，通过这些一致性能够对我们的模型调参工作以及对深度学习的理解给予一些启示。]]></content>
      <categories>
        
          <category> NNI </category>
        
      </categories>
      <tags>
        
          <tag> NNI </tag>
        
          <tag> AutoML </tag>
        
          <tag> 工具 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[深度学习自动调参之NNI使用体验]]></title>
      <url>/nni/2018/12/16/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%87%AA%E5%8A%A8%E8%B0%83%E5%8F%82%E4%B9%8BNNI%E4%BD%BF%E7%94%A8%E4%BD%93%E9%AA%8C/</url>
      <content type="text"><![CDATA[									深度学习自动调参之NNI使用体验	在机器学习建模时，除了准备数据，最耗时耗力的就是尝试各种超参组合，找到模型最佳效果的过程了。即使是对于有经验的算法工程师和数据科学家，有时候也很难把握其中的规律，只能多次尝试，找到较好的超参组合。而对于初学者来说，要花更多的时间和精力。自动机器学习这两年成为了热门领域，着力解决超参调试过程的挑战，通过超参选择算法和强大的算力来加速超参搜索的过程。NNI (Neural Network Intelligence) 是微软亚洲研究院开源的自动机器学习工具。与当前的各种自动机器学习服务或工具相比，有非常独特的价值。在这篇文章中，你将看到：	什么是自动机器学习	目前的一些自动机器学习工具	关于NNI	NNI的安装及使用过程初体验	总结NNI可以改进的方面一、关于AutoML1.1、AutoML出现原因机器学习的应用需要大量的人工干预，这些人工干预表现在：特征提取、模型选择、参数调节等机器学习的各个方面。AutoML视图将这些与特征、模型、优化、评价有关的重要步骤进行自动化地学习，使得机器学习模型无需人工干预即可被应用。1.2、AutoML问题定义	从机器学习角度讲，AutoML可以看作是一个在给定数据和任务上学习和泛化能力非常强大的系统。但是它强调必须非常容易使用。	从自动化角度讲，AutoML则可以看作是设计一系列高级的控制系统去操作机器学习模型，使得模型可以自动化地学习到合适的参数和配置而无需人工干预。一个通用的AutoML定义如下：AutoML的核心任务：	更好的训练效果	没有人为干预	更低的计算力需求1.3、AutoML问题构成AutoML的主要问题可以由三部分构成：特征工程、模型选择、算法选择。	特征工程		特征工程在机器学习中有着举足轻重的作用。在AutoML中，自动特征工程的目的是自动地发掘并构造相关的特征，使得模型可以有最优的表现。除此之外，还包含一些特定的特征增强方法，例如特征选择、特征降维、特征生成、以及特征编码等。这些步骤目前来说都没有达到自动化的阶段。		上述这些步骤也伴随着一定的参数搜索空间。第一种搜索空间是方法自带的，例如PCA自带降维参数需要调整。第二种是特征生成时会将搜索空间扩大。	模型选择		 模型选择包括两个步骤：选择一个模型，设定它的参数。相应地，AutoML的目的就是自动选择出一个最合适的模型，并且能够设定好它的最优参数。	算法选择		对于算法选择，AutoML 的目的是自动地选择出一个优化算法，以便能够达到效率和精度的平衡。常用的优化方法有 SGD、L-BFGS、GD 等。使用哪个优化算法、对应优化算法的配置，也需要一组搜索空间。最终从全局来看，我们可以将以上三个关键步骤整合起来看，一个完整的 AutoML 过程可以分成这么两类：一类是将以上的三个步骤整合成一个完整的 pipeline；另一类则是 network architecture search，能够自动地学习到最优的网络结构。在学习的过程中，对以上三个问题都进行一些优化。1.4、基本的优化策略一旦搜索空间确定，我们便可以实用优化器 (optimizer) 进行优化。这里，AutoML 主要回答三个问题： 	选择的优化器可以作用在哪个搜索空间上？ 	 它需要什么样的反馈？ 	 为了取得一个好的效果，它需要怎样的配置？简单的优化搜索方式包括 grid search 和 random search。其中 grid search 被广泛使用。从样本中进行优化的方法主要包括启发式搜索、derivative-free 优化、以及强化学习方法。梯度下降法是一种重要的优化策略。1.5、评价策略基本评价策略在设计评价策略时，AutoML主要回答三个问题： 	这种策略能能够快速进行评价吗？ 	这种策略能够提供准确的评价吗？	这种策略需要怎样的反馈？基本的评价策略包括： 	直接评价。直接在目标数据上进行评价。这是被使用最多的策略。 	采样。当数据样本量非常大时，采样一些样本进行评价。	Early Stop。当遇到一些极端情况使得网络表现效果不好时，可以考虑进行 Early Stop 来节省资源。 	参数重用。将之前学习过的参数重复利用在新任务上，以加快训练速度。这在两种任务配置差不多时可用。 	共轭评价。对于一些可量化的配置，可以用共轭评价法进行。高级评价策略除了上述比较基本的评价策略，我们有时候还会使用更高级评价策略，其主要包括两种：Meta-Learning 和 Transfer Learning。	Meta-Learning 法：从先前的学习经验中提炼出基本的参数和结构配置。	Transfer Learning法：从先前的学习经验中提炼出可以重用的一些知识。1.6、应用	使用 Auto-sklearn 进行模型选择。	使用强化学习进行 Neural Architecture Search。	使用 ExploreKit 进行自动特征构建。1.7、展望未来可能的研究方向：	提高 AutoML 的效率。	更明确的问题定义。	发展基本和高级的搜索策略。	找到更适合的应用。二、目前的 AutoML 工具比较2.1、 TPOT简介：TPOT是一个使用genetic programming算法优化机器学习piplines的Python自动机器学习工具。TPOT DemoTPOT 通过智能地探索数千种可能的 piplines 来为数据找到最好的一个，从而自动化机器学习中最乏味的部分。An example Machine Learning pipeline一旦 TPOT 完成了搜索，它就会为用户提供Python代码，以便找到最佳的管道，这样用户就可以从那里修补管道。An example TPOT pipeline输出结果：最佳模型组合及其参数 ( python 文件) 和最佳得分。优劣：TPOT 在数据治理阶段采用了 PCA 主成份分析，在模型选择过程中可以使用组合方法，分析的过程比起其他工具更科学，并能直接生成一个写好参数的python 文件，但输出可参考的结果较少，不利于进一步分析。2.2、Auto_Sklearn简介：Auto-sklearn将机器学习用户从算法选择和超参数调整中解放出来。它利用了最近在贝叶斯优化、元学习和集成构建方面的优势。img主要使用穷举法在有限的时间内逐个尝试最优模型，上图是它的架构体系，看的出来他的工作逻辑是目前的开源框架中最复杂的一款，步骤就不细说了，大体过程应该是与Tpot相似的。输出结果：计算过程以及最终模型的准确率。优劣：穷举法简单粗暴，但也是最靠谱的，如果时间充裕的情况下可以加大预算周期不断让机器尝试最优解，但输出结果太少，基本上对进一步数据分析的帮助不大。2.3、AdvisorAdvisor 框架简介：Advisor 是用于黑盒优化的调参系统。它是基于 Google Vizier 的开源实现，编程接口与 Google Vizier 相同。输出结果：推荐参数与训练模型。优劣：方便与 API、SDK、WEB 和 CLI 一起使用，支持研究和试验抽象化，包括搜索和 Early Stop算法，像 Microsoft NNI 一样的命令行工具。介绍完以上这些开源的自动机器学习工具，下面一节就要隆重介绍 NNI 了，NNI 对比以上的工具有很多吸引人的优势，同时作为一个刚刚开源的项目，不可避免也存在一些可以改进的方面，让我们开始吧！三、关于NNI3.1、什么是NNINNI (Neural Network Intelligence) 是自动机器学习（AutoML）的工具包。 它通过多种调优的算法来搜索最好的神经网络结构和（或）超参，并支持单机、本地多机、云等不同的运行环境。3.2、我们能用NNI做什么？	在本地 Trial 不同的自动机器学习算法来训练模型。	在分布式环境中加速自动机器学习（如：远程 GPU 工作站和云服务器）。	定制自动机器学习算法，或比较不同的自动机器学习算法。	在自己的机器学习平台中支持自动机器学习。3.3、NNI 安装和使用过程体会安装NNI 的安装过程非常方便，基本没遇到什么障碍，但需要注意的是，NNI目前只支持 Linux 和 Mac（据说兼容 Win10 版本正在开发中），不过在 Win10上有一个解决方案是可以去应用商店下载一个 Ubuntu 虚拟机，也能很方便的开发。NNI的官方文档上介绍了三种安装方式，整个过程非常简单，和大部分的开源框架一样，我使用的是第一种：	通过 pip 命令安装 NNI 先决条件：python &gt;= 3.5python3 -m pip install --upgrade nni使用体验将 NNI 从 Github 上 clone 到本地后，进入路径 ~/nni/examples/trials/mnist-annotation，这里是官方提供的 mnist-annotation 例子，能够带你迅速的了解 NNI 的使用。将NNI的使用总结一下大体是如下的流程：流程	首先定制搜索空间（即你提供给NNI一个参数选择范围）	命令行启动 NNI ：nnictl create --config ./config.yml启动nni.PNG	在浏览器中打开 Web UI url，可以看到NNI的可视化界面：drawingdrawingNNI 的可视化界面也是相较其他工具最讨喜的方面之一，能够让用户在整个实验过程中获得对训练结果的直观理解，方便分析。	首页，点击 overview，可以看到当前试验的进展情况，搜索参数和效果最好的一些超参组合。支持下载 Experiment 结果。img	查看最好结果的 Trial。img	通过超参的分布图来直观地看到哪些超参值会明显比较好，或者看出它们之间的关联。通过下面的颜色图就能直观地看到红色（即精度较高的超参组合）线条所表达的丰富信息。如：	卷积核大一些会表现较好。	全连接层大了不一定太好。也许是所需要的训练时间增加了，训练速度太慢造成的。	而学习率小一些（小于0.005），表现基本都不错。	ReLU 比 tanh 等其它激活函数也好不少。	...Hyper Parameter	通过试验状态页面，能看到每个试验的时间长度以及具体的超参组合。试验	点击 &quot;Trials Detail&quot; 标签查看所有 Trial 的状态。 特别是：	Trial 详情：Trial 的 id，持续时间，开始时间，结束时间，状态，精度和搜索空间。		img	如果在 OpenPAI 或 Kubeflow 平台上运行，还可以看到 hdfsLog。		img	Kill: 可终止正在运行的任务。	支持搜索某个特定的 Trial。	中间结果图。		img3.4、NNI的优势	支持私有部署。云服务中的自动机器学习直接提供了自动机器学习的服务，不仅包含了自动机器学习的功能，也包含了算力。如果团队或个人已经有了很强的算力资源，就需要支持私有部署的自动学习工具了。 NNI 支持私有部署。整个部署也很简单，使用 pip 即可完成安装。	分布式调度。NNI 可以在单机上完成试验，也支持以下两种分布式调度方案：	GPU 远程服务器。通过 SSH 控制多台 GPU 服务器协同完成试验，并能够计划每个试验所需要的 GPU 的数量。	OpenPAI。通过 OpenPAI，NNI 的试验可以在独立的 Docker 中运行，支持多样的实验环境。在计算资源规划上，不仅能指定 GPU 资源，还能制定 CPU，内存资源。	超参搜索的直接支持。当前，大部分自动机器学习服务与工具都是在某个任务上使用，比如图片分类。这样的好处是，普通用户只要有标记数据，就能训练出一个高质量的平台，不需要任何模型训练方面的知识。但这需要对每个训练任务进行定制，将模型训练的复杂性包装起来。 与大部分现有的自动机器学习服务与工具不同，NNI 需要用户提供训练代码，并指定超参的搜索范围。这样的好处在于，NNI 几乎是通用的工具，任何训练任务都可以使用 NNI 来进行超参搜索。但另一方面，NNI 的通用性，也带来了一定的使用门槛。使用 NNI 需要有基本的模型训练的经验。步骤	兼容已有代码。NNI 使用时，可以通过注释的方法来进行无侵入式的改动。不会影响代码原先的用途。通过注释方式支持 NNI 后，代码还可以单独运行。	易于扩展。NNI 的设计上有很强的可扩展性。通过下面这些扩展性，能将系统与算法相隔离，把系统复杂性都包装起来。	Tuner 接口，可以轻松实现新的超参调试算法。研究人员可以使用 NNI 来试验新的超参搜索方法，比如在强化学习时，在 Tuner 中支持 off-policy 来探索比较好的超参组合，在 Trial 里进行 on-policy 的实际验证。也可以使用 Tuner 和训练代码相配合，支持复杂的超参搜索方法。如，实现 ENAS ，将 Tuner 作为 Control，在多个 Trial 中并行试验。	Accessor 接口，可以加速参数搜索，将表现不好的超参组合提前结束。	NNI 还提供了可扩展的集群接口，可以定制对接的计算集群。方便连接已经部署的计算集群。	可视化界面。在启动一次超参搜索试验后，就可以通过可视化界面来查看试验进展，并帮助超参结果，洞察更多信息。四、对NNI的建议	拓展web UI的功能：通过控制页面还可以实时的增加试验的超参组合，或者调整超参的范围。能够在界面中读取之前的log文件	中文文档：便于学生使用与查阅，拓展使用群体，降低上手门槛（已解决）	加入新算法：就在发稿前些天，NNI 支持了较新的 ENAS 算法。看来 DARTS 指日可待。这些 Tuner 算法会在后续的文章中依次介绍的。五、参考资料	Taking Human out of Learning Applications: A Survey on Automated Machine Learning	一篇比较全面的AutoML综述	重磅!微软开源自动机器学习工具 - NNI	开源自动机器学习(AutoML)框架盘点]]></content>
      <categories>
        
          <category> NNI </category>
        
      </categories>
      <tags>
        
          <tag> NNI </tag>
        
          <tag> AutoML </tag>
        
          <tag> 工具 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[微软新工具 NNI 使用指南之 Tuner 篇]]></title>
      <url>/nni/2018/12/16/%E5%BE%AE%E8%BD%AF%E6%96%B0%E5%B7%A5%E5%85%B7-NNI-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%E4%B9%8B-Tuner-%E7%AF%87/</url>
      <content type="text"><![CDATA[									微软新工具 NNI 使用指南之 Tuner 篇	什么是 Tuner在开始之间我们首先需要了解什么是 Tuner。正如之前的博文在 NNI 使用体验中提到的，通俗的来讲，Tuner的作用为让机器自动决定下一次测试的超参设置或下一次尝试的模型结构。 而这篇文章根据学术届的分类将其分为超参调优 (Hyperparameter Optimization)和网络结构搜索 (Neural Architecture Search) 两个部分来介绍。并在每部分结尾处简单介绍一些 NNI 尚未实现但出现在最新顶会中有趣的算法。注：本文中出现的所有引用均可以在该仓库 内找到Hyperparameter OptimizationHO(Hyperparameter Optimization) 为超参调优。简单的来说，该算法仅仅是是使用一系列操作针对超参集中选择最优的超参，但未对原有模型结构进行调优。准确的定义如下：In machine learning, hyperparameter optimization or tuning is the problem of choosing a set of optimal hyperparameters for a learning algorithm.From WikipediaAnnealAnneal Tuner 来源于模拟退火算法 SA(Simulated Annealing)，该算法是一种通用的概率演算法，常用来在一定时间内寻找在一个很大的搜索空间中的近似最优解。该算法类似于贪心算法和遗传算法的结合，其先对上一步中尝试的超参组合进行随机扰动产生新解，之后若该新解有更好的结果则接受新解，若结果变差则按 Metropolis 准则以一定概率接受新解。根据 NNI 的 Anneal 说明文档，建议在每个 Trial 的时间不长，并且有足够的计算资源，或搜索空间的变量能从一些先验分布中采样的情况下使用，但是结果和 Random Search 相当。Batch Tuner（手动批处理模式）Batch Tuner 的使用场景为，用户只想让 NNI 作为一个批处理工具的场景。该 Tuner 让用户自行写入要尝试的超参组合，之后 NNI 会按照设置好的超参组合进行尝试。优点：节省手工启动测试程序的步骤缺点：NNI 仅作为一个批处理工具注：该方法存在的意义在于，可以节省两次调参调整中间的间隔时间Grid Search网格搜索是一个非常符合人类直观思路的方法，即穷居法。它穷尽了所有种可能的超参排列方式，再依次进行尝试后找到最优方案。过程和我们在中学时学的排列组合一样，三个超参分别有三种取值 3、4、5，那么三种超参就有 3*4*5 = 60 种取值方式。显然这种搜索方式是不科学的，非常容易组合爆炸，是一种非常不高效的调参方式。优点：该算法考虑到了搜索空间内所有的参数组合缺点：存在组合爆炸的问额注：强烈不推荐使用Random Search该思想来源于神经网络三巨头 Bengio 在 JMLR 2012的工作 [1]，这篇论文的核心贡献是从经验和理论上证明了，随机搜索超参的方式相比传统的网格搜索，能在更短的时间内找到一样或更好的模型参数。该算法也从此成为各个优化算法的对比标准。优点：容易理解缺点：高维搜索空间表现一般贝叶斯优化系列实际上 Grid Search 和 Random Search 都是非常普通的方法，同时暴力搜索和随机搜索当然也很难体现算法设计者的智慧。而接下来要讲的贝叶斯优化系列则“很可能”存在与人工经验调惨相媲美的实力。	Bayesian Optimization		贝叶斯优化 (Bayesian Optimization)，这个工作最初是由 J Snoek et.[2] 在 NIPS 2012 中提出的，并随后多次进行改进 [3, 4]。它要求已经存在几个样本点，并且通过高斯过程回归（假设超参数间符合联合高斯分布）计算前面 n 个点的后验概率分布，得到每一个超参数在每一个取值点的期望均值和方差，其中均值代表这个点最终的期望效果，均值越大表示模型最终指标越大，方差表示这个点的效果不确定性，方差越大表示这个点不确定是否可能取得最大值非常值得去探索，具体的细节分析可以参见这篇博客		但是这个算法仅仅在低纬空间表现优于 Random Search，而在高维空间和 Random Search 表现相当。				优点：在低维空间显著优于 Random Search		缺点：在高维空间仅和 Random Search 相当		注：这个算法的另一个名称为 Spearmint[2]			TPE		TPE 算法来源于 Y Bengio 在顶会 NIPS2011 的工作 [7]。 TPE 依旧属于贝叶斯优化，其和 SMAC 也有着很深的渊源。其为一种基于树状结构 Parzen 密度估计的非标准贝叶斯优化算法。相较于其他贝叶斯优化算法，TPE 在高维空间表现最佳。				优点 1：相比其他贝叶斯优化算法，高维空间的情况下效果更好		优点 2：相比 BO 速度有显著提升		缺点：在高维空间的情况下，与 Random Search 效果相当		注：所有贝叶斯优化算法在高维空间下表现均和 Random Search 相当 [6]			SMAC		SMAC 算法出自于期刊 LION 2011[5]，其论文中表明，由于先前的 SMBO 算法，不支持离散型变量。SMAC 提出使用 Random Forest 将条件概率 p(y|λ) 建模为高斯分布，其中 λ 为超参的选择。这使得它能够很好的支持离散型变量，并在离散型变量和连续型变量的混合的时候有着不错的表现 [6]。				优点 1：能很好的支持离散型变量，并针对高维空间有一定改善		优点 2：相比 BO 速度有显著提升		缺点：效果不稳定，高维空间表现和 Random Search 相当		HyperBandHyperBand 来源于非常新的来自 JMLR 2018 的工作 [6]，实质为 Multi-Armed Bandit 问题。其解决的问题为如何平衡“探索”(exploration) 和“利用”(exploitation)。该算法相比之前的算法，最突出的特点为，其限定了资源的总量，优化算法的问题转化为如何在给定资源的情况下，更好的利用资源找出最优解的问题。这些可以体现在超参的设计当中。优点 1：考虑到了资源有限的情况优点 2：可以和其他的算法，如 TPE 等进行融合（当前 NNI 不支持）[6]缺点：算法筛选结果的方式为每多步之后，保留 TopK 的结果，其将导致一些开始收敛较慢的参数配置被淘汰。MetisMetis 为微软提供的自动调参服务，在论文 [8]中指出，之前的贝叶斯优化算法存在两个较大的问题，一方面贝叶斯优化算法，存在过度采样的问题，这在资源密集型的算法，如深度学习的情况下是一笔很重的开销。另一方面贝叶斯优化和高斯处理算法均假设，问题是理想的无噪声或仅受高斯分布的噪声影响，但实际情况比这个假设要复杂。而 Metis 在一定程度上解决了这个问题。Metis 的本质是随机搜索，为了最小化调整配置时造成的系统性能损失，该算法当且仅当预测的配置可以带来高于一定程度的优化时才会切换配置。优点 1：在系统配置选择等存在非高斯噪声的情况下，Metis 显著优于 TPE 算法优点 2：能为接下来的尝试提供候选缺点：训练时间基本由样本点的数量决定，且呈立方级增长（复杂度为 $O(N^{3}+N^{2}D)$ 其中 N 为样本点数量，D 为样本点的维数）。配置选择时间同样基本由样本点的数目决定，其复杂度为 $O(N^{2}+ND)$ $\Rightarrow$ 和样本点的数据量高度相关且复杂度高注：NNI 中 Metis 只支持 choice, quniform, uniform 和 randint 类型。Neural Architecture Search什么是 NAS和上一节中的 HO 不同的是，在超参调整中，NAS 调整的超参会影响到模型的结构。意在通过大量尝试探索一种更为合理的网络结构。而这些超参的调整已经超出了之前介绍的贝叶斯优化系列的能力范畴。在 NAS 中遗传算法系列和强化学习系列有着不错的表现。Naive EvolutionNaive Evolution 出自 ICML 2017 的工作 [9]，其将遗传算法引入模型结构的搜索。根据论文中的描述，该方法为一种典型的遗传算法，其通过设置一定量的初始种群，经过“突变”（更改超参）后根据“自然筛选”（筛选出表现优秀的模型）保留优异的个体。论文中是针对图片分类的模型探索，支持多种“突变”，细节可参见原论文 [9]。除了使用遗传算法这个特点外，其还有一个特点是支持权重迁移的模型，这个特点会使得每个突变个体只需要少量的 epoch 来训练。而这个优点也在 Morphism 算法中得以体现。但是该算法也存在陷入局部最优解的问题，论文中描述该问题主要可以通过增大以下两个参数解决。	population size		这个参数的增大会让初始种群个体数量更多，这样子可以通过增加会避免模型陷入局部最优解。	the number of training steps per individual		这个参数的增大会让每一个个体的表现得到最大的发挥，从而不会错淘汰掉潜力优秀的个体注：遗憾的是 NNI 中还未能让用户自行调整这两个参数优点 1: 使用遗传算法进行模型结构的搜索优点 2: 对支持权重迁移的模型，运算的速度会有显著提升缺点：同所有的遗传算法一样，优于需要尝试大量的突变个体，需要大量的计算资源Network MorphismNetwork Morphism 来自另一个开源自动调参工具 Auto-keras 的其中一篇 arXiv 上的文章 [10]。该算法的设计的初衷是减少调参的计算损耗。为了提高算法的效率，其引入了一个新的神经网络核 (Neural Network Kernel) 和 一个树形采样函数，来用贝叶斯优化的方式探索搜索空间。除此之外，另一点提高效率的方式为引入了 Morphism Operation。该操作是在已经训练好的模型上进行调整，这样新生成的网络结构就只需要少量的训练就能达到好的效果。这个方式在 Naive Evolution 也有使用。就论文中在 MNIST，FASHION，CIFAR-10 三个数据集上的实验结果看，其显著好于 Random Search，SPMT[11]，SMAC，SEA[12]，NASBOT[13]。优点 1: 在探索网络结构的时候引入了贝叶斯优化，提高了搜索效率优点 2: Morphism 操作，保留了之前的训练的结果，让新的变种网络训练的更快缺点：当前版本不支持 RNN 的网络模型探索ENASENAS 源自 CVPR 2018 的一个工作 [14]，其使用 RNN 作为控制器然后根据收敛精度调整 RNN，论文中在 Cifar10 上训练 RNN，之后再应用到 ImageNet 上，效果很惊人（应用到 Fast-RCNN 上之后居然有 4% 的准确率提升）。但是该算法需要很多预先的人为的前提设定，同时速度还是很慢, 而 ENAS 的主要工作为在其工作上，增加参数共享的方式，避免新产生的模型重训练，加快了训练速度。优点：相较于其他的 NAS 算法，该算法速度非常快缺点：同它的改进的模型一样，其需要设置每个 Cell 的 Block 等多个前提设定的参数注：相较于其他算法，这个算法更有趣，论文中的效果现实也是最有意思最好的NNI 未实现但很有趣的算法NASBOTNASBOT 源自 NIPS2018 的一个工作 [13]， 其核心就在于计算 OTMANN distance。该方法使用了 layer masses 和 path length 来定义 OTMANN distance。三者的定义如下：	layer masses：在比较两个神经网络时匹配的层的个数	path length：两个层之前的距离，如 (2, 5, 8, 13) 是一条从 layer2 到 layer13 的一条长度为 3 的路径	OTMANN distance：通过最优化算法得出的神经网络之间的距离就结果而言论文中表明 NASBOT 在 Cifar10 数据集上，在计算量和运行时间方面显著优于其他 tuner 算法，如随机搜索，进化算法等。优点 1：对 MLPs 和 CNN 的支持效果较好优点 2：训练需要的总计算量较小缺点：在寻找下一个网络架构的用时较长  注：该算法的 python 实现：源码链接DARTSDARTS 为在 arXiv 上的一篇很有意思的工作 [15]，其正在等待 ICLR 2019 的审核，详情可以参见 OpenReview 该论文的链接。该工作的中心思想为科学选择神经网络结构，将神经网络结构作为参数进行优化。该论文在 Cifar10，ImageNet 等数据集上进行了大量的实验，发现此算法适用于图像分类的高性能卷积结构和语言建模的循环神经网络结构，并提出此方法的效率高于 ENAS。优点 1：将模型结构视为参数，扩大搜索空间优点 2：较高的可迁移性缺点：参数过多，对算力和数据量的要求较高 注：该论文的复现可以参考作者的源码：源码链接参考论文	[1] Bergstra J, Bengio Y. Random search for hyper-parameter optimization[J]. Journal of Machine Learning Research, 2012, 13(Feb): 281-305.	[2] Snoek J, Larochelle H, Adams R P. Practical bayesian optimization of machine learning algorithms[C]//Advances in neural information processing systems. 2012: 2951-2959.	[3] Swersky K, Snoek J, Adams R P. Multi-task bayesian optimization[C]//Advances in neural information processing systems. 2013: 2004-2012.	[4] Snoek J, Rippel O, Swersky K, et al. Scalable bayesian optimization using deep neural networks[C]//International Conference on Machine Learning. 2015: 2171-2180.	[5] Hutter F, Hoos H H, Leyton-Brown K. Sequential model-based optimization for general algorithm configuration[C]//International Conference on Learning and Intelligent Optimization. Springer, Berlin, Heidelberg, 2011: 507-523.	[6] Li L, Jamieson K, DeSalvo G, et al. Hyperband: A novel bandit-based approach to hyperparameter optimization[J]. arXiv preprint arXiv:1603.06560, 2018: 1-48.	[7] Bergstra J S, Bardenet R, Bengio Y, et al. Algorithms for hyper-parameter optimization[C]//Advances in neural information processing systems. 2011: 2546-2554.	[8] Li Z L, Liang C J M, He W, et al. Metis: robustly optimizing tail latencies of cloud systems[C]//Proceedings of the 2018 USENIX Conference on Usenix Annual Technical Conference. USENIX Association, 2018: 981-992.	[9] Real, E., Moore, S., Selle, A., Saxena, S., Suematsu, Y. L., Tan, J., ... &amp; Kurakin, A. (2017, July). Large-Scale Evolution of Image Classifiers. In International Conference on Machine Learning (pp. 2902-2911).	[10] Jin H, Song Q, Hu X. Efficient neural architecture search with network morphism[J]. arXiv preprint arXiv:1806.10282, 2018.	[11] Snoek, J., Larochelle, H., and Adams, R. P.Practical bayesian optimization of machine learning algorithms.In Advances in Neural Information Processing Systems, pp. 2951–2959, 2012.	[12] Elsken, T., Metzen, J. H., and Hutter, F. Neural architec- ture search: A survey. arXiv preprint arXiv:1808.05377, 2018.	[13] Kandasamy, K., Neiswanger, W., Schneider, J., Poczos, B., and Xing, E. Neural architecture search with bayesian optimisation and optimal transport. NIPS, 2018.	[14] Zoph B, Vasudevan V, Shlens J, et al. Learning transferable architectures for scalable image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 8697-8710.	[15] Liu, Hanxiao, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055 (2018).]]></content>
      <categories>
        
          <category> NNI </category>
        
      </categories>
      <tags>
        
          <tag> NNI </tag>
        
          <tag> AutoML </tag>
        
          <tag> 工具 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[微软新工具 NNI 使用指南之 Assessor 篇]]></title>
      <url>/nni/2018/12/16/%E5%BE%AE%E8%BD%AF%E6%96%B0%E5%B7%A5%E5%85%B7-NNI-%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97%E4%B9%8B-Assessor-%E7%AF%87/</url>
      <content type="text"><![CDATA[									微软新工具 NNI 使用指南之 Assessor 篇	什么是 Assessor在说 NNI 中的 Assessor 算法前，首先需要了解下什么是 Assessor。通常来说，“Assessor” 不是论文中的通常叫法，一般而言 Assessor 在论文中被叫做 “Early Stop”。顾名思义，该模块的作用在于判断当前尝试的超参是否有“前途”，如果当前设置被判断为即便多次迭代后仍无法获得更好的结果时便提前终止迭代，以节约宝贵的计算资源。注：本文中出现的所有引用均可以在该仓库 内找到NNI 已有 Assessor 算法介绍Median StopMedian Stop 出现在 Goolge 的自动调参工具的论文 [1] 中，该方法在论文中的描述为，在当前尝试的超参训练的过程中，如果出现最新 step 的结果比之前所有的 step 的结果的均值要低的情况，则终止该训练。这种方法的优点为不需要拟合曲线，运算简单，但是缺点也很明显，即利用之前步骤的信息较少，判断可能不是很准确。优点：算法简单，算法运算速度快缺点：对之前已有的信息利用不充分，判断结果相对较不准确，如曲线震荡幅度比较大但是最终结果很好的情况。CurvefittingCurvefitting 是一篇在顶会 IJCAI 的工作 [2]，相比之前的 Median Stop 算法，该算法同贝叶斯优化算法类似，使用之前的 n 个样本点来拟合学习曲线。而与传统的使用高斯处理来拟合学习曲线不同的是，该算法引入了马尔可夫蒙特卡洛 (Markov Chain Monte Carlo)，这使得算法能够更加充分利用之前的样本点中的信息，以更好的预测当前训练的最终结果。该算法的停止标准为，当预计当前算法的最优结果低于之前的最优结果，则决定提前停止当前尝试。优点：能够更好的学习到前几次的尝试样本，能更准确的判断是否该提前停止缺点：该算法需要冷启动，需要设置较多超参论文中的实验将 Curvefitting 同 SMAC 结合，如图 2.1 所示，可以看出效果还是非常明显的。图 2.1 Curvefitting结语当前已有的 Assessor 主要出自 Google 的 Google Vizier[3]，经过对论文的调查有效的 Early Stop 算法均以实现。 当前 NNI 内的 Assessor 的定义有些类似 Early Stop，而这个单独作为一类的话似乎有点太奢侈。在 Tuner 的调研中，发现有一些算法，如 Hyperband[4] 是可以和其他 Tuner 进行融合而具有更好的结果的。如果将 Assessor 定义为可以和基础算法进行融合的算法的话，似乎可以让 Tuner 的配置更具灵活性。但是不管怎么说，Assessor 的使用简单明确，对新手非常友好是一个很好的组件。NNI 也是一个简单易用的工具，希望未来 NNI 越来越完善，最终成为炼丹师们的好帮手。参考论文	[1] Golovin D, Solnik B, Moitra S, et al. Google vizier: A service for black-box optimization[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017: 1487-1495.	[2] Domhan T, Springenberg J T, Hutter F. Speeding Up Automatic Hyperparameter Optimization of Deep Neural Networks by Extrapolation of Learning Curves[C]//IJCAI. 2015, 15: 3460-8.	[3] Golovin D, Solnik B, Moitra S, et al. Google vizier: A service for black-box optimization[C]//Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2017: 1487-1495.	[4] Li L, Jamieson K, DeSalvo G, et al. Hyperband: A novel bandit-based approach to hyperparameter optimization[J]. arXiv preprint arXiv:1603.06560, 2018: 1-48.]]></content>
      <categories>
        
          <category> NNI </category>
        
      </categories>
      <tags>
        
          <tag> NNI </tag>
        
          <tag> AutoML </tag>
        
          <tag> 工具 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow进阶之数据导入]]></title>
      <url>/tensorflow/2018/12/16/Tensorflow%E8%BF%9B%E9%98%B6%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/</url>
      <content type="text"><![CDATA[不同格式的数据的导入Numpy 数据的导入这种导入非常直白，就是使用 Numpy 把外部的数据进行导入，然后转换成 tf.Tensor ，之后使用 Dataset.from_tensor_slices()。就可以成功导入了。简单的案例如下：# Load the training data into two NumPy arrays, for example using `np.load()`.with np.load("/var/data/training_data.npy") as data:  features = data["features"]  labels = data["labels"]# Assume that each row of `features` corresponds to the same row as `labels`.assert features.shape[0] == labels.shape[0]dataset = tf.data.Dataset.from_tensor_slices((features, labels))上面的简单的实例有一个很大的问题，就是 features 和 labels 会作为 tf.constant() 指令嵌入在 Tensorflow 的图中，会浪费很多内存。所以我们可以根据 tf.palceholder() 来定义 Dataset，同时在对数据集初始化的时候送入 Numpy 数组。with np.load("/var/data/training_data.npy") as data:  features = data["features"]  labels = data["labels"]# Assume that each row of `features` corresponds to the same row as `labels`.assert features.shape[0] == labels.shape[0]features_placeholder = tf.placeholder(features.dtype, features.shape)labels_placeholder = tf.placeholder(labels.dtype, labels.shape)dataset = tf.data.Dataset.from_tensor_slices((features_placeholder, labels_placeholder))# [Other transformations on `dataset`...]dataset = ...iterator = dataset.make_initializable_iterator()sess.run(iterator.initializer, feed_dict={features_placeholder: features,                                          labels_placeholder: labels})TFRecord 数据的导入TFRecord 是一种面向记录的简单二进制格式，很多 Tensorflow 应用采用这种方式来训练数据。这个也是推荐的做法。将它做成 Dataset 的方式也非常简单，就是单纯的通过 tf.data.TFRecordDataset 类就可以实现。# Creates a dataset that reads all of the examples from two files.filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.data.TFRecordDataset(filenames)同样我们也能设定成，在初始化迭代器的时候导入数据。其中需要注意的是 filenames 需要设置成 tf.String 类。filenames = tf.placeholder(tf.string, shape=[None])dataset = tf.data.TFRecordDataset(filenames)dataset = dataset.map(...)  # Parse the record into tensors.dataset = dataset.repeat()  # Repeat the input indefinitely.dataset = dataset.batch(32)iterator = dataset.make_initializable_iterator()# You can feed the initializer with the appropriate filenames for the current# phase of execution, e.g. training vs. validation.# Initialize `iterator` with training data.training_filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]sess.run(iterator.initializer, feed_dict={filenames: training_filenames})# Initialize `iterator` with validation data.validation_filenames = ["/var/data/validation1.tfrecord", ...]sess.run(iterator.initializer, feed_dict={filenames: validation_filenames})Dataset 的预处理Dataset.map()Dataset.map(f) 转换通过将指定函数 f 应用于输入数据集的每个元素来生成新数据集。简单的实例（解码图片数据并调整大小）如下：# Reads an image from a file, decodes it into a dense tensor, and resizes it# to a fixed shape.def _parse_function(filename, label):  image_string = tf.read_file(filename)  image_decoded = tf.image.decode_jpeg(image_string)  image_resized = tf.image.resize_images(image_decoded, [28, 28])  return image_resized, label# A vector of filenames.filenames = tf.constant(["/var/data/image1.jpg", "/var/data/image2.jpg", ...])# `labels[i]` is the label for the image in `filenames[i].labels = tf.constant([0, 37, ...])dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))dataset = dataset.map(_parse_function)至此为止，我们对图片的处理还是使用的是 TensorFlow 中的 API，那么我们想用 Python 自带的奇奇怪怪的包应该怎么做呢。TensorFlow 给了我们 tf.py_func() 这个选项来使用任意 Python 逻辑。我们只用在 Dataset.map() 中调用 tf.py_func() 指令就可以了。简单的例子如下：import cv2# Use a custom OpenCV function to read the image, instead of the standard# TensorFlow `tf.read_file()` operation.def _read_py_function(filename, label):  image_decoded = cv2.imread(filename.decode(), cv2.IMREAD_GRAYSCALE)  return image_decoded, label# Use standard TensorFlow operations to resize the image to a fixed shape.def _resize_function(image_decoded, label):  image_decoded.set_shape([None, None, None])  image_resized = tf.image.resize_images(image_decoded, [28, 28])  return image_resized, labelfilenames = ["/var/data/image1.jpg", "/var/data/image2.jpg", ...]labels = [0, 37, 29, 1, ...]dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))dataset = dataset.map(    lambda filename, label: tuple(tf.py_func(        _read_py_function, [filename, label], [tf.uint8, label.dtype])))dataset = dataset.map(_resize_function)批处理数据  简单的批处理 简单的批处理我们直接调用 Dataset.batch() 这种 API 即可，但是它有一个限制就是对于每个组件 i，所有元素的张量形状都必须完全相同。     inc_dataset = tf.data.Dataset.range(100) dec_dataset = tf.data.Dataset.range(0, -100, -1) dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset)) batched_dataset = dataset.batch(4) iterator = batched_dataset.make_one_shot_iterator() next_element = iterator.get_next() print(sess.run(next_element))  # ==&gt; ([0, 1, 2,   3],   [ 0, -1,  -2,  -3]) print(sess.run(next_element))  # ==&gt; ([4, 5, 6,   7],   [-4, -5,  -6,  -7]) print(sess.run(next_element))  # ==&gt; ([8, 9, 10, 11],   [-8, -9, -10, -11])        填充批处理张量 和简单批处理相比，这种方式可以对具有不同大小的张量进行批处理。这种方法的 API 为 Dataset.padded_batch()。简单的实例展示如下：     dataset = tf.data.Dataset.range(100) dataset = dataset.map(lambda x: tf.fill([tf.cast(x, tf.int32)], x)) dataset = dataset.padded_batch(4, padded_shapes=[None]) iterator = dataset.make_one_shot_iterator() next_element = iterator.get_next() print(sess.run(next_element))  # ==&gt; [[0, 0, 0], [1, 0, 0], [2, 2, 0], [3, 3, 3]] print(sess.run(next_element))  # ==&gt; [[4, 4, 4, 4, 0, 0, 0],                         	   #      [5, 5, 5, 5, 5, 0, 0],                         	   #      [6, 6, 6, 6, 6, 6, 0],                         	   #      [7, 7, 7, 7, 7, 7, 7]]        可以通过 Dataset.padded_batch() 转换为每个组件的每个维度设置不同的填充，并且可以采用可变长度（在上面的示例中用 None 表示）或恒定长度。也可以替换填充值，默认设置为 0。  训练工作流程处理多个周期有时候我们希望我们的数据集能训练很多个周期，简单的方法是使用Dataset.repeat() API。filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.repeat(10)dataset = dataset.batch(32)上述例子中，我们将 dataset 重复了 10 个周期，值得注意的是如果 repeat 中没有参数代表中无限次地重复使用，即不会在一个周期结束和下一个周期开始时发出信号。如果我们想在每个周期结束时收到信号，则可以编写在数据集结束时捕获 tf.errors.OutOfRangeError 的训练循环。此时，就可以收集关于该周期的一些统计信息。filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.batch(32)iterator = dataset.make_initializable_iterator()next_element = iterator.get_next()# Compute for 100 epochs.for _ in range(100):    sess.run(iterator.initializer)    while True:        try:            sess.run(next_element)        except tf.errors.OutOfRangeError:            break    # [Perform end-of-epoch calculations here.]随机重排数据有时候我们希望能随机的选取 Dataset 中的元素，则可以使用 Dataset.shuffle()。filenames = ["/var/data/file1.tfrecord", "/var/data/file2.tfrecord"]dataset = tf.data.TFRecordDataset(filenames)dataset = dataset.map(...)dataset = dataset.shuffle(buffer_size=10000)dataset = dataset.batch(32)dataset = dataset.repeat()参考  [官方教程]]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
      </categories>
      <tags>
        
          <tag> TensorFlow </tag>
        
          <tag> Dataset </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python数据可视化之 seaborn]]></title>
      <url>/python/2018/12/11/python%E6%95%B0%E6%8D%AE%E5%8F%AF%E8%A7%86%E5%8C%96%E4%B9%8Bseaborn/</url>
      <content type="text"><![CDATA[简介Seaborn 是一个数据可视化的库，主要用来生成热力图的，详情查看它的官网。这个工具一定要混合 matplotlib 来使用，我们在做好图之后还是必须要用 plt.show 才能展示图片，同时图片的布局也是靠 matplotlib。热力图热力图很多人其实是第一次接触，我在查了百度之后的百度百科结果不太满意，转而投奔 Wiki 结果如下：  A heat map (or heatmap) is a graphical representation of data where the individual values contained in a matrix are represented as colors. “Heat map” is a newer term but shading matrices have existed for over a century.除此之外我们还可以看到一些重要的描述如下：  Heat maps originated in 2D displays of the values in a data matrix. Larger values were represented by small dark gray or black squares (pixels) and smaller values by lighter squares.我们可以看到它其实是用深浅来表示大小的。调用 seaborn 的代码实现如下：import numpy as npimport seaborn as snsimport matplotlib.pyplot as pltnp.random.seed(0)sns.set()uniform_data = np.random.rand(10, 12)ax = sns.heatmap(uniform_data)plt.show()当然这仅仅是热图中的一小部分，它还有其他功能，可以参考官网 heatmap 的章节。密度图通过这个图我们能看出数据的密度，能很直观的看出输出的值最集中的区域。简单的代码（二维密度图）如下：import numpy as np; np.random.seed(10)import seaborn as sns; sns.set(color_codes=True)import matplotlib.pyplot as pltmean, cov = [0, 2], [(1, .5), (.5, 1)]x, y = np.random.multivariate_normal(mean, cov, size=50).Tax = sns.kdeplot(x, y, shade=True)plt.show()同理，其他细节可以参考官网 kdeplot 章节。结尾对于可视化的工具，个人比较喜欢插它官网的 Gallery，比如 Echart 的 Gallery，在 Gallery 中我们可以很好的把握图标的样式，还能得到样例代码，不失为一件美事。参考  4种更快更简单实现 Python 数据可视化的方法  seaborn 官网 Gallery]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python 多进程并发]]></title>
      <url>/python/2018/12/10/python%E5%A4%9A%E8%BF%9B%E7%A8%8B%E5%B9%B6%E5%8F%91/</url>
      <content type="text"><![CDATA[前言最近在处理大数据相关的东西，数据动辄上百万，还不能用 GPU 加速，于是开始动起了多进程的念头。众所周知，Python 的多线程是假的，不过好在开发者老大还是给我们留了一个活路，也就是进程池。这个方法的优点在于进程的并发细节完全不用我们操心，我们只需要把并发的任务仍到进程池里就好了。直接使用进程池import multiprocessingdef mission(param1):	print(param1)# 设置一个允许3个进程并发的进程池pool = multiprocessing.Pool(processes = 3)for i in range(1000):	# 将进程仍入进程池，mission 后面的这个含有 i 的tuple 代表给mission的参数	pool.apply_async(mission, (i))	# 扔了 1000个进程进进程池后，关闭进程池，不允许新的进程加入pool.close()# 运行进程池中的进程pool.join()	一些说明  示例中使用的是apply_async这个代表是非阻塞的，意味着输出不保证顺序，而apply 是阻塞的，输出是按照输入顺序输出的。  pool.terminate() 代表着结束工作进程，不再处理未完成的任务  pool.join() 阻塞主进程，等待子进程的推出， join 要在 close 或 terminate 之后使用。  进程数大家可以按需要调整，它不是越大越好，也不是说 CPU 只有 4 个核就只能开到 4，在个人的 PC 上，开到 10 会有一个很明显的提升，在个人任务中，速度提升了至少 7 倍左右。参考  http://www.cnblogs.com/kaituorensheng/p/4465768.html]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[python使用二进制文件存取中间变量]]></title>
      <url>/python/2018/12/10/python%E4%BD%BF%E7%94%A8%E4%BA%8C%E8%BF%9B%E5%88%B6%E6%96%87%E4%BB%B6%E5%AD%98%E5%8F%96%E4%B8%AD%E9%97%B4%E5%8F%98%E9%87%8F/</url>
      <content type="text"><![CDATA[前言我们经常遇到一种情况，就是废了很大的精力和时间通过程序算取的数值，在程序结束后就会被销毁，而下次再想使用则需要再算一遍。通用的存储这些值的方法为把他们以文本的方式存到文件中，之后需要的时候再读取。然而这种方式的效率实在是比较低，python 为我们提供了一个将值存储到 2进制文件的方案，其速度亲测可以快 3 倍左右。使用方法简单的来说就是调用 pickle 这个 python 自带的库。  按顺序存储变量     import pickle data1 = 1 data2 = 1 output = open('filename.pkl', 'wb') pickle.dump(data1, output) pickle.dump(data2, output, -1) output.close()        按顺序读取变量     import pickle		 _input = open('filename.pkl', 'rb') data1 = pickle.load(_input) data2 = pickle.load(_input) _input.close()      参考文章  python数据持久存储]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow 进阶之 Estimator]]></title>
      <url>/tensorflow/2018/12/10/Tensorflow%E8%BF%9B%E9%98%B6%E4%B9%8BEstimator/</url>
      <content type="text"><![CDATA[之前的入门部分的 Estimator介绍了如何使用预训练模型，对整体有了一个直观的感受感受。在这部分中着重讲解如何创建自定义 Estimator。Estimator 模型的简单说明所有的 Estimator 的模型的基类为 tf.estimator.Estimator ，这意味着即便是预设置的模型其实也是用自定义模型的方式设置的。和之前介绍的使用预创建的 Estimator 的唯一区别在于，我们需要自行编写模型函数（model_fn）model_fn输入格式虽然是自定义模型我们还是要遵循一定的输入和输出的格式的，输入格式简单介绍如下：def my_model_fn(   features, # This is batch_features from input_fn   labels,   # This is batch_labels from input_fn   mode,     # An instance of tf.estimator.ModeKeys   params):  # Additional configuration其中需要注意的是 params 是字典的格式。模型具备的内容简单的来看，我们训练模型肯定需要三个操作，即训练，评估和预测。那么理所当然，这个模型中必须具有这三个操作。除此之外，在模型的书写方面也有一定的规范如下：  定义模型结构 这一步和正常构建模型结构是相同的。通过从 input_fn 传入的 features 和 labels 进行构建模型就好了  预测、训练和评估 这一步简单的来说，就是使用通过对 mode 进行一个简单的条件判断就可以将三者区分开来，同时对于每一个 mode 值都需要返回一个 tf.estimator.EstimatorSpec 的一个实例，其中包含调用程序所需的信息。     if mode == tf.estimator.ModeKeys.PREDICT:     # your predict code for mode.predict()     predictions = {     	'class_ids': predicted_classes[:, tf.newaxis],     	'probabilities': tf.nn.softmax(logits),     	'logits': logits,     }     return tf.estimator.EstimatorSpec(mode, predictions=predictions) if mode == tf.estimator.ModeKeys.EVAL:     # your code for mode.evaluate()     # compute loss and accurarcy     loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)     accuracy = tf.metrics.accuracy(labels=labels,                         	   	   predictions=predicted_classes,                         	       name='acc_op')     # set metrics     metrics = {'accuracy': accuracy}     return tf.estimator.EstimatorSpec(     	mode, loss=loss, eval_metric_ops=metrics) if mode == tf.estimator.ModeKeys.TRAIN:     # your code for mode.train()     optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)     train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())     return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)        注：需要注意的其实是 EstimatorSpec 中的三者的参数有略微的不同。  自定义模型的使用和之前使用封装好的模型的步骤相同，只是在实例化 Estimator 的时候需要传入自定的模型。classifier = tf.estimator.Estimator(        model_fn=my_model,        params={            'feature_columns': my_feature_columns,            'your params1': your params1,            'your params2': 2,        })检查点的使用使用检查点保存模型我们使用 GPU 进行训练的时间会很长，我们非常不希望因为一些意外原因比如“熊孩子踹掉电源”的时间导致模型完全重新训练。Estimator 有一种简单的方式可以实现实时存盘的功能。值得一提的是，检查点的功能其实是默认开启的，在我们实例化 Estimator 的时候，未指定 model_dir 的话，会自动存到 Python 的 tempfile,mkdtmep 函数选择的临时文件夹中。这样看，我们直接指定 model_dir 就可以使用检查点了，所以检查点的使用代码，就是在实例化 Estimator  的时候加一个参数 model_dir。# sample from tensorflow tutorialclassifier = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[10, 10],    n_classes=3,    model_dir='models/iris')模型默认有两个情况是写入检查点的：  每 10 分钟写入一个检查点  在 train 方法开始（第一次迭代）和完成（最后一次迭代）时写入一个检查点注：只在目录中保留 5 个最近写入的检查点显然我们可以通过 RunConfig 对象来定义所需要的存档频率，步骤如下：  创建一个 RunConfig 对象来定义所需要的时间安排  实例化的时候传入 RunConfig。my_checkpointing_config = tf.estimator.RunConfig(    save_checkpoints_secs = 20*60,  # Save checkpoints every 20 minutes.    keep_checkpoint_max = 10,       # Retain the 10 most recent checkpoints.)classifier = tf.estimator.DNNClassifier(    feature_columns=my_feature_columns,    hidden_units=[10, 10],    n_classes=3,    model_dir='models/iris',    config=my_checkpointing_config)注：在 Keras 入门中提到的 GPU 并行操作，也是在这里设置的。检查点的恢复简单的来说，我们并不需要额外设置检查点的回复，Estimator 在使用 train，evaluate 或 predict 方法的时候，都会以以下方式自动恢复检查点。  Estimator 通过运行 model_fn() 构建模型图。  Estimator 根据最近写入的检查点中存储的数据来初始化新模型的权重。当然，这种简单的恢复就仅仅支持和当前模型匹配的检查点恢复，如果更改了模型之前的检查点就不能用了。结尾至此为止，我们学会了如何构建一个自定义的模型，以及如何在训练的时候使用 Estimator 保存模型和输入 Tensorboard 可以使用的 log 文件。]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
      </categories>
      <tags>
        
          <tag> TensorFlow </tag>
        
          <tag> Estimator </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow入门之数据导入]]></title>
      <url>/tensorflow/2018/12/09/Tensorflow%E5%85%A5%E9%97%A8%E4%B9%8B%E6%95%B0%E6%8D%AE%E5%AF%BC%E5%85%A5/</url>
      <content type="text"><![CDATA[tf.data API 简介借助这个 API 可以较为快速的入门数据导入的部分。自定义数据输入可以说是跑任何模型必须要会的部分。学习这部分 API 是入门 Tensorflow跳不过的部分。本部分和之前的 Tensorflow 部分一样，主要是筛选自官方教程，意在跳出自己认为核心的入门内容，抛去复杂的细节，以求快速入门。tf.data 中的两个主要类tf.data.Datasettf.data.Dataset 表示一系列元素，其中每个元素包含一个或多个 Tensor 对象。在图像管道中，这个元素可能是单个训练样本，具有一对表示图像的数据和标签的张量。  创建来源：通过一个或多个 tf.Tensor 对象构成，可以使用 Dataset.from_tensor_slices() 来构建。  转换：通过 tf.data.Dataset 对象构建数据集，如Dataset.batch()tf.data.Iterator上面介绍了如何创建数据集，这里就提供了如何从数据集中提取元素的方法。Iterator.get_next() 返回的操作会在执行时生成 Dataset 的下一个元素，并且此操作通常充当输入管道代码和模型之间的接口。最简单的迭代器是”单次迭代器”，它与特定的 Dataset 相关联，并对其进行一次迭代。要实现更复杂的用途，您可以通过 Iterator.initializer 操作使用不同的数据集重新初始化和参数化迭代器，这样一来，这样就可以在同一个程序中对训练和验证数据进行多次迭代。输入管道的流程      构建 Dataset 并处理    tf.data.Dataset.from_tensors() 或 tf.data.Dataset.from_tensor_slices()来构建 Dataset，当然如果以特定格式存储的数据，也有对应的读取方式，如 TFRecord 的为tf.data.TFRecordDataset。之后我们可以用 map 等函数进行对原始数据的二次加工，这部分可以查看 tf.data.Dataset 的文档。        创建迭代器          Iterator.initializer 可以初始化迭代器的状态，可以达到一些复杂的操作。      Iterator.get_next() 取下一个对象。      数据集的结构首先数据集中的每个元素的结构需要是相同的。一个元素可以包含一个或多个 Tensor 对象，这些对象为组件。每一个组件都有一个 td.Dtype 表示张量的类型，和一个 tf.TensorShape 来表示元素的形状。我们可以通过 Dataset.output_types 和 Dataset.output_shapes 来查看数据的类型和形状。dataset1 = tf.data.Dataset.from_tensor_slices(tf.random_uniform([4, 10]))print(dataset1.output_types)  # ==&gt; "tf.float32"print(dataset1.output_shapes)  # ==&gt; "(10,)"dataset2 = tf.data.Dataset.from_tensor_slices(   (tf.random_uniform([4]),    tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)))print(dataset2.output_types)  # ==&gt; "(tf.float32, tf.int32)"print(dataset2.output_shapes)  # ==&gt; "((), (100,))"dataset3 = tf.data.Dataset.zip((dataset1, dataset2))print(dataset3.output_types)  # ==&gt; (tf.float32, (tf.float32, tf.int32))print(dataset3.output_shapes)  # ==&gt; "(10, ((), (100,)))"注：值得注意的是这部分可以看到，Dataset 中的每个样本，在 output_shapes 中被写成了列向量的形式。我们还能使用字典为每个组件命名，方法如下：dataset = tf.data.Dataset.from_tensor_slices(   {"a": tf.random_uniform([4]),    "b": tf.random_uniform([4, 100], maxval=100, dtype=tf.int32)})print(dataset.output_types)  # ==&gt; "{'a': tf.float32, 'b': tf.int32}"print(dataset.output_shapes)  # ==&gt; "{'a': (), 'b': (100,)}"使用迭代器我们有了数据的 Dataset 之后，下一步就是使用创建 Iterator 的方式来访问数据集的内容。Iterator 有以下四种类型  单次：dataset.make_one_shot_iterator()，这种迭代器是不能初始化的，仅支持对数据集进行一次迭代，不需要显示初始化。单次迭代器可以处理基于队列的现有输入管道支持的几乎所有情况，但它们不支持参数化。      dataset = tf.data.Dataset.range(100)  iterator = dataset.make_one_shot_iterator()  next_element = iterator.get_next()  sess = tf.Session()  for i in range(100):    value = sess.run(next_element)    assert i == value        注：是唯一易于与 Estimator 搭配使用的类型。    可初始化  迭代器中含有 Tensor 参数的时候，即数据集中含有 placeholder 时，需要显式调用 iterator.initializer 操作才能使用该迭代器。      max_value = tf.placeholder(tf.int64, shape=[])  dataset = tf.data.Dataset.range(max_value)  iterator = dataset.make_initializable_iterator()  next_element = iterator.get_next()  # Initialize an iterator over a dataset with 10 elements.  sess.run(iterator.initializer, feed_dict={max_value: 10})  for i in range(10):    value = sess.run(next_element)    assert i == value  # Initialize the same iterator over a dataset with 100 elements.  sess.run(iterator.initializer, feed_dict={max_value: 100})  for i in range(100):    value = sess.run(next_element)    assert i == value        可重新初始化  可馈送消耗迭代器中的值通过上一部分的例子我们就可以看出使用迭代器的值其实是这样一个过程：  通过 Iterator.get_next() 的方法返回 Tensor 对象，但是在 run 之前是不会运行的。  通过 Session.run() 来运行，这时候迭代器才会真正进入下一个状态。  当迭代器到达数据集的尾部的时候，会生成 tf.errors.OutofRangeError，之后迭代器将处于不可用状态，余姚重新进行初始化。这一步一般来说会封装在 try - except 结构中。保存迭代器状态tf.contrib.data.make_saveable_from_iterator 函数通过迭代器创建一个 SaveableObject，该对象可用于保存和恢复迭代器（实际上是整个输入管道）的当前状态。这样创建的可保存对象可以添加到 tf.train.Saver 变量列表或 tf.GraphKeys.SAVEABLE_OBJECTS集合中，以便采用与 tf.Variable 相同的方式进行保存和恢复。请参阅保存和恢复，详细了解如何保存和恢复变量。# Create saveable object from iterator.saveable = tf.contrib.data.make_saveable_from_iterator(iterator)# Save the iterator state by adding it to the saveable objects collection.tf.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable)saver = tf.train.Saver()with tf.Session() as sess:  if should_checkpoint:    saver.save(path_to_checkpoint)# Restore the iterator state.with tf.Session() as sess:  saver.restore(sess, path_to_checkpoint)结语这部分，主要摘抄自官方教程导入数据部分，不过由于教程比较全面，所以挑出了最重要的内容，做为自己的入门记忆部分。Dataset 与 Estimator，TFRecord 等文件格式的读取，做为接下来进阶部分的教程来介绍。简单的来说通过这部分的学习主要理解了，迭代器和数据集的工作流程。为接下来学习更加实际细节的操作打下基础。]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
      </categories>
      <tags>
        
          <tag> TensorFlow </tag>
        
          <tag> Dataset </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow 入门之Keras]]></title>
      <url>/tensorflow/2018/12/08/Tensorflow%E5%85%A5%E9%97%A8%E4%B9%8BKeras/</url>
      <content type="text"><![CDATA[Keras 官宣特征  简单快速的圆形部署  支持 CNN 和 RNN，也支持两者结合  同时支持 CPU 和 GPU 计算。Keras 设计理念  User friendliness, Keras is an API designed for human beings, not machines.  Modularity, a model is understood as a sequence or a graph of standalone, fully-configurable modules that can be plugged together with as few restrictions as possible.  Easy extensibility. New modules are simple to add (as new classes and functions), and existing modules provide ample examples.Hello World  导入 Keras     import tensorflow as tf from tensorflow import keras        模型的导入 在 Keras 中的模型被看作是一个 Sequential 模型，一个层组成的堆。     from keras.models import Sequential from keras.layers import Dense model = Sequential() # 实例化 equential # 构建模型 model.add(Dense(units=64, activation='relu', input_dim=100)) model.add(Dense(units=10, activation='softmax')) # 用 .compile() 设计得分函数和学习方式 model.compile(loss='categorical_crossentropy',         	  optimizer='sgd',         	  metrics=['accuracy']) model.compile(loss=keras.losses.categorical_crossentropy,         	  optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True)) # train model.fit(x_train, y_train, epochs=5, batch_size=32) model.train_on_batch(x_batch, y_batch) # val loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128) # 直接预测 classes = model.predict(x_test, batch_size=128)      序列模型简单的来说，Keras 就像搭积木一样以组合层的方式来构成图。最常用的为序列模型 tf.keras.Sequential。一般构建图的流程如下：  事例化一个Sequential：model = keras.Sequential()  往里面添加层：model.add(&lt;layers&gt;)配置层简单的层面来说我们可以使用很多预定制的 keras.layers。他们有一些统一的参数如下：  activation：设置层的激活函数。此参数由内置函数的名称指定，或指定为可调用对象。  kernel_initializer 和 bias_initializer：创建层权重（核和偏差）的初始化方案。此参数是一个名称或可调用对象，默认为 "Glorot uniform" 初始化器。  kernel_regularizer 和 bias_regularizer：应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。训练流程构建了模型之后，通过调用 compile 来配置模型的训练流程：model.compile(optimizer=tf.train.AdamOptimizer(0.001),              loss='categorical_crossentropy',              metrics=['accuracy'])tf.keras.Model.compule 有是那个重要参数：  optimizer：此对象会指定训练过程。从 tf.train 模块向它传递优化器实例如 AdamOptimizer、RMSPropOptimizer 或 GradientDescentOptimizer。  loss：值得在优化期间你是用什么得分函数。常见的有 mse、categorical_crossentropy 和 binary_crossentropy。损失函数可以在 tf.keras.losses模块来找。  metrics：用于监控训练。它在 tf.keras.metrics模块中找。# Configure a model for mean-squared error regression.model.compile(optimizer=tf.train.AdamOptimizer(0.01),              loss='mse',       # mean squared error              metrics=['mae'])  # mean absolute error# Configure a model for categorical classification.model.compile(optimizer=tf.train.RMSPropOptimizer(0.01),              loss=keras.losses.categorical_crossentropy,              metrics=[keras.metrics.categorical_accuracy])数据导入  NumPy 数据     import numpy as np data = np.random.random((1000, 32)) labels = np.random.random((1000, 10)) model.fit(data, label, epochs=10, batch_size=32)        tf.keras.Model.fit 有三个重要训练参数：          epochs：以周期为单位进行训练，即一个周期有多少个 epoch。      batch_size：模型会将输入的数据切分成较小的 batch，并在训练时迭代这些 batch。              validation_data：传递此参数（输入和标签元组）可以让该模型在每个周期结束时以推理模式显示所传递数据的损失和指标。简单的来说就是在每个周期结束的时候在验证集测试一遍。          import numpy as np  data = np.random.random((1000, 32))  labels = np.random.random((1000, 10))  val_data = np.random.random((100, 32))  val_labels = np.random.random((100, 10))  model.fit(data, labels, epochs=10, batch_size=32,            validation_data=(val_data, val_labels))                      tf.data 数据集 和 NumPy 数据传参数一样，但是不同的地方在于出现了一个 steps_per_epoch，这个参数的官方说明如下：          steps_per_epoch: Integer or None. Total number of steps (batches of samples) before declaring one epoch finished and starting the next epoch. When training with input tensors such as TensorFlow data tensors, the default None is equal to the number of samples in your dataset divided by the batch size, or 1 if that cannot be determined.        简单的来说就是，dataset 指定一个 epoch 有多少 batch，不指定的话或者 1 的话，就是所有的 batch。同时由于 Dateset 会生成批次数据，所以不需要指定 batch_size。     # Instantiates a toy dataset instance: dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32) dataset = dataset.repeat() # Don't forget to specify `steps_per_epoch` when calling `fit` on a dataset. model.fit(dataset, epochs=10, steps_per_epoch=30) # 用于验证 dataset = tf.data.Dataset.from_tensor_slices((data, labels)) dataset = dataset.batch(32).repeat() val_dataset = tf.data.Dataset.from_tensor_slices((val_data, val_labels)) val_dataset = val_dataset.batch(32).repeat() model.fit(dataset, epochs=10, steps_per_epoch=30,     	  validation_data=val_dataset,     	  validation_steps=3)      评估和预测评估是采用 tf.keras.Model.evaluate 和 tf.keras.Model.predict 方法，导入用来评估的数据依旧是 NumPy 和 tf.data.Dataset 都可以用。# 评估model.evaluate(x, y, batch_size=32)model.evaluate(dataset, steps=30)# 预测model.predict(x, batch_size=32)model.predict(dataset, steps=30)保存和恢复仅限权重使用 tf.keras.Model.save_weights 保存并加载模型的权重：# Save weights to a TensorFlow Checkpoint filemodel.save_weights('./my_model')# Restore the model's state,# this requires a model with the same architecture.model.load_weights('my_model')仅限配置可以保存模型的配置，此操作会对模型架构（不含任何权重）进行序列化。即使没有定义原始模型的代码，保存的配置也可以重新创建并初始化相同的模型。Keras 支持 JSON 和 YAML 序列化格式：# Serialize a model to JSON formatjson_string = model.to_json()# Recreate the model (freshly initialized)fresh_model = keras.models.from_json(json_string)# Serializes a model to YAML formatyaml_string = model.to_yaml()# Recreate the modelfresh_model = keras.models.from_yaml(yaml_string)注：子类化模型不可序列化，因为框架由 call 方法正文中的 python 代码定义。整个模型# Create a trivial modelmodel = keras.Sequential([  keras.layers.Dense(10, activation='softmax', input_shape=(32,)),  keras.layers.Dense(10, activation='softmax')])model.compile(optimizer='rmsprop',              loss='categorical_crossentropy',              metrics=['accuracy'])model.fit(data, targets, batch_size=32, epochs=5)# Save entire model to a HDF5 filemodel.save('my_model.h5')# Recreate the exact same model, including weights and optimizer.model = keras.models.load_model('my_model.h5')同 Estimator 联动Estimator API 用于针对分布式环境训练模型。它适用于一些行业使用场景，例如用大型数据集进行分布式训练并导出模型以用于生产。可以通过 tf.keras.estimator.model_to_estimator 将该模型转换为tf.estimator.Estimator 。model = keras.Sequential([layers.Dense(10,activation='softmax'),                          layers.Dense(10,activation='softmax')])model.compile(optimizer=tf.train.RMSPropOptimizer(0.001),              loss='categorical_crossentropy',              metrics=['accuracy'])estimator = keras.estimator.model_to_estimator(model)注：整体 Keras 的 Sequential 对 Estimator 支持的不太好，我在使用的时候只有以函数式编程的方式才顺利的进行了 Keras 到 Estimator的转换。多GPU支持tf.keras 模型可以使用 tf.contrib.distribute.DistributionStrategy 在多个 GPU 上运行。此 API 在多个 GPU 上提供分布式训练，几乎不需要更改现有代码。目前，tf.contrib.distribute.MirroredStrategy 是唯一受支持的分布策略。MirroredStrategy 通过在一台机器上使用规约在同步训练中进行图内复制。# 定义模型model = keras.Sequential()model.add(keras.layers.Dense(16, activation='relu', input_shape=(10,)))model.add(keras.layers.Dense(1, activation='sigmoid'))optimizer = tf.train.GradientDescentOptimizer(0.2)model.compile(loss='binary_crossentropy', optimizer=optimizer)model.summary()# 定义输入def input_fn():  x = np.random.random((1024, 10))  y = np.random.randint(2, size=(1024, 1))  x = tf.cast(x, tf.float32)  dataset = tf.data.Dataset.from_tensor_slices((x, y))  dataset = dataset.repeat(10)  dataset = dataset.batch(32)  return dataset# 定义策略strategy = tf.contrib.distribute.MirroredStrategy()config = tf.estimator.RunConfig(train_distribute=strategy)# 转换成 estimatorkeras_estimator = keras.estimator.model_to_estimator(  keras_model=model,  config=config,  model_dir='/tmp/model_dir')# 训练keras_estimator.train(input_fn=input_fn, steps=10)注意！注意！注意！：在用了 estimator 之后不会像 Keras 模型的 model.fit 那样有一个非常优雅的输出，它会没有输出！曾经新手的我，在这里浪费了快半天，只需要在代码的最开始加上 tf.logging.set_verbosity(tf.logging.INFO) 就可以输出了，默认是每 100 step 输出一次，可以手动在 RunConfig 中修改。更多详细参考这里的最后一章节。]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
      </categories>
      <tags>
        
          <tag> TensorFlow </tag>
        
          <tag> Keras </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow入门之Estimator]]></title>
      <url>/tensorflow/2018/12/08/Tensorflow%E5%85%A5%E9%97%A8%E4%B9%8BEstimator/</url>
      <content type="text"><![CDATA[Estimator 作用简介Estimator 使用来简化机器学习训练、评估、预测的一个高阶 TensorFlow API。我们可以使用预创建的 Estimator，也可以自己编写自定义的 Estimator，但是所有的 Estimator 都是基于 tf.estimator.Estimator 类的类。Why Estimator  可以在本地主机上或分布式多服务器环境中运行基于 Estimator 的模型，而无需更改模型。  使用高级直观代码开发先进的模型。简言之，采用 Estimator 创建模型通常比采用低阶 TensorFlow API 更简单。  Estimator 本身在 tf.layers 之上构建而成，可以简化自定义过程。  Estimator 会自动构建图。  Estimator 提供安全的分布式训练循环，可以控制如何以及何时：          构建图      初始化变量      开始排队      处理异常      创建检查点文件并从故障中恢复      保存 TensorBoard 的摘要不过在使用 Estimator 编写应用时，我们必须将数据输入管道从模型中分离出来，而这种分离会方便我们以后在不同数据集上部署实验。      使用已经封装好的 Estimator 模型  编写一个或多个数据导入函数。 官方推荐我们可以创建两个函数来导入数据，一个用来导入训练集，一个用来导入测试集。函数输出要求如下两点：          一个字典，其中 key 是特征名称，value 是包含相应特征数据的张量      一个包含一个或多个标签的张量         def input_fn(dataset):     ... # manipulate dataset, extracting the feature dict and the label     return feature_dict, label        定义特征列 每个 tf.feature_column 都标识了特征名称、特征类型和任何输入预处理操作。     # Define three numeric feature columns. population = tf.feature_column.numeric_column('population') crime_rate = tf.feature_column.numeric_column('crime_rate') median_education = tf.feature_column.numeric_column('median_education',                 	normalizer_fn=lambda x: x - global_education_mean)	        注：第三个特征中定义了一个匿名函数，用来调节原始数据    实例化 Estimator     # Instantiate an estimator, passing the feature columns. estimator = tf.estimator.LinearClassifier(     feature_columns=[population, crime_rate, median_education],     )        使用模型进行训练、评估或推理方法     # my_training_set is the function created in Step 1 estimator.train(input_fn=my_training_set, steps=2000)      自定义 Estimator 模型研究用 Tensorflow 大概率上不会有预定义好的 Estimator 所以我们还是需要了解如何写模型函数。由于我们需要自己写模型，所以我们可能需要自行实现包含但不仅仅包含多GPU并行计算等功能。官方推荐的工作流如下：  假设存在合适的预创建的 Estimator，使用它构建第一个模型并使用其结果确定基准。  使用此预创建的 Estimator 构建和测试整体管道，包括数据的完整性和可靠性。  如果存在其他合适的预创建的 Estimator，则运行实验来确定哪个预创建的 Estimator 效果最好。  可以通过构建自定义 Estimator 进一步改进模型。显然这一部分只有第四个符合我们的需求，当然我们也可以用 1-3 部分来检验读入的数据是否完整等。至于模型部分的构建我们可以通过直接创建自定义模型和使用 Keras 构建模型之后转换成 Estimator 这两种方式来构建。前者会在之后的进阶Estimator 笔记中总结，而后者已经在 Keras 入门阶段进行了介绍。参考文献  Tensorflow官方教程 - Estimator]]></content>
      <categories>
        
          <category> Tensorflow </category>
        
      </categories>
      <tags>
        
          <tag> TensorFlow </tag>
        
          <tag> Estimator </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[IPdb 的简单实用说明]]></title>
      <url>/python/2018/12/08/IPdb%E7%9A%84%E7%AE%80%E5%8D%95%E5%AE%9E%E7%94%A8%E8%AF%B4%E6%98%8E/</url>
      <content type="text"><![CDATA[简介IPdb 是一个 python 用的 “gdb” 超级好用，如果你的编辑环境中懒得装 IDE，或者工作环境没有图形化界面的话，这个工具简直就是为你量身定制的。我已经把这个工具融入到了自己的 VIM 脚本中，成为日常码农生活的一份子。安装pip3 install ipdb在源码中使用import ipdb# codingx = 10ipdb.set_trace()y = 20# coding程序会在 x = 10 后终止，这个在 Ipython 中还是挺好用的。命令行这个也是我想要的，我想在 VIM 中结合进这种方式来调试，我希望有一种能变执行边调试的工具，就和 gdb 一样。python3 -m ipdb your_code.py这行指令就让你进入了调试环境。Happy debugging。调试命令  h 会列出 IPDB 支持的指令，help [command] 可以查看每个操作的用处。  whatis [variable_name]：查看变量属性  where：输出当前位置，和上下文的代码  p：打印变量值  a：打印传入函数的值  n：下一步  c：运行直到断点或结束  b [line_number]：在某行设置断点参考   使用IPDB调试Python代码 ]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tensorflow 多GPU并行计算报错]]></title>
      <url>/troubleshoot/2018/12/07/Tensorflow%E5%A4%9AGPU%E5%B9%B6%E8%A1%8C%E8%AE%A1%E7%AE%97%E6%8A%A5%E9%94%99/</url>
      <content type="text"><![CDATA[问题分析在 Keras 中配置多 GPU 并行计算的时候，报错 libnccl.so.2 not found。简单的来说就是 Tensorflow 多 GPU 运行的话使用了一个 NCCL 2.x 的库，但是不是默认安装的，同时又由于NCCL 2.x是动态加载的，因此不会影响不调用NCCL原语的程序，也就是说直到你第一次尝试多 GPU 为止都不会发现这个问题。解决解决的方式当然是相当简单咯，安装就可以了，你可以选择从官网安装，也可以用如下操作来偷懒。注：也有可能直接 sudo apt-get install libnccl2 就可以直接安装。sudo apt-key adv --fetch-keys http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/7fa2af80.pubwget http://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64/cuda-repo-ubuntu1604_9.1.85-1_amd64.debwget http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64/nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb sudo dpkg -i cuda-repo-ubuntu1604_9.1.85-1_amd64.deb sudo dpkg -i nvidia-machine-learning-repo-ubuntu1604_1.0.0-1_amd64.deb sudo apt-get updatesudo apt-get install libnccl2成功安装后，再次跑程序试试，不出意外就会成功啦。最后祝大家 Happy Coding。]]></content>
      <categories>
        
          <category> TroubleShoot </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> TroubleShoot </tag>
        
          <tag> Tensorflow </tag>
        
          <tag> 多GPU </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[SSH 的使用]]></title>
      <url>/linux/2018/12/03/SSH%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      <content type="text"><![CDATA[服务球端安装 ssh  测试服务器是否安装了 ssh     ps -e | grep ssh # output # 存在 ssh-agent 说明有 ssh 客户端 # 存在 sshd 说明有 ssh 服务器端        安装 ssh     # ssh-client sudo apt-get install openssh-client # ssh-server sudo apt-get install openssh-server        更换 ssh 服务默认端口          使用 vim 打开配置文件 /etc/ssh/sshd_config，搜索 port 更改端口号即可。          sudo /etc/init.d/ssh restart # 使配置生效                用自己想用的用户名登陆              在~/.ssh中创建一个文件config  在里面输入    Host *    User root        重启终端即可SCPscp -r /local/path username@server_ip:/server/pathscp -r username@server_ip:/server/path /local/path scp 跨机远程拷贝免密登陆  在本机生成公钥、私钥对     ssh-keygen # 会生成 id_rsa.pub 到 ~/.ssh 里面        将 id_rsa.pub 传到服务器端     scp id_rsa.pub your_username@ip:~/home/your_username        服务器端创建 authorized_keys     touch ~/.ssh/authorized_keys sudo chmod 600 ~/.ssh/authorized_keys        追加公钥到 authorized_keys 文件中     cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys      ]]></content>
      <categories>
        
          <category> Linux </category>
        
      </categories>
      <tags>
        
          <tag> ssh </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[为Python执行脚本加可用参数和提示]]></title>
      <url>/python/2018/12/01/%E4%B8%BAPython%E6%89%A7%E8%A1%8C%E8%84%9A%E6%9C%AC%E5%8A%A0%E5%8F%AF%E7%94%A8%E5%8F%82%E6%95%B0%E5%92%8C%E6%8F%90%E7%A4%BA/</url>
      <content type="text"><![CDATA[简介我们经常希望自己的脚本输入能变得“优雅”一点，所以让脚本能支持参数指定就很重要了。这里就记录下如何让自己的脚本支持带提示的参数的输入。教程  引入相关包     import argparse        使用相关操作     parser = argparse.ArgumnetParser(description='manual to this script') parser.add_argument('--file_path', type=str, default='') args = parser.parse_args() # use args file_path = args.file_path      ]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[pip 主题]]></title>
      <url>/python/2018/12/01/pip%E4%B8%BB%E9%A2%98/</url>
      <content type="text"><![CDATA[pip 的使用  安装第三方库     pip install packageName        卸载     pip uninstall packageName        检测最新包 &amp; 升级包     pip list --outdated packageName pip install --upgrade packageName pip3 list --outdated | grep -e '^[a-z]' | cut -d ' ' -f 1 | xargs -n1 pip3 install -U # 小 trick 一键升级所有的包        显示所有已经安装的包名     pip freeze pip list      pip 换源目的  解决 pip 因为 GFW 的原因速度慢，不稳定的问题    步骤    创建~/.pip/pip.config     mkdir ~/.pip vim ~/.pip/pip.config        编辑 config 文件，输入如下     [global] index-url = https://pypi.doubanio.com/simple/ timeout = 1000 [install] use-mirrors = true mirrors = https://pypi.doubanio.com//        重启终端即可环境快速安装  首先我们要有一个存有需求库的文件          文件的中需求库的表达格式          Flask==0.10.1 # 这种格式显然使用 freeze 导出的                    生成需求库文件  很简单，就是用输出重定向就好啦，重定向相关知识可以在我的 shell 相关教程中查看。          pip freeze &gt; requirement  pip3 freeze &gt; requirement                      使用 pip 安装相关文件     pip install -r [filename]        结合 virualenv 和 pipgreqs 安装和项目相关的库文件 这种方法相当推荐，在网上也很少见到有大佬写这部分的文章。这里就贴出凝雨大佬的文章作为日后查看的索引。          virtualenv      pipreqs      参考文献   豆瓣用户“纠要改叫沉冷静”的文章  好看的大佬“凝雨”的博客]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[GTD 使用方法]]></title>
      <url>/%E6%9D%82%E9%A1%B9/2018/11/27/GTD%E4%BD%BF%E7%94%A8%E6%96%B9%E6%B3%95/</url>
      <content type="text"><![CDATA[							GTD 使用方法	前言自从进入了研究生以后，时间就开始严重不够用了。虽然已经把玩游戏，刷微博看八卦这种纯粹浪费时间的事情全部去掉了，但是每天仍然被做不完的事情所充斥着。陪家人和女票的时间变得非常的少，同时觉得获得的知识摄入量也没有因此大量提升，个人觉得自己进入了吴军老师说的假忙碌状态。于是总结了以下两点原因：	事情安排出了问题		做了大量不重要但紧急的事情，导致重要不紧急的事情没有做，最终导致 deadline的时候，被各种重要紧急的事情压破，紧张感导致了工作效率的下降，最终导致恶性循环。	个人能力问题		时间的掌控，和身体的健康状况。个人不能做到到了指定的时间就立刻停止手头的事情。总会长一点占用两个事件之间的休息时间，直接导致下一个任务过度劳累，效率下降。同时多次因为各种“事情”压迫了自己读书和健身的时间，这点长远来看是非常不好的。GTD 时间管理为了解决这两个问题，开始使用得到《五分钟商学院》中学到的一个GTD时间管理法，通过约束一天的事情总量，和排空大脑记录要做的事情来改善目前的现状。GTD的工作流如下：	早晨：查看当天要做的内容 -&gt; 在 Things 这个软件中的今天栏目（一件一件往下做）		原则为：					这个列表只能添加，不能重新回到其他列表			晚上：查看当天完成的内容，并准备第二天的内容 -&gt; 在 Things 的 AnyTime 栏目中选择任务		原则为：					选择 Sometime 列表中部分内容加入当天列表			查看当天任务完成情况，包含不同实际用时和预测用时番茄数的差别，查看当天的时间分配是否合理			其他时间：快速添加脑子中闪过要做的事情添加到收集栏目中 -&gt; 通过手机或电脑添加任务到 things 收件箱中		原则为：					快速，准确的描述想要做的任务即可			在等电梯等零碎时间，查看收件箱的内容对收件箱进行如下6个操作									删除：一时冲动加进来的，现在看起来毫无价值					归档：有价值的资料，比如好的微信文章，笔记等，加入到相应的 Ulysses 目录中					将来 / 可能：这些事情将来可能去做，但不会现在做，就加入到 Things 的 Sometime 中，每天晚上看看要不要加入到当天列表。					等待。这件事情需要排别人完成，就立刻去做，然后加上一个时间提醒（Things 会自动移出 Inbox）。					下一步行动。如果需要亲自完成，若为2分钟以内能做完的，立刻做完，若两分钟以上，移到当天要做的任务中					下一步行动中很复杂的任务，对其建立项目，之后定期回顾处理						其他注意事项	注意自己的时间颗粒度，即分配时间的时间块，从半小时做起，慢慢减少。	注意自己的时间中，学习，生活，身体管理，这三方面的平衡。	注意自己在每个时间块中间的时间利用率]]></content>
      <categories>
        
          <category> 杂项 </category>
        
      </categories>
      <tags>
        
          <tag> GTD </tag>
        
          <tag> 效率 </tag>
        
          <tag> 生活 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n 笔记07 TensorFlow 入门]]></title>
      <url>/nlp/2018/11/20/CS224n%E7%AC%94%E8%AE%B007TensorFlow%E5%85%A5%E9%97%A8/</url>
      <content type="text"><![CDATA[简介这节课主要的内容其实就是简单的介绍了 TensorFlow 的一些基本理念，和一些简单的用法，并用半节课的时间实现了之前我们做过的实验 Skipgram。什么是 Tensorflow  是一个来自 Google Brain Team 的开源软件  是以流图的方式进行数值计算的库  用于部署机器学习算法，并执行算法其他的话：现在学术界很多老师是相当不喜欢 Tensorflow 的，他们更加推崇 Theano 或者 Pytorch，不过 Tensorflow 不论是在 Github 的 Star 数量还是在论文中再现算法的使用量上都占据着比较大的优势。TensorFlow 的基本使用前言这一部分，我将简单的讲课堂上的内容做以笔记，由于今后会大量的使用 Tensorflow，所以之后会另其栏目专门记录学习 TensorFlow 以及其源码的笔记。TensorFlow 的理念  表达计算流程的方式是一个图  图的节点代表的是计算（operations）  图的边代表的是张量（tensors），可以简单理解为多维数组。     简单使用  一、Variable &amp; Placeholder  Variable Variable 是保存图的运算状态的，也就是说模型会保留这些变量，不会删除他们，同时在反向传播中会更新他们。更加具体的来讲，他们就是在我们下载别人训练好的模型时的参数，类似于算法模型这个骨架内的血肉。     tf.Variable(tf.zeros((N,))) # 使用有点像numpy        Placeholder Placeholder 在中文的教程中很多称之为占位符，他们和 Variable 的不同在于两点：          定义的时候不需要指定初始值，它只起占位作用      在每个节点运算结束后不会保留或更改它的值 鉴于它的特性，我们很容易想到，Placeholder 的主要作用在于用来存储数据集和标签等操作。它是在运行模型的时候，即执行 session 的时候从外部数据导入的。         tf.placeholder(data_type, shape) # eg input_placeholder = tf.placeholder(tf.float32, shape=(batch_size, n_features))                      Operations 我们现在有了变量，那么我们就可以使用他们来计算了。TensorFlow 中有很多计算模块，我们可以十分直观的进行使用，方法如下：     tf.matmul(x, W) tf.nn.relu(tf.matmul(x, W) + b)        注：值得注意的是这里的 + 仍然指代的是 tensorflow 的加法操作，在 tensorflow 中对于直接用符号+-*/ 的加减乘除操作都会自动转换乘相应的 tensorflow 操作。    描述图运算  需要强调下，在这里讲课的清秀小哥说，这里的 h 和最终的那个 ReLu 操作的计算单元其实是一样的。这里需要特别注意，因为之后我们用 session 其实本质来讲就是通过类似这种 h 来定位到计算节点来获得输出的。二、Session之前我们其实是画出了一个图，我觉得更准确的说法是之前画的是图纸，但是并没有根据图纸进行施工。而 Session 就是做这样一个工作的。那么第一个问题就 session 调用的图在哪里，其实是有一个默认的 graph 的。我们可以通过 tf.get_default_graph() 得到。其中值得注意的是，在斯坦福的 TensorFlow 课中，讲师特别强调不建议自己定义多个图，使用默认的图就好了1。使用 session 运行图的步骤：  创建一个 session     sess = tf.Session()        初始化变量     sess.run(tf,initialize_all_variables()) # 后来清秀小哥使用的方法是 init= tf.global_variables_initializer() sess.run(init)        运算     sess.run(fetches, feeds)        feches 是图的节点，由于图是一个连通图，所以通过一个节点总能找到所有的节点。而 feeds 则指的是 placeholder 和喂入的数据之间的映射关系。   三、训练神经网络之前我们可以说是定义了一个图，但是没有定义如何训练它，接下来是将训练部分的内容。  定义损失函数 tensorflow 中有很多 API 来定义损失函数，下面是一个例子。     prediction = tf.nn.softmax(...)  #Output of neural network label = tf.placeholder(tf.float32, [100, 10])							 cross_entropy = -tf.reduce_sum(label * tf.log(prediction), axis=1)            规定如何计算梯度 我们初始化一个 optimizer， 然后在调用 tensorflow 来执行一些梯度下降算法等。由于我们之前学到梯度下降算法是遵循链式法则的，所以每个节点就计算自己的损失就好了，这也是为什么这个库选择使用数据流图来设计的原因之一。    就像大多特定的函数都具有自己特定的导数一样，图中运算节点都附带了梯度操作。在反向传播中，用户不需要编写梯度计算与参数更新的代码，而是交给 optimizer 自动完成。    调用方法如下：     train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)        训练 每次执行梯度下降就是一次 epoch，所以简单的来说就是用 session 调用梯度下降的这个计算单元就好了。 四、变量共享当我们使用不同机子不同GPU来训练同一个模型的时候，我们需要共享一些变量。 TF 给出的解决方案是，使用字典建立变量名到 Variable 的映射：variables_dict = {	"weights": tf.Variable(tf.random_normal([782, 100]),						 	name="weights"							),	"biases": tf.Variable(tf.zeros([100]), name="biases")}有了变量名，那么肯定就有命名空间咯，使用方法如下：with tf.variable_scope("foo"):    v = tf.get_variable("v", shape=[1])  # v.name == "foo/v:0"with tf.variable_scope("foo", reuse=True):    v1 = tf.get_variable("v"                         # Shared variable found!                         ) )with tf.variable_scope("foo", reuse=False):    v1 = tf.get_variable("v"# CRASH foo/v:0 already exists!参考资料  课程课件链接  课程视频链接  大佬的博客笔记            https://github.com/learning511/cs224n-learning-camp/blob/master/cs20is/1-graphs%20and%20Sessions.pdf&nbsp;&#8617;      ]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
          <tag> TensorFlow </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[troubleshoot] 2080/2080ti显卡驱动安装(ubuntu)]]></title>
      <url>/troubleshoot/2018/11/20/2080_2080ti%E6%98%BE%E5%8D%A1%E9%A9%B1%E5%8A%A8%E5%AE%89%E8%A3%85(ubuntu)/</url>
      <content type="text"><![CDATA[[troubleshoot] 2080/2080ti显卡驱动安装(ubuntu)问题描述使用 ubuntu 自动识别的显卡驱动版本不支持 2080 和 2080ti 显卡。所以需要手动的去官网下载对应显卡驱动，进行手动安装。安装步骤准备安装包清单如下：  nvidia 驱动：官网链接  使用命令行直接下载      wget -c http://cn.download.nvidia.com/XFree86/Linux-x86_64/410.78/NVIDIA-Linux-x86_64-410.78.run        禁用系统默认显卡驱动          修改/etc/modprobe.d/blacklsit.conf 文件，加入如下内容          # for nvidia display device install  blacklist vga16fb  blacklist nouveau  blacklist rivafb  blacklist rivatv  blacklist nvidiafb                    更新 initramfs：          sudo update-initramfs -u  sudo reboot # 重启电脑                    查看是否禁用成功          lsmod | grep nouveau  # 没有输出就是成功了                      安装显卡驱动          卸载旧有驱动          # 方法一  sudo apt-get --purge remove nvidia-*   # 需要切换到bash下，zsh下不支持这个*的操作  # 方法二  sudo ./NVIDIA-Linux-x86_64-410.78.run -uninstall  # 个人情况不能识别 ubuntu 自动安装的驱动，需要用方法一                    安装驱动  首先，使用ctrl + alt + F3切换到命令行界面，在图形界面下安装显卡驱动会失败。          sudo chmod +x NVIDIA-Linux-x86_64-410.78.run  sudo ./NVIDIA-Linux-x86_64-410.78.run                注 1：安装过程中会提示 pre-install script failed 可以不用理继续安装。  注 2: 安装完成后，之前装的 cuda 和 cudnn 可以不用重新安装，还是可以使用的。            更新内核          sudo update-initramfs -u  sudo reboot now                    其他注意事项  采用这种方法安装的驱动，每次内核更新后，都要按照上面的方法重新手动安装一边才能启用新的驱动。  目前默认系统在软件更新中的显卡驱动还未支持 2080 和 2080ti参考  Ubuntu18.04 安装 RTX 2080Ti显卡驱动  显卡驱动安装步骤（这个教程里让我知道了一定要推出图形界面才能安装）]]></content>
      <categories>
        
          <category> TroubleShoot </category>
        
      </categories>
      <tags>
        
          <tag> Ubuntu </tag>
        
          <tag> TroubleShoot </tag>
        
          <tag> ‘NVIDIA' </tag>
        
          <tag> 显卡驱动 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[安装配置 Oh my Zsh]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/11/15/config_Oh_my-Zsh/</url>
      <content type="text"><![CDATA[Why Zsh  好看！  有git实时显示简单直白粗暴、上述亮点就是我换oh my zsh的初衷。用了它直接免去了很多的重复操作、让终端更美观、让生活更美好。安装  Talk is Cheap Show me the Codegit clone git://github.com/robbyrussell/oh-my-zsh.git ~/.oh-my-zsh首先git到～/.oh-my-zsh目录下。cp ~/.oh-my-zsh/templates/zshrc.zsh-template ~/.zshrc然后用模版设置配置文件。chsh -s /bin/zsh之后、切换默认打开的终端指令。如果想换回bash可以用chsh -s /bin/bash换回来ZSH_THEME="ys"最后是换主题，见下一章。更换Powerline主题  git下powerline的主题。     git clone git://github.com/jeremyFreeAgent/oh-my-zsh-powerline-theme         随后，./install.sh就好了。    安装字体。     git clone https://github.com/powerline/fonts.git        同理，./install.sh就可以安装号字体了。    终端下设置字体为Meslo LG M for powerLine。  在zshrc里设置主题     ZSH_THEME="powerline"       更换个人的主题  将自己的主题文件（file.theme）放入~/.oh-my-zsh/themes 文件夹中  在~/.zshrc 中 ZSH_THEME 选择自己的主题。[Plugin] Zsh-syntax-highlighting可以语法高亮你的 shell 命令，重要的是会标出你输入错误的指令。  从 git 上下载到指定位置     git clone https://github.com/zsh-users/zsh-syntax-highlighting ~/.oh-my-zsh/custom/plugins/zsh-syntax-highlighting        添加到 zshrc 中的 plugin list 它的名字为 zsh-syntax-highlighting，添加到plugins中就好了[Plugin] Zsh-autosuggestions它会根据你的指令输入记录来补全你的指令，非常有用。  从 git 上下载     git clone https://github.com/zsh-users/zsh-autosuggestions ~/.oh-my-zsh/custom/plugins/zsh-autosuggestions        添加到 zshrc 中的 plugin list 它的名字为 zsh-autosuggestions，添加到plugins中就好了    参考    本文主要参考为bo_song的文章，里面还有详细的配色可以去关注一下～  ]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> oh_my_zsh </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[《AI 未来》 - 李开复]]></title>
      <url>/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/11/14/AI-%E6%9C%AA%E6%9D%A5-%E6%9D%8E%E5%BC%80%E5%A4%8D/</url>
      <content type="text"><![CDATA[简介简单的来说全书是这样的一个逻辑，首先在书中一开始，说明了这次的人工智能和之前的深蓝这种专家系统1有着本质的区别，它跳出了人用自己的知识告诉机器应该怎么办这种固有的思维框架，而“放手”设计结构让机器自行去学习。在接下来的章节中，开复指出了接下来的人工智能的主要的两个竞争国家为中国和美国，两者有着不同的优势。美国有着领先数年的技术，而中国有着实干的精神。这点，开复认为现在是从科学落地的实干年代，也就是说他认为中国可能在这里更具有一些优势。之后，在第三章中着重强调了中国的特殊性，一方面开复承认了中国对“剽窃”的宽容，并允许企业间进行一些比较“残忍”的竞争，不过这样激烈的竞争也让创业者被迫进行不停的创新和改进，另一方面开复也指出在政府层面上，中国有着美国不同的优势，就是政府能够通过政府设置的国家队风投进行引导投资来加速技术的落地。除此之外，在这些章节中，开复也指出了，人工智能的时代，是一个赢者通吃的时代，成功的企业在数据，技术，资金，人才会达成“飞轮效应”，形成良性循环，最终财富快速积累到少数精英的手上。这也引出了书中最后一部分的话题，人工智能引起的社会分层和失业引起的社会动荡问题。在最后的几个章节中，开复在书中提到自己的患癌症的经历，所以他强调 AI 和人最大的不同在于人有感情，而 AI 没有。也就是说，开复在最后一章中提到的种种措施如 UBI，和政府层面支持为社会贡献的行为的方案，都是认为人有人应该做的，且擅长做的事情，而重复的机械的工作就应该交给机械，或者说 AI 来做。对个人的收获这本书对个人的收获在于，让我去回想人为什么之所以为人，在于他有感情，我们应该对身边帮助过自己的人心存感激，而不是和机器一样每日斤斤计较自己时间，以求单位时间利益最大化，但是一定程度的追求单位时间利益最大化还是必须的。除此之外，在最后一章说的，“让我们选择让机器当机器，让人当人”让我有很多思考。一个人应该做的是一些需要动用脑力，有创造性的工作，而不是一些机械性，重复性的工作，如不断的重复不加思考的使用别人开发的模块，做一个脚本小子或者调包侠。也许可以以这个为出发点，使用吴军老师说的生活减法，生活会变得更加美好。当然我也是很期待什么时候 AI 能做到自动 Debug（Facebook 已经在开发了），自动调参。如果 AI 能大幅度缩减这些时间，给我们人类更多的时间去思索，去创造，就像当年的雅典，他们统治着大量的奴隶来为他们提供物质保障，而雅典公民则做一些更加抽象的思考，来生产文明。不知道未来世界会变成什么样子，真的是非常期待了。            https://zh.wikipedia.org/zh-hans/专家系统&nbsp;&#8617;      ]]></content>
      <categories>
        
          <category> 读书笔记 </category>
        
      </categories>
      <tags>
        
          <tag> Reading </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n 笔记05 反向传播与项目指导]]></title>
      <url>/nlp/2018/11/13/CS224n-%E7%AC%94%E8%AE%B005-%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E4%B8%8E%E9%A1%B9%E7%9B%AE%E6%8C%87%E5%AF%BC/</url>
      <content type="text"><![CDATA[这次的课比较轻松，简单的来说就是链式法则，帅小哥为了让学生更能理解所以用了不同的方式来讲解这个 BP 的推导。流程图的形式的推导对之后的 Tensorflow 的直观理解很有帮助，不过这里就只保留 BP 推导后的一些经验的总结。为什么要推导 BP 算法呢  真正理解神经网络背后的数学 - 反向传播算法  Backprop can be an imperfect abstraction - BP 还是不完善的          如会出现梯度消失、梯度爆炸等        当你 Debug 模型，和设计新模型的时候会需要懂它需要记忆的内容BP 整体就是链式法则的推导，但是作为以后可能会经常使用的数学工具，我们希望有一些经验性的总结来方便我们之后的计算。  第 $l$ 层的残差：          $\delta^{(l)} = (W^{(l)T}\delta^{(l+1)}) \odot f’(z^{(l)})$  其中残差为根据某个损失函数得到的误差，而 $\odot$ 为相同大小的向量之间的 element wise product。同时 $f$ 为激活函数， $z$ 为线性函数 。        对第 $l$ 层的权值矩阵的梯度：          $\frac{\partial E_R}{\partial W^{(l)}} = \delta^{(l+1)}(a^{(l+1)})^T + \lambda W^{(l)}$        对第 $l$ 层的偏置的梯度：          $\frac{\partial E_R}{\partial b^{(l)}} = \delta^{(l+1)} + \lambda b^{(l)}$        注：其中 $a$ 是激活值： $a^{(l)} = f(z^{(l)})$  BP 的细节理解除了通用的数学推导，帅小哥还是用了另外三种方式来推理。其中个人感觉流程图中的介绍更适用于理解画出来的神经网络，而电路的模式更接近 tensorflow 的编程思想。具体的内容就不赘述了，内容对之后反过来查找回忆帮助不大。感兴趣的读者可以参考大佬的这一章笔记的博客。课程项目(直接转自大佬的总结)  不要想着一上来就发明个新模型搞个大新闻  也不要浪费大部分时间在爬虫上面，本末倒置  把旧模型用于新领域\新数据也是不错的项目  先要按部就班地熟悉数据、熟悉评测标准、实现基线方法  再根据基线方法的不足之处思考深度学习如何能带来改进  再实现一个已有的较为前沿的模型  观察该模型犯的错误，思考如何改进  这时才能没准就福至心灵发明一个新方法参考  大佬的博客  课程第五课]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[MacOS 制作Ubuntu安装U盘]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/11/10/MacOS-%E5%88%B6%E4%BD%9CUbuntu%E5%AE%89%E8%A3%85U%E7%9B%98/</url>
      <content type="text"><![CDATA[							MacOS 制作Ubuntu安装U盘	一般而言我们在 windows 下使用 UtraISO 能很容易做出一个 linux 的安装U盘。但是在 MacOS 下应该怎么做呢？答案是万能的命令行！操作步骤	使用 hdituilhdiutil convert -format UDRW -o &lt;output_filename&gt; &lt;your_isoFile&gt;		我们使用 hdituil 对下载的 ISO 文件转换成 dmg 文件。其中 -format UDRW，代表着转换成具有读写权限的镜像。	使用 diskutil 卸载U盘diskutil list # 查看哪个是我们要卸载的U盘diskutil unmountDisk &lt;TheDisk&gt; # The Disk 代表着我们要选的U盘 如 /dev/disk2	使用 dd 命令					if 输入的文件路径			of 输出的文件路径			bs 块大小，一般使用 1m		mv &lt;your dmg&gt; &lt;iso&gt; # 将之前的转换成的dmg文件重命名成iso文件sudo dd if=/your/iso/path of=/your/disk/path bs=1m# 注意这里的disk path需要前面加r，比如/dev/disk2 -&gt; /dev/rdisk2	重要数据销毁sudo dd if=/dev/urandom of=/your/disk/path		用随机数填充U盘，来彻底销毁数据。]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> Linux 安装 </tag>
        
          <tag> 技巧 </tag>
        
          <tag> Mac </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Git 操作总结]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/11/09/Git-%E5%9F%BA%E7%A1%80%E6%80%BB%E7%BB%93/</url>
      <content type="text"><![CDATA[									Git 操作总结	基础操作	git init：将文件目录转为 git 仓库	git add &lt;filename&gt;：将文件存入暂存区	git push：将修改同步到远端仓库	git pull：将远端仓库的新修改同步到本地	git diff &lt;filename&gt;：查看工作区的修改了哪些内容，没有 add 的文件是不在 diff 的作用范围的。	git commit -m &lt;msg&gt;：将暂存区的内容提交到仓库，commit 后会得到干净工作区。	git status：查看工作区的状态	git log [--graph]：查看记录	git tag：查看标签Git 结构说明	工作区		工作区可以理解为咱们存放自己代码的文件目录。	版本库		工作区中含有一个隐藏的目录 .git ， 这个就是 Git 的版本哭。里面有很多东西，我们的 Git 中的很多历史信息都是存在这里，版本回溯也是靠着里面的信息做到的。	暂存区		暂存区，可以理解为我们从工作区将代码移入“分支”前的中转站，在这里我们可以进行一些检查工作。忽略特殊文件Mac 下有 .DS_Store，运行 python 有 .pyc文件，make 有很多中间生成的文件，这些都是我们不想上传到仓库的文件，.gitignore 就是让我们配置哪些文件不用上传的位置。	配置方式		其实就是将不想上传的文件放在里面就好了，可以使用一些正则语法如*，[]等等。同时 Github 为我们准备了一些常用的忽略列表可以直接使用。# Mac.DS_Store# Python*.py[cod]	强制添加文件		git add -f &lt;file_name&gt;：强制添加忽略规则内的文件。	查看文件为什么没有添加		git check-ignore -v &lt;file_name&gt;：会输出到底是哪个规则忽略了这个文件。版本回退	git log/reflog：查看当前位置之前/之后的 commit	git reset --hard HEAD^/&lt;commitID&gt;：回退到上一步 / 某一个 commit 的状态	git checkout -- &lt;filename&gt;：退回到 commit 或 add 时的状态。这个指令也可以恢复删掉的文件。		注：如果没有 -- 就成了切换分支指令了	git reset HEAD &lt;filename&gt;：回退到提交之前最新的状态标签管理	git tag &lt;tag_name&gt;：打开一个标签	git tag：查看所有标签	git tag -a &lt;tag_name&gt; -m &lt;description&gt; &lt;commit_id&gt;：tag 是针对 commit 的，也就是说这次 commit 出现的分支上都可以看到这个标签。	git show &lt;tag_name&gt;：查看 tag 的详细信息。	git push origin &lt;tag_name&gt;：将 tag 推送到远程服务器，tag 不会自动推送到远程。	git tag -d &lt;tag_name&gt;：本地删除 tag	git push origin :refs/tags/&lt;tag_name&gt;：本地删除后，让远程仓库端也删除标签远程仓库首先要注意的是，远程仓库和本地的仓库是不同的仓库，这个概念在心里要有。	git remote -v：查看有哪些远程仓库，-v 是更加详细显示。	git remote add &lt;name&gt; &lt;git_url&gt;：同远程仓库关联，一般 name 为 origin。	git push -u origin master：第一次提交会将本地master 分支和远程的 master 分支关联起来。	git branch --set-upstream-to=origin/&lt;branch_name&gt; &lt;local_branch_name&gt;：当不能从远程仓库 pull 的时候，根据提示设置本地分支和远程分支的链接。	git rebase：正常 merge 后，git log --graph 展现的会出现分叉，有的人觉得这样子对比不直观，不想有这些分叉，于是就出现了这个 rebase。分支管理这里要注意的是，我们对 git 仓库的操作可以想象是我们操作的是在操作一个链表上的指针。	git branch -a：查看当前仓库有什么分支。	git checkout -b &lt;new_branch_name&gt;：新建一个分支，相当于 git branch &lt;new_branch_name&gt; 之后再 git checkout &lt;new_branch_name&gt;。	git merge [--no-ff -m &quot;msg&quot;] &lt;branch_name&gt;：将别的分支合并到当前分支。默认时 Fast forward 模式，也就是说这种模式下删除分支后会丢掉分支信息，--no-ff 取消了Fast forward。	git branch -d/D &lt;branch_name&gt;：删除/强制删除某个分支Git 开发分支思路	master：应该非常稳定，仅用来发布新版本，平时不在上面进行操作，需要关联到远程仓库。	dev：这个是真正干活的地方，大家维护自己的分支，之后合并到 dev 上，需要关联到远程仓库。	bug：bug 修复分支，如果没有合作开发就不关联到远程仓库。	Feature：新的功能开发，如果没有合作开发就不关联到远程仓库Github 的其他使用说明	合作开发：一般而言，我们不具有别人仓库的修改权限，所以我们要是想参与开源项目的话，需要有以下几个步骤：					Fork 别人的仓库：这时候我们就有一个能修改的仓库了。			pull request：当你修改了一个 bug 或者开发了一个新 feature 以后，可以通过pull request 告知仓库所有者，看对方是否接受你的 pull request。			别名		有些指令很长，我们不希望每次都要手动输入，解决这个问题的方式就是使用别名。当然这种别名设置在 zshrc 中也可以。					在 .gitconfig 文件中的 [alias] 下设置# in ~/.gitconfig[alias]  lg = log --color --graph --pretty=format:&#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%ci) %C(bold blue) &lt;%an&gt;%Creset&#39; --abbrev-commit			直接使用命令行				使用 git config --global alias.&lt;alias_cmd&gt; &lt;cmd&gt; 来设置别名git config --global alias.ci commit			搭建 Git 服务器		除了使用 Github 官方的服务器外，我们还可以自己搭建自己的 Git 服务器，搭建的方法教程链接为这里。参考资料	廖雪峰 Git 教程]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> Github </tag>
        
          <tag> 代码管理 </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n 笔记04 Word Window Classification and Neural Networks]]></title>
      <url>/nlp/2018/11/03/CS224n_note_04_WWC_NN/</url>
      <content type="text"><![CDATA[									CS224n 笔记04 Word Window Classification and Neural Networks	简介这次的课程视频中主要讲了神经网络的构成，和在神经网络下的 BP 算法。整体为接下来的作业打好了基础，视频的最后也给出了对一个项目研究的基本步骤。分类问题 - 逻辑回归的拓展这部分为了让我们在接下来能够看的更顺利一点，做了一些名词解释：	 ${x^{(i)},y^(i)}^N_{i=1}$ 					 $x_i$：输入，e.g. words (indices or vectors!), context windows, sentences, documents, etc.			 $y_i$：输入的标签向量 			 $\mathcal{R}^{C\ \times\ d}$ ：代表该矩阵是 $C\ \times\ d$的防止过拟合这里讲的很浅，只讲了加了一个 L1 正则项，详情可以参考我之前的线性回归章节中对 Ridge 回归和 Lasso 回归的说明，是一样的。还讲了一些过拟合的图如下，老生常谈吧，比较简单：其实，一般而言神经网络越复杂，在数据集很小的情况下非常容易过拟合。The softmax and cross-entropy error使用交叉熵可以被写为熵和 Kullback-Leibler 距离的形式：$$H(p,q) = H(p) + D_{KL}(p||q)  $$其中 $D_{KL}(p||q)$为如下公式：The KL divergence is not a distance but a non-symmetric measure of the difference between two probability distributions p and q$$D_{KL}(p||q) = \sum_{c=1}^C p(c)\log{\frac{p(c)}{q(c)}}$$当然我们的例子中， $H(p)$ 是 0。在我们用的时候它的公式如下：$$H(p,q) = -\sum_{c=1}^C p(c)log{q(c)}$$其中 $p(c)$ 代表着 c 和其他单词的关系，在我们这里用的是 one-hot 向量，所以只有选定的词的概率会输出。 一些小建议这些建议是在小数量集的情况下，re-training词向量多次会让模型表达不好，例子如下：	在训练集中，训练的向量中telly, TV，television 这三个词在一起			如果重新训练，可能 television 没有划分在里面，那么训练结果显然就有问题了。		之前我们在 softmax 来表示 $p(y|x)$ 中使用的是 $W_y·x$ 这个公式作为 $exp(f(x))$ 中的 $f(x)$ 。Window classification只通过一个单词来判断它的意思是很难做到的。一个单词很可能有多种意思，这些意思甚至是相反的意思。所以我们需要通过上下文，也就是窗口来判断这个单词是什么意思。也就是说输入的 $X_window$ 是一个词向量拼接而成的，如下：这个窗口向量的维度为 $\mathcal R^{5d}$，其输出的 $\hat y$ 为：$$\hat y = p(y|x) = \frac{exp(W_y·x)}{\sum\nolimits_{c=1}^C exp(W_c·x)} $$ 它的交叉信息熵如下：$$J(\theta) = \frac{1}{N}\sum_{i=1}^{N} -log(\frac{e^{f_{y_i}}}{\sum_{c=1}^C e^{f_c}}) $$接下来就是 BP 的推导，这部分还是在 Assignment01 的总结里推导这部分公式。这部分就是把一个对一个词向量 BP 的过程扩展成一个矩阵的运算，用矩阵运算编出来的程序会快至少一个数量级。小建议	小心的定义你的变量们，并且一直跟踪他们的维度	使用链式法则 (Chain Rules)！神经网络从逻辑回归到神经网络，在之前的机器学习的文章中，我提到过集成学习中除了 boost 和 bag 两个策略外还有一个 Stack。我们可以粗浅的理解神经网络是用一堆逻辑回归通过某种策略链接而成的由一堆若分类器构成的集成学习。接下来就是一些简单的神经网络说明术语：	neuron：个人的理解就是一个逻辑回归连一个激活函数			A neural network：多个逻辑回归的输出，有几个逻辑回归输出下一层就有几个神经元			 $score(x)$：得分函数，可以理解为前向传播的结果 一些矩阵方面的说明可以看到中间层中每一个神经元对应了一个 $W$ 矩阵，图中的一共 3 个 $W$ 。为什么要激活函数想想我们逻辑回归是线性的，也就是只能画“直线”解决线性问题，那么出现了非线性的问题怎么办。假设不实用激活函数，结果则是一堆逻辑回归的叠加，即线性层的叠加效果很差。所以使用 sigmoid 函数来将非线性转线性，当然现在用 ReLu 比较多。这样子就能解决非线性问题了。间隔最大化目标函数这一部分大佬的博客中有详细的说明，如下我就整段摘抄了：怎么设计目标函数呢，记 $s_c$ 代表误分类样本的得分， $s$ 表示正确分类样本的得分。则朴素的思路是最大化 $(s - s_c)$ 或最小化 $(s - s_c)$。但有种方法只计算 $s_c > s \Rightarrow (s_c−s)>0 $ 时的错误，也就是说我们只要求正确分类的得分高于错误分类的得分即可，并不要求错误分类的得分多么多么小。这得到间隔最大化目标函数：这里的逻辑其实是，之前的逻辑回归画的“线”可以有很多条，之前没有规定选哪条最好，现在这个算法就是选一个最好的“线”，将间隔转换为几何间隔，这点和 SVM 很类似，公式中的 1 可以理解为，SVM 中的松弛因子是一个超参，它代表着其中一些点的重视程度，具体可以参考我之前的SVM 的博客中的松弛因子部分的说明。注：帅小哥说通常 1 的效果比较好有意思的说明	使用矩阵来运算而不是写个 for 循环单独对一个个向量运算会快很多。帅小哥的例子中快了 12 倍。所以矩阵是你的朋友，所以尽可能多用他们。			在实验中神经网络多出来的那一层（隐含层）的作用，可以理解为是不同输入词之间的非线性作用	传统 ML 和神经网络的区别	实验的流程（这一波不知道为什么 PPT 中没给）					Split your dataset：分为训练集80%，验证集10%，测试集10%。其中测试集直到快 deadline 前都不能用。			Establish a baseline：选一个模型来训练作为你的基准			Implement existing neural net model			Always be close to your data：									可视化他们					Collect summart statistics					查看模型错误					注意超参的影响							Try out different model variants									多选用多种分布的数据集进行比对结果，看看哪些分布没有捕捉到							研究的流程					早点开始调研看 paper			拼直觉（本事）找到现有模型的漏洞			和导师聊自己的看法			测试自己的想法（但是要小心验证）				简单的来说就是，大胆猜想，谨慎验证，现在行动	SGD 的一些说法：					由于使用 SGD，你不停的在随机更新，这使得你的模型不容易陷入局部最优解，这也是SGD比较快，效果还好的一部分原因。			事实上你的窗口越小，你的随机性越大			关于 BP 的推导在大佬的文章中有，大家可以自行查看，之后在作业的总结中还会推。参考	CS224n 课程：视频链接	大佬的笔记：链接在此]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[降维处理之 PCA 与 SVD]]></title>
      <url>/mathtools/2018/10/30/%E9%99%8D%E7%BB%B4%E5%A4%84%E7%90%86%E4%B9%8B-PCA%E4%B8%8ESVD/</url>
      <content type="text"><![CDATA[									降维处理之 PCA 与 SVD	PCA简要说明PCA 的全称为主成分分析（Principal Component Analysis）。简单的来说，PCA 的目的是将原来的坐标系旋转为新的坐标系，新的坐标系的选择是由数据本身决定的。第一个坐标系的选择是原始数据中方差最大的方向，第二个是和第一个坐标系正交（orthogonal）且方差最大的坐标轴，第三个，第四个均重复上述操作，重复次数为原始数据中特征的数目1。实现手法实际上我们是通过协方差矩阵2及其特征分析来求这些主成分的值的。其中特征分析就是指的是求他们的特征向量和特征值。协方差能导出一个变换矩阵，这个矩阵能使数据完全去相关(decorrelation)。从不同的角度看，也就是说能够找出一组最佳的基以紧凑的方式来表达数据。(完整的证明请参考瑞利商)。 这个方法在统计学中被称为主成分分析(principal components analysis)，在图像处理中称为Karhunen-Loève 变换(KL-变换)。——— Wiki具体实现的伪代码如下：# 去除平均值 （使用numpy中的mean）# 计算协方差矩阵 （使用numpy中的cov函数，根据定义构建）# 计算协方差矩阵的特征值和特征向量（使用numpy中的eig()函数返回特征值和特征向量）# 将特征值从大到小排序（使用numpy中的argsort(),得到最大的特征值对应的特征向量的位置）# 保留最大的N个特征向量# 将数据转换到上述N个特征向量构建的新空间SVD简述SVD（Singular Value Decomposition），即奇异值分解，本质上是一种矩阵分解技术，在应用领域已经出现了进百年。矩阵分解技术指的是将一个原始矩阵表示成新的易于处理的形式，这种形式是两个或多个矩阵的乘积，可以简单的理解为因式分解。最早使用 SVD 的应用的领域为信息检索，使用 SVD 的检索方法称为隐性语义索引（Latent Semantic Index，LSI）或隐性语义分析（Latent Semantic Analysis，LSA）。细节说明SVD 将原始的数据集矩阵 $Data$ 分解成三个矩阵 $U$、 $\Sigma$ 、 $V^T$。若原始矩阵是 $m*n$ 矩阵，那么 $U$ 为 $m*m$， $\Sigma$ 为 $m*n$， $V^T$ 为 $n*n$，写成公式如下3：$$Data_{m*n} = U_{m*m}\Sigma_{m*n}V^T_{n*n} $$其中 $\sum$ 为对角矩阵，这些对角上的值为奇异值，这个奇异值和特征值是有关系的，它是矩阵 $Data*Data^T$ 的特征值的平方根。实现方式已经很成熟啦，使用numpy的库就好啦，函数如下：from numpy import *U,Sigma,VT = linalg.svd(Matrix)    注：详细实现过程参考《Numerical Linear Algebra》4	《机器学习实战》 P243 &#x21A9;&#xFE0E;	https://zh.wikipedia.org/wiki/协方差矩阵 &#x21A9;&#xFE0E;	《机器学习实战》 P254 &#x21A9;&#xFE0E;	L.Trefethen and D.Bau III, Numerical Linear Algebra(SIAM: Society for Industrial and Applied Mathematics, 1997) &#x21A9;&#xFE0E;]]></content>
      <categories>
        
          <category> MathTools </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> MachineLearning </tag>
        
          <tag> MathTools </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n笔记03 A Deeper Look at Word Vectors]]></title>
      <url>/nlp/2018/10/30/CS224n_note_03_A_Deeper_Look_at_Word-Vectors/</url>
      <content type="text"><![CDATA[									CS224n笔记03 A Deeper Look at Word Vectors	简述这个课，简单的来说就是说，word2vec 除了之前的 skip-gram 算法，还有一个传统的算法使用基于窗口的共现矩阵来表示。他们都有缺点，于是诞生了 GloVe。SkipGram 的进阶思考上一个视频我们已经学了 Skip-gram 的全部思想啦，但是细心的大家肯定发现了，在Skip-gram 算法中，它对每个文中的词都给了一个窗口来进行运算，同时预测概率的公式的分母部分是有超级多的向量进行点积操作，显然到此为止，这个算法的实际应用开销太大。注：这里之前有错误，感谢一起学习课程的“彼岸花开”同学指出了此处的错误于是这里引入我们在 assignment01 中要使用的 negative sampling 来实现 skip-gram。这个思想是使用一种采样子集来简化运算。具体的做法是，取除中心词和上下文外的随机几个样本作为负例，训练 binary logistic regression。简单的来说就是，让非窗口内的单词出现概率最小，窗口内的最大。注：- 实际应用中直接全局取随机，抽到窗口内的单词概率很小，这是帅小哥的原话。- 其中 $P(w)$ 为 unigram 分布，旨在总是缓解出现总是抽到出现频率特别多的词的情况- 其中 $\sigma$ 是我们的常用的 sigmoid 函数，这里用了它另一个性质 $\sigma(-x) = 1- \sigma(x)$ 另一个方法我们可以看到 word2vec 将窗口作为训练的单位，每移动一次都需要计算一次参数，那么我们是否能用单词在窗口内出现的频次来构建参数呢。答案是肯定的，在 word2vec 之前很久，就已经出现了许多得到词向量的方法，这些方法是基于统计共现矩阵（co-occurrence matrix）的方法。如果在窗口级别上统计词性和语义共现，可以得到相似的词。如果在文档级别上统计，则会得到相似的文档（潜在语义分析LSA）1。Window based co-occurrence matrix样例样本：(window = 1)	I like deep learning	I like NLP	I enjoy flying虽然生成方式很简单，但是它的局限性也很明显：	要加入新的单词的时候，矩阵的维度都需要改变	矩阵的维度特别大	矩阵特别稀疏（所以提到了降维度）解决方案：	使用 SVD 进行降维处理	限制高频词的频次，或者干脆停用词	根据与中央词的距离衰减词频权重	用皮尔逊相关系数代替词频使用 SVD 存在的缺陷：	很难加入新词或文本	和其他的 DL 的训练思路不同，很难作为下游的模型的输入	计算维度太高Count based vs direct prediction注：红色的部分是缺点锵锵！综合两者的算法：GloVeGloVe 目标函数$$J(\theta) = \frac{1}{2}\sum_{i,j=1}^w f(P_{ij})(u_i^Tv_j - logP_{ij})^2$$其中 $P_{ij}$ 是两个词共同出现的频次， $u$ 和 $v$ 是共现矩阵中的行和列向量 $f$ 做了一个阀值，不让高频词的频率太高。注：- 最终得到的词向量一般是 $u+v$ - 第二部分是让他们的内积更加接近真实值优点	训练的很快	可扩展性高	可以在小训练集上也有不错的表现注1：GolVe 显著好于其他，但是维度不一定越高越好。不过数据量越多越好。注2：wiki 的词库效果好于新闻的效果评测方案评测方向有两个，Intrinsic 和 extrinsic：	Intrinsic 					Evaluation on a specific/intermediate subtask			Fast to compute			Helps to understand that system			Not clear if really helpful unless correlation to real task is established				注：可以理解为实验理想环境，不确定真实情况是否有效	Extrinsic:					Evaluation on a real task			Can take a long time to compute accuracy			Unclear if the subsystem is the problem or its interaction or other subsystems			If replacing exactly one subsystem with another improves accuracy		注：可以理解为实际环境，耗时长，需要至少两个 subsystems 同时证明。这类评测中，往往会用 pre-train 的向量在外部任务的语料上 retrain2。其他有意思的	做展示图的时候，曲线需要收敛了才行，帅小哥说图只截出了趋势会扣分的（笑	视频中还谈了谈一些适合word vector的任务，比如单词分类。有些不太适合的任务，比如情感分析。课件中则多了一张谈消歧的，中心思想是通过对上下文的聚类分门别类地重新训练3。		参考	CS224n 课程：视频链接	大佬的笔记：链接在此	http://www.hankcs.com/nlp/cs224n-advanced-word-vector-representations.html &#x21A9;&#xFE0E;	http://www.hankcs.com/nlp/cs224n-advanced-word-vector-representations.html &#x21A9;&#xFE0E;	http://www.hankcs.com/nlp/cs224n-advanced-word-vector-representations.html &#x21A9;&#xFE0E;]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[[CS224 Paper]A Simple but Tough-to-beat Baseline for Sentence Embeddings]]></title>
      <url>/nlp/2018/10/28/CS224-Paper-A_Simple_but_Tough-to-beat_Baseline_for_Sentence_Embeddings/</url>
      <content type="text"><![CDATA[									[CS224 Paper]A Simple but Tough-to-beat Baseline for Sentence Embeddings	简介@Sanjeev Arora, Yingyu Liang, Tengyu Ma@Princeton University@ICLR 2017简单介绍在 CS224 的第二节课中我们学了 word2vec 算法。该算法的的目的是找出一种对文字更合适的表示方法。但是实际生活中我们更关心的是一个句子的意思。这篇文章就是找出一种合适的句子的向量表示。Why Sentence Embedding首先，如果我们能拿到句子的向量表示的话，我们就能用向量内积的方式来计算句子的相似度。除此之外，如果我们能用向量表示句子的话，就可以用它作为一个句子的特征来进行分类来完成一些任务，如情感分析(sentiment analysis)等。已有的做法	Simple additional composition of the word vectors			Use sophisticated architectures such as convolutional neural networks and recurrent neural networks		论文的灵感来源这种训练一个Averaging model 的思路来源于Wieting et al., 20161，之后通过经验观察得到，当人们用内积来表示两个句子的相关性的时候引入了很多没有实际含义的方向（direction），于是产生了第二步的降纬度处理。These anomalies cause the average of word vectors to have huge components along semantically meaningless directions.—— In the Page2本论文的新方法论文中的方法非常的简单，一共只有两步。核心就是加权词带法加上去除一部分特殊方向。weighted Bag-of-words + remove some special direction	使用一种加权的词带法				其中 $p(w)$ 为某个单词的频率，这一步中引入了一个特殊的权值，这个权值会让高频词的权值下降。求和后得到暂时的句向量。	计算语料库中所有句向量构成的矩阵的第一个主成分 u，让每个句向量减去它在 u 上的投影（类似PCA）。其中，一个向量 v 在另一个向量 u 上的投影定义如下：		$$		Proj_u v = \frac{uu^T v}{||u||^2}  		$$		注：本段原文完全来自大佬的博客A Probabilistic Interpretation我们是这样假设的，一个单词的出现分为两种情况，一种是和某个中心词相关，而另一种则是和中心词无关的。浴室可以写出以下的公式。其中公式 2中第一部分，代表着和中心次无关的出现的情况，而第二项则是代表着和中心词有关的情况，其中 $\beta$代表着该单词出现是和中心词相关是因为语法 (syntax) 相关的情况。注：式子中的 $\alpha$ 和 $\beta$ 是超参。 Results参考	大佬的文章（链接地址）	[Paper]A Simple but Tough-to-beat Baseline for Sentence Embeddings	John Wieting, Mohit Bansal, Kevin Gimpel, and Karen Livescu. Towards universal paraphrastic sentence embeddings. In International Conference on Learning Representations, 2016. &#x21A9;&#xFE0E;]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n 笔记02 词的向量表示：word2vec]]></title>
      <url>/nlp/2018/10/28/CS224n_note_02_word2vec/</url>
      <content type="text"><![CDATA[									CS224n 笔记02 词的向量表示：word2vec	如何表达一个词语的意思要表达一个词语的意思，首先我们要知道什么是词语的意思呢。Definition: meaning (Webster dictionary)	the idea that is represented by a word, phrase, etc.	the idea that a person wants to express by using words, signs, etc.	the idea that is expressed in a work of writing, art, etc.从这个定义中，我们可以看到，一个词语的意思是通过以一个含义和一个符号（词语）进行对应来表达的，有一种 key - value 的意味。计算机处理词语这里主要有两个思路，一种称为discrete / symbolic representation，另一种为 distributed representations。而后者是现在主流的思路。	discrete / symbolic representation		在早期的计算机处理方案是使用分类词典，人工的把不同的单词按照某种关系按层分类，如 NLTK 中可以通过 WordNet 查询熊猫的hypernyms (is-a，上位词)，得到“食肉动物”“动物”之类的上位词。但是这种做法的坏处也是明显的：					同义词的意思实际上还是有微妙的差别：adept, expert, good, practiced, proficient, skillful			缺少新词，而且不可能做到实时更新			需要花大量人力去整理，同时有很强的主观化			无法计算准确的词语相似度，因为 one-hot 向量表达的不同词之间的点积只能是 0 / 1。				注：无论是规则学派，还是统计学派，绝大多数NLP学家都将词语作为最小单位。事实上，词语只是词表长度的 one-hot 向量，这是一种 localist representation（一种通过表达词语在语料库中的位置的方式）。在不同的语料库中，词表大小不同，也就是意味着每个 one - hot 向量的长度就不同。试想，Google的 1TB 语料词汇量是1300万，这个向量就大约有 24 维。	 distributed representations		为了表达词语在符号层面上的相似度，我们需要用一种更为合适的方式来编码。在这点上语言学家给我们了启发，J. R. Firth 提出，通过一个单词的上下文可以得到它的意思。他甚至认为，只有你能把单词放到正确的上下文中去，才说明你掌握了它的意义。				”You shall know a word by the company it keeps”		——— From (J. R. Firth 1957: 11)						这么做也就引入了另一个变量，即上下文。我们都知道机器学习是基于统计学的，只有更多的数据被引入才能好的提升我们的模型。神经网络 word embeddings 的基本思路	定义一个选定中心词后表达预测正确上下文的某个单词的模型：		$$		p(contex|w_1) = ...  		$$	定义损失函数		$$		J = 1 - p(w_{-t}|w_t)  		$$		其中 $w_{-t}$ 表示 $w_t$ 的上下文。在以后的表示中（负号表达除 XX 之外的集合），若全部预测正确，该函数为 0。 word2vec 的主要思想Word2vec 顾名思义就是将词用向量来表示，课中提及了如下两种算法和两种高效的训练方式。两种算法：	Skip-grams(SG)：预测上下文 				注：课程中的算法	Continuous Bag of Words （CBOW）：预测目标单词两种稍微高效一点的训练方法：	Hierarchical softmax	Negative sampling		注：其他的几个的大佬的博客推导Word2vec 细节实现首先可以由定义写出中心词 $w_t$ 对于窗口为 $m$ 的上下文的预测结果，即所有位置的预测结果的乘积：$$J(\theta) = \prod\limits_{t=1}^{T}\prod\limits_{\substack{-m \leq j \leq m \\ j \neq 0}} log({p(w_{t+j}|w_t)}) $$之后，由于乘积的运算非常缓慢，所以使用 $log$ 将其转换为求和的形式：$$J(\theta) = - \frac{1}{T}\sum\limits_{t=1}^{T}\sum\limits_{\substack{-m \leq j \leq m \\ j \neq 0}} log({p(w_{t+j}|w_t)})$$注：目标函数的术语有好几种，Loss function，cost function,objective function这些都是。对于 softmax 来说，常用的损失函数为交叉熵。而其中某个上下文的条件概率密度 $p(w_{t+1}|w_t)$，我们使用 softxmax 来得到：$$p(o|c) = \frac{exp(u_o^T v_c)}{\sum_{w=1}^v exp(u_w^T v_c)}  $$注：其中 o 是输出的上下文词语中的确切某一个，c 是中间的词语。u 是对应的上下文词向量，v 是词向量。并且用点积代表两个向量的相似程度，越大越相似。除此之外，值得强调的是 Softmax function：是一种从实数空间到概率分布的标准映射方法Skipgram这个是课堂中老师使用的 ppt，虽然有点乱，但是解释的还算清楚，但是第一次看很容易晕。以下对一些重点进行解释。	第一个 $W $ 指的是词库	第二个 $W'$ 指的是文章的上下文，每一个向量都代表着单词。	 $w_t$ 和 $Truth$ 都是 one-hot 向量，都代表着选中某一个位置现在来分析整体的过程是什么样子的。	首先通过 $w_t·W$ 得到中心词的第一个向量。	之后用得到词的向量去和文章中的所有向量点积，即 $v_c·W'$。得到中心词和文中所有词的相似度。	通过 softmax 将相似度转成概率。	最后通过 $Truth$ 这个one-hot向量，将所有的紧挨着中心词的位置中的概率提出来。		注：这步就是指，我们得到了中心词和周围的所有词的相似度之后，通过one-hot向量将实际中中心词周围的词和中心词的相似度取出来。乘在一起就是之前说的损失函数。 	进行梯度下降算法		注：这一步就是通过调整 $W$ 和 $W'$ 的值，最终得到词库的单词向量 $W$ 和基于上下文的词库的向量 $W'$。最终对于表示一个词的两个向量，我们通过求和或者拼接来使用		课堂有意思的参考Christopher Manning 的 BabyMath课堂其他有趣的点	Christopher Manning 提到的矩阵求导公式：		$$		\frac{\partial x^T a}{\partial x} = \frac{\partial a^T x}{\partial x} = a  		$$	BP就是链式法则（Chain Rule）！	神经网络喜欢嘈杂的算法，这可能是SGD成功的另一原因。参考	CS224n 课程：视频链接	大佬的笔记：链接在此]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[CS224n笔记01 自然语言处理与深度学习简介]]></title>
      <url>/nlp/2018/10/23/CS224n_note_01-introduction/</url>
      <content type="text"><![CDATA[									CS224n笔记01 自然语言处理与深度学习简介	简单声明本笔记为 CS224n 课程在学习过程中留下的笔记。整体风格以简洁为主，尽量去掉不必要的背景知识说明，只留下个人觉得最重要的内容以供日后参考回忆。其中部分内容为了巩固英语，可能会因为方便采用英文记录。课程先决条件	Python基础知识	高等数学、概率论、线性代数知识	基础机器学习算法					梯度下降			线性回归			逻辑回归			Softmax			SVM			PAC		注：斯坦福CS229 / 周志华西瓜书本课学习收获总览	整体自然语言处理的流程	深度学习的 NLP 和之前的有什么区别	NLP 的难点在哪里自然语言处理的总览Phonetic : representing speech sounds by means of symbols that have one value onlyPhonology ： the phonetics and phonemics of a language at a particular timeMorphology：a study and description of word formation (such as inflection, derivation, and compounding) in languageSyntactic：of, relating to, or according to the rules of syntax or syntactics根据上述 Webstar 词典的翻译我们可以看到，这个整个过程很像是通信领域信号的发出和接受的过程。自然语言处理系统的输入分为两个部分，一个是文本的输入（我们暂且不谈），另一个则是语音的输入。好的，我们收到了语音，我们都知道这些语音不一定都是标准的，可能还带有方言之类的，所以我们首先要分析它，也就是第一个圈圈。之后由于单词的变形有很多，所以我们要把它们全部变换成原本的形式，也就是 Morphological Analysis 这个过程。之后，再进行语法（Syntactic）分析，最后再进行语义（Semantic Interpretation）理解。NLP 为什么难	Complexity in representing, learning and using linguistic/situational/world/visual knowledge	Human languages are ambiguous (unlike programming and other formal languages)	Human language interpretation depends on real world, common sense, and contextual knowledge机器学习 VS 深度学习如下图所示，传统的机器学习中很大一部分人工部分是人力的去观察你的数据，然后从中人为的提出特征，这需要消耗大量的人力，甚至这个人力还必须由有博士学位的专家才能做，而机器只是代替人类做了人类不容易做到的对算法调优的过程。那么深度学习 (Deep Learning) 是什么呢，首先从宏观的来讲深度学习隶属于表征学习（Representation Learning），即特征学习（Feature learning）。正如其名，表征学习就是自动的从原始数据中提取分类和特征提取（feature detection）所需要的特征（representation），也就是说深度学习可以做之前机器学习中人力的那部分内容。宏观结束了，进入微观理解， 如下图所示，深度学习中是一个多层的网络，每一层都会学习出一部分特征，然后将这些特征喂给下一层，这个学习过程可以反复的去修正 / 训练这些提出的特征，效率高。最后，大家在学机器学习的时候都知道有两种机器学习，一种是有监督学习，一种是无监督学习。深度学习两者都能做，这点是真的有点厉害。其他总结这堂课中有一个很重要的总结，就是在所有的 NLP 学习 Level 中，所有字的表达和其表达的含义（representations for words and what they actually represent）都是用向量(Vectors)来代替的。这点非常重要，特此记录。]]></content>
      <categories>
        
          <category> NLP </category>
        
      </categories>
      <tags>
        
          <tag> NLP </tag>
        
          <tag> CS224 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[代理 Socks 转 http]]></title>
      <url>/linux/%E5%B0%8F%E6%8A%80%E5%B7%A7/2018/10/13/%E4%BB%A3%E7%90%86Socks%E8%BD%AChttp/</url>
      <content type="text"><![CDATA[前言这两天，学长派的任务中，需要使用 Scrapy 爬去许多国外的网站，需要给 Scrapy 搭梯子，而 Scrapy 只支持 http 的代理，故记录下这次 socks 转 http 的步骤。前置软件  shadowsocks （就不解释了，大家都知道）      privoxy    注：polipo 的方案已经过时了，privoxy 作为新的方案，更灵活  配置步骤  安装     sudo apt-get install shadowsocks privoxy            更改 privoxy 配置     vim /etc/privoxy/config # 找到 forward-socks5t 取消他的注释，注意后面的端口跟着的是 shadowsocks 的端口 forward-socks5t / 127.0.0.1:1086 . # 注意后面有一个点，以及是 5t listen-address localhost:8118 # 默认是 8118，可以自己改 # 0.0.0.0 的话，可以给同局域网的设备使用 ss，比如 ps4 等            启动了 shadowsocks 之后，启动 privoxy     sudo systemctl restart privoxy.serivce #或者 sudo privoxy --no-daemon /etc/privoxy/config            测试     export all_proxy=http://127.0.0.1:8118 curl ip.cn #想要 scrapy 生效，这步 export all_proxy, 经过测试表明是必须的      ]]></content>
      <categories>
        
          <category> Linux </category>
        
          <category> 小技巧 </category>
        
      </categories>
      <tags>
        
          <tag> privoxy </tag>
        
          <tag> socks5 </tag>
        
          <tag> http_proxy </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[关于苏格兰启蒙运动的思索]]></title>
      <url>/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/10/09/%E5%85%B3%E4%BA%8E%E8%8B%8F%E6%A0%BC%E5%85%B0%E5%90%AF%E8%92%99%E8%BF%90%E5%8A%A8%E7%9A%84%E6%80%9D%E7%B4%A2/</url>
      <content type="text"><![CDATA[关于苏格兰启蒙运动的思索苏格兰启蒙运动，对于我这种纯编程的学生来说是很陌生的，也是在看了些关于《苏格兰：现代世界闻名的起点》这本书的书评后才有所了解，并有所感悟，特此记录。苏格兰启蒙运动有三个代表人物，他们是哈尔森，休谟和亚当·斯密。当然对于第三个人大家都知道是 《国富论》 这本经济学奠基之作的作者，但是大家不知道的其第一个成名作是 《道德的情操论》 的撰写。而哈奇森则是亚当·斯密的老师，也是启蒙运动的起始者，他的观点是道德是与生俱来的，也可理解为人心本善，而反对者则是休谟，他的观点是人总是目光短浅，且利己，也可以说是人心本恶的感觉。到这里再反观亚当·斯密的 《国富论》，里面推理的原点不乏有很多休谟的影子。但是我最大的收获还不是他们的思想是怎么样子的，对我而言最大的收获在于，对于 “启蒙” 二字的理解。小时候上历史课的时候，需要去背一些这个启蒙运动，那个启蒙运动，到后来看到艺术史的时候各个派别的演变都夹杂着 “启蒙” 二字。那么启蒙到底是什么？现在我的理解是，让人们自己给予事实独立的思考，而不迷信于某些权威。我觉得这里是重点，科学是历史长河中的偶然产物，从古代开始，大部分的强势文明都是崇尚鬼神这种玄幻的东西，他们的理论看似可以解释世间万物，但是只是基于思辨这种不结实的空中楼阁。而从古希腊发源出来的 “科学” ，则是扎根于实验和数学，任何结论都是可重复，可证伪的，但也注定了很难出现一种能解释世间万物的理论。这个道理我们很早就听过了，但是为什么我这次还是有些感悟呢。那是因为虽然我们认为我们都是信奉科学的，但是也许实际情况则是也许我们迷信了科学。大家唯科学马首是瞻，就像赫拉利在他的三部曲中说的那样，科学成为了当代的一种宗教。为什么这么说呢？想一想你有没有这样一种经历，一个人兴致勃勃的给你将某个从不知道哪里听来的所谓专家的说法，当你反问他的时候他甚至不知道这个专家的科研背景，也不知道自己嘴中说的 “科学名词” 的真正含义。除此之外，还不允许你去质疑，质疑了就会被反问 “你是专家么？” “你不相信科学！”这类的话。想问这种说法，和宗教中的“你不信仰 XXX 神” “你没资格评判神” 有着异曲同工之妙（笑）。那么真正的科学，和一种我认可的思维方式是什么样子的呢？科学是允许质疑的，不如说质疑使科学进步。要知道，科学界对于自己的研究结果是有很严格的规范的，比如要说自己能预测意见事情的话，需要准确度高于 99.74%（来源于正态分布中的 3 倍标准差）。举个例子如果你有 98% 的把握，实验观察到了一种粒子，你的论文题目只能写发现了该粒子的踪迹。除此之外，任何结果是可以复现的，即我照着你说的步骤严格一步一步的去做，一定能得到你的结果。这也是为什么科学不怕质疑，也是科学进步的源泉，科学走的每一步虽然慢，但是绝对可靠，它的结果大概率是可以相信的，我们可以在前人的基础上继续前进。就像吴军说的“我们不要重新造轮子”。临末，想再次强调一下，个人的收获，就是自己独立的思考，对于别人说的事情保持一定的怀疑心态，独立思考，而这个也算是我个人比较欠缺的地方。突然暴露了我这个作为调包侠的本质（捂脸）。最后，祝大家都能从科学中受益呀。]]></content>
      <categories>
        
          <category> 读书笔记 </category>
        
      </categories>
      <tags>
        
          <tag> Reading </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[列表解析和 for 循环的比较]]></title>
      <url>/python/2018/10/07/%E5%88%97%E8%A1%A8%E8%A7%A3%E6%9E%90%E5%92%8Cfor%E5%BE%AA%E7%8E%AF%E7%9A%84%E6%AF%94%E8%BE%83/</url>
      <content type="text"><![CDATA[分析首先先放上，Stackflow 的高票答案的结论  A list comprehension is usually a tiny bit faster than the precisely equivalent for loop (that actually builds a list), most likely because it doesn’t have to look up the list and its append method on every iteration. However, a list comprehension still does a bytecode-level loop.显然，列表推导比 for 循环稍快。答者的分析为列表解析没有创造一个列表的实体，所以它的稍微的性能提升来源于不需要在每次迭代中查找列表，和使用它的 append 函数。其他理解正如文中说，这种优化方案是没有意义的，因为你话费同样的经历去优化你的 python 代码，不如重构成 C 的代码来优化，这样子前者的效率提高只是 15% 左右，而后者会得到 300% 的提高，这是数量级的优化。参考文章Stackflow 的提问： Are list-comprehensions and functional functions faster than “for loops”? ]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[命运的一些思考]]></title>
      <url>/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/10/06/%E5%91%BD%E8%BF%90%E7%9A%84%E4%B8%80%E4%BA%9B%E6%80%9D%E8%80%83/</url>
      <content type="text"><![CDATA[命运的一些思考在听了吴军老师的《硅谷来信 311～314》期的内容后有一些小小的感悟。我们的未来，也就是命运是由什么决定的呢，这个问题困扰我多年。今天就来记录下自己的所思所想。定义命运是什么？首先让我们看看百度百科的定义。  命运，即宿命和运气，是指事物由定数与变数组合进行的一种模式，命与运是两个不同的概念。命为定数，指某个特定对象。运为变数，指时空转化。抽象的来说就是一种固定的不会因为我们的意志发生转移的定数和一个我们可以干预的或者抓住的变量组成的。吴军的解读如下。      命是环境因素和我们自己对未来一辈子走向划定的两条线——一个人的人生轨迹走不出这两条线。个人的努力、运气等不过是让人在这两条线之间作微调而已。    从人的角度看，命就是一个人看问题的方法和做事的方法  在命运这个方面，觉得吴军指的是不可控的环境因素和应对这些不可控因素时所采取的因对思路，指的是思维方式。我觉得我们的“命运”很大程度上掌握在我们的思维方式上面。现在的中国虽然是由农耕文明发展而来，但是随着近代自由的思想的普及，社会中的机会开始均等起来，一个人的最单纯的出生阶级已经不在时我们人生的天花板，但是随着原生阶级中带给我们的思维方式缺很大程度上决定了我们的命运。也就是说命运高度相关于我们的思维方式和做事方式。困局思维方式和做事方式这么重要，那么我们要怎么做呢。首先吴军警告我们说不要抱有人定胜天的说法，任何与命开玩笑的做法早晚都没有好结果，并举了一个他的师姐高考超水平发挥考上清华的例子。对此我个人的理解是，人要时时刻刻保持谦卑，我们很容易夸大自己的个人能力在一件事情中的重要程度，如今想要做成一件规模较大的事情，莫不是需要调动多方资源，任何一个小小的差池都将导致最终的失败。所以当我们得到了某些“成就”的时候，不妨更多的去感谢帮助过自己的人，感谢老天带来的好运气。除此之外，我们的思维方式，很大程度上来源于我们的原生家庭，而这个也是我觉得为什么有有时候不同的阶级的人的小孩最终大概率还是在不同阶级。我在观看了 《人生七年》 中也有这样的感受，富人的孩子从小接受了较为好的教育，他们变得愿意去思考一些更为抽象的问题，人生观较为和谐和乐观，而底层的孩子身边则由于复杂的环境而具有一定的攻击性和一种锋芒毕露的思维方式。而最终那些相对成功的孩子（包括那个逆袭成了物理学家的小孩），最终也不是依靠自己的父母的人脉去走后门，去取巧获得的地位。因而整体上来讲，我的观点是上层阶级的小孩的优势在于，他的父母有着一个更“优秀”的思维方式，一种更“先进”的生活方式，同时在丰厚的财力的辅助下，其小孩更容易形成目光更为长远的思考方式和消费观。这也是为什么，当年美国以直接发放现金的形式扶贫没法活的成功，也是为什么暴发户不容易被社会所接纳。想晋升阶级最重要的或许是思维方式，而不是金钱和地位。那么思维方式要怎么培养才可以呢？首先，我们需要知道的是，思考方式的形成是一个长期的过程，这意味着短期想完全改变是不现实的，同时改变是痛苦的，它会颠覆一些原有的三观。比如，富人和领导都是靠着贿赂上位的，领导是愚蠢的这类三观。其次，和大部分的事情一样，没人能告诉你正确的方法，因为环境是复杂的，成功的原因是千奇百怪的，大多数还是归因于运气。那么没有正确的方法我们是不是就束手无策了呢？在这里，或许恰恰就需要一个小小的三观上的转变，我们从小都教育的是要找正确的答案。但是遗憾的是和数学一样，有解的问题是很少的，甚至都不知道有没有解，那么这种情况下要怎么办呢？吴军给出的方法则是，找寻已经成功的心灵导师，注意的是，要是已经成功的。也就是说，什么失败者的任何对人生的成功方法的推测都是没有意义的。找到了以后，跟随这个导师，研究他的一些为人处事方案，学习策略，学习自己的偶像，并在生活中慢慢尝试。就像很多 zsh 的 alias设定一样，我们在开始使用的时候不知道为什么，只能抱着这是成功的程序员总结的没有冲突的快捷键来使用，随着使用会慢慢的体会到其中的精髓，当然在使用的时候也会想着以自己的方式进行改进。最后，人生就是一个被不停的扇耳光的过程，我们怎么处理挫折就显得很重要了，扇回去？认命？显然都不是，可能最好的策略也许是，先反思为什么会被扇，然后分析处境后给出最优策略，切忌让情绪走在思维前面。结语可能到了奔三的年纪，大家都开始惶恐，很多朋友都在前途方向上抉择，我也不例外。这个问题大概没有最优解，个人的看法一直都是接受现在的处境，接受现在自己，思索自己最不想成为的人是什么样子，然后尽量去避免它成为现实。一般而言，想成为的样子千千万，但是不想成为的样子就少很多了。最后，祝大家都能接受现在的自己，好也好，不好也罢，停止感叹，做自己认为有意义的事情。也祝自己最近的一些大大小小的计划能够成功。]]></content>
      <categories>
        
          <category> 读书笔记 </category>
        
      </categories>
      <tags>
        
          <tag> Reading </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[显示键盘按键 - Keycastr]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/10/05/%E6%98%BE%E7%A4%BA%E9%94%AE%E7%9B%98%E6%8C%89%E9%94%AE-Keycastr/</url>
      <content type="text"><![CDATA[显示键盘按键 - Keycastr前言最近写 VIM 专栏需要显示能够实时显示按键顺序的插件，其中 Keycastr 这个 Github 的开源项目使用的最舒心，安装也是最方便，特意留个档记录下。安装  很简单，使用 brew 安装即可。     brew cask install keycastr        在安装好了以后，给这个程序权限即可结语这个软件可以看着自己在用 vim 的时候，完成一个操作的按键次数越来越少，真的很开心。同时在录制自己的操作动图的时候，也能更加清楚的展示。不过美中不足的是，这个软件每次升级之后都要重新设置下权限才可以。]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> Mac 软件 </tag>
        
          <tag> 杂项 </tag>
        
          <tag> 效率 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Mac OS下 安装Scapy后出现的YCM报错]]></title>
      <url>/troubleshoot/2018/09/18/Mac-OS%E4%B8%8B-%E5%AE%89%E8%A3%85Scapy%E5%90%8E%E5%87%BA%E7%8E%B0%E7%9A%84YCM%E6%8A%A5%E9%94%99/</url>
      <content type="text"><![CDATA[									Mac OS下 安装Scapy后出现的YCM报错	起因Mac OS 安装了 Scapy 后，在 YouCompleteMe 中出现如下报错：X:ValueError: unknown locale: UTF-8 in Python解决方案添加如下代码到配置环境变量文件（zsh 或者 bash）export LC_ALL=en_US.UTF-8export LANG=en_US.UTF-8]]></content>
      <categories>
        
          <category> TroubleShoot </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> TroubleShoot </tag>
        
          <tag> Scapy </tag>
        
          <tag> VIM </tag>
        
          <tag> YouCompleteMe </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[关于设置YouCompleteMe Python3语法支持]]></title>
      <url>/troubleshoot/2018/09/16/%E5%85%B3%E4%BA%8E%E8%AE%BE%E7%BD%AEYouCompleteMe-Python3%E8%AF%AD%E6%B3%95%E6%94%AF%E6%8C%81/</url>
      <content type="text"><![CDATA[问题描述YouCompleteMe 只能支持 Python2 的补全，不支持 Python3 库的补全。解决  重新去 YouCompleteMe 用 git pull 一下。  使用 pip 安装 jedi     pip3 install jedi        重新用 python3 编译（非常重要）     python3 ./install.py --all        在 vimrc 中添加支持     let g:ycm_server_python_interpreter='/usr/bin/python3' let g:ycm_python_binary_path = '/usr/local/bin/python3'      参考  YouCompleteMe Issue 2876   How do I complete Python3 with YouCompleteMe? ]]></content>
      <categories>
        
          <category> TroubleShoot </category>
        
      </categories>
      <tags>
        
          <tag> YouCompleteMe </tag>
        
          <tag> Vim </tag>
        
          <tag> TroubleShoot </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Python 交互模式下自动补全]]></title>
      <url>/python/2018/09/15/Python%E4%BA%A4%E4%BA%92%E6%A8%A1%E5%BC%8F%E4%B8%8B%E8%87%AA%E5%8A%A8%E8%A1%A5%E5%85%A8/</url>
      <content type="text"><![CDATA[前言有时候我们需要测试一个小功能，懒人如我完全不愿意新建一个 python 文件去测试，但是默认的 python 交互模式下没有代码补全就很恼火，今天就把它解决了。步骤首先在 HOME 目录下新建一个叫做 pythonstartup 的文件。touch ~/.pythonstartup接下来在里面输入如下内容：&lt;pre name="code" class="python"&gt;import rlcompleterimport readlineimport atexitimport os# http://stackoverflow.com/questions/7116038/python-tab-completion-mac-osx-10-7-lionif 'libedit' in readline.__doc__:	readline.parse_and_bind('bind ^I rl_complete')else:	readline.parse_and_bind('tab: complete')histfile = os.path.join(os.environ['HOME'], '.pyhist')try:	readline.read_history_file(histfile)except IOError:	passatexit.register(readline.write_history_file, histfile)del readline, rlcompleter, histfile, os最后把它添加到环境变量中，zsh 在  zshrc 中，bash 在 bash_profile 中。echo 'export PYTHONSTARTUP=~/.pythonstartup' &gt;&gt; ~/.zshrc# for oh my zshecho 'export PYTHONSTARTUP=~/.pythonstartup' &gt;&gt; ~/.bash_profile# for bash参考文献CSDN 中 jorrell 博主的  交互模式下 python 自动补全 ]]></content>
      <categories>
        
          <category> Python </category>
        
      </categories>
      <tags>
        
          <tag> Python </tag>
        
          <tag> 技巧 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[EM算法]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/15/EM%E7%AE%97%E6%B3%95/</url>
      <content type="text"><![CDATA[							EM算法	1、必要的前置知识	最大似然估计（MLE）：找出一组参数(模型中的 参数)，使得模型产出观察数据的概率最大。	贝叶斯算法估计：从先验概率和样本分布情况来计算后验概率的一种方式。	最大后验概率估计（MAP）：另一种MLE的感觉，求 $\theta$ 使 $P(x|\theta)P(\theta)$ 的值最大，这也就是要求 $\theta$ 值不仅仅是让似然函数最大，同时要求 $\theta$ 本身出现的先验概率也得比较大。	Jensen不等式：如果函数 $f$ 为凸函数，那么存在公式$f(\theta x + (1-\theta)y) \leq \theta f(x) + (1-\theta) f(y)$，进一步推论得到若 $\theta_1,...\theta_k \geq 0$ 且$\theta_1+\theta_2+...+\theta_k = 1 $，则有 $f(\theta_1 x_1 + ... + \theta_k x_k) \leq \theta_1 f(x_1) + ... + \theta_k f(x_k)$。2、EM算法EM算法（Expectation Maximization Algorithm）最大期望算法，是一种较为常用的算法思路，像之前的 SoftMax 算法就算是 EM 算法的一种，这种算法主要分为以下两个循环操作：	E步骤：估计隐藏变量的概率分布期望函数	M步骤：根据期望函数重新估计分布参数简单的来说就是写出带参数的表达预测正确的概率公式，然后用一种方法使得这个预测正确的概率最大。其整体的流程如下：注：这个使预测概率最大化的过程往往会得到一些关键的参数，有时候这个参数就是我们要求得某个事件发生的概率。	初始化分布参数	重复下列操作直至收敛					E步骤：估计隐藏变量的概率分布期望函数			M步骤：根据期望函数重新估计分布参数（用的是MAP），即重新调整模型参数$\theta$，公式如下				设数据集中包含着我们不能观测到的隐含数据$z = {z_1, z_2, ...,z_m}$				$$				\begin{split}				\theta &= arg\ max_\theta\sum_{i=1}^m log(P(x_i;\theta)) \\				&arg\ max_\theta\sum_{i=1}^m log(\sum_{z_i}P(z_i)P(x_i|z_i;\theta)) \\				&arg\ max_\theta\sum_{i=1}^m log（\sum_{z_i}(P(x_i,z_i;\theta))				\end{split}				$$		3、EM算法求解原理根据上一章中的EM算法总述看来，现在我们整体要做的就是根据对数似然函数，调整参数 $\theta$ 使得对数似然函数向变大的方向前进。按照以往的惯例，我们直接进行求导就好了。然而在这里则行不通，我们可以看到中间存在隐藏数据。为什么说是隐藏的呢，那是因为这个数据是不可观测的，也就是说我们不能在每一步中直接测量到这些数据。不过好消息是我们可以得到这个隐藏函数的分布。于是进行的曲线救国过程的第一步如下：\begin{split}l(\theta) &= \sum_{i=1}^m log\sum_z p(x,z;\theta) \\&= \sum_{i=1}^m log \sum_z Q(z;\theta) * \frac{p(x_i,z;\theta)}{Q(z;\theta)}\ \ \ 步骤1 \\&= \sum_{i=1}^m log(E_Q(\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤2 \\&\geq \sum_{i=1}^m E_Q(log(\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤3 \\&= \sum_{i=1}^m \sum_z Q(z;\theta) log (\frac{p(x_i,z;\theta)}{Q(z;\theta)})\ \ \ 步骤3\end{split}首先，我们明确的目标是得到最佳参数 $\theta$ 也就是说，我们企图得到一种可以把似然函数写成纯 $\theta$ 的公式，之后进行通过求偏导的方式来得到我们要求的最佳参数 $\theta$。步骤一中，我们引入了隐藏数据的分布 $Q(z;\theta) $ , 根据分布函数的性质$\sum_z Q(z;\theta)=1 $ ，所以我们加入这个参数是不会改变原函数的数值，之后根据数学期望的定义将右边式子写成了数学期望的形式，于是得到了步骤二。在步骤三中，根据根据Jensen不等式，我们可以得到$f(E(x)) \leq E(f(x))$，所以我们将$log$塞到期望函数里面，最终得到了步骤四。好了到现在为止，我们只是得到了另一个看似依旧不能计算的式子，貌似多此一举的引入了一个叫做 Jensen 不等式的东西，但是这一步是很关键。在未引入Jensen不等式之前，我们只是单纯的添加了一个叫做隐含数据的变量，但是它在式子中并未和别的参数相关，而引入了Jensen不等式之后，这个式子中的隐含数据将和数据集关联就变得可以测量得到了。所以曲线救国的第二步就是，根据Jensen不等式的取等条件得到 $\frac{p(x,z;\theta)}{Q(z;\theta)} = c$ 的时候等号成立，于是根据如下推导得到当等号成立的时候，得到的关系公式 3.1 。$$Q(z,\theta) = \frac{p(x,z;\theta)}{c} = \frac{p(x,z;\theta)}{c*\sum_{z_i}p(x,z_i;\theta)} = \frac{p(x,z;\theta)}{p(x;\theta)} = p(z|x;\theta)\ \ \ 公式3.1$$之后将公式 3.1 带入之前步骤三的公式中，得到如下公式3.2$$\sum_{i=1}^m \sum_z p(z|x_i;\theta) log (\frac{p(x_i,z;\theta)}{p(z|x_i;\theta)})\ \ 公式3.2$$又由于给定数据集之后 $p(z|x_i;\theta)$ 的值是固定的，所以可以去掉，而保留前方的 $p(z|x_i;\theta)$ 的原因类似 MAP 模型，要求先验概率也要大。之后便是传统的求能使公式3.2最大的模型参数 $\theta$ 。至此，我们公式已经推完了，简单的整体分析一下，式子中含有两个变量$z$ 和 $\theta$ ，我们先假定了 $\theta$ 的初始值，显然根据求导等于0能得到 $z$ 的值，之后再根据 $z$ 的值可以反求 $\theta$ 的变化方向。如此往复就是 EM 算法的求解过程。4、结语整体而言 EM 算法相比之前的算法引入了一个叫做隐含数据的变量，这个变量我们认为是会对结果发生影响的。整个算法都是在讨论一种，如何将这种我们不清楚的变量和我们预测手法联系在一起，其中用到了 Jensen 不等式来让他们联系在一起，变得可以被计算。以后有时间的话还会补充 GMM 在EM算法中的应用。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[支持向量机的补充之核函数]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/09/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%E7%9A%84%E8%A1%A5%E5%85%85%E4%B9%8B%E6%A0%B8%E5%87%BD%E6%95%B0/</url>
      <content type="text"><![CDATA[							支持向量机的补充之核函数	支持向量机的补充之核函数在之前的SVM章节中我们介绍了其具体的原理和大致推导过程，但是由于SVM只能应用于线性可分的数据，那么如果出现了线性不可分的情况怎么办呢，这就要引入今天的重点核函数。这种思想将在未来的深度学习中也会出现。为什么要使用核函数	之前我们在线性回归算法中讲到的，使用多项式扩展来考虑属性间有相关性的问题。	将非线性问题变成线性问题。核函数的核心思想与优势什么是核函数呢？核函数就是两个向量在隐式映射过后的高纬空间中的内积的函数。它的价值在于它虽然也是将特征进行从低维到高维的转换，但核函数它事先在低维上进行计算，而将实质上的分类效果表现在了高维上，也就说它避免了直接在高维空间中的复杂计算。简单的来说，就是原本我们将低维的特征值映射到高维度的空间的时候，我们需要先将需要进行内积的两个向量映射到高维度的空间，之后进行内积操作。而使用核函数的价值在于，先对两个向量进行内积操作，之后再进行高维度的映射，而这个结果可以和之前先对向量进行映射的操作的方法大致相同。众所周知，在低维度进行内积操作的话，其运算的复杂度显然比高维度好很多，所以采用核函数的思想被广泛的应用。常用的核函数那么常用的核函数有以下三种	多项式核：$\kappa(x_1, x_2)=(\langle x_1, x_2 \rangle + R)^d$	高斯核函数RBF：$\kappa(x_1, x_2) = exp (-\frac{||x_1 - x_2||}{2\sigma^2})$	线性核函数：$\kappa(x_1,x_2) = \langle x_1, x_2 \rangle$结语使用核函数的时候其实就把原来的目标函数中的内积换成核函数就好了，不过其实在Sklearn中这些是可以在调用函数的过程中选择的，个人觉得知道这个概念和思想就好了。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Vim作者对高效使用编辑器的建议]]></title>
      <url>/vim/2018/09/02/Vim%E4%BD%9C%E8%80%85%E5%AF%B9%E9%AB%98%E6%95%88%E4%BD%BF%E7%94%A8%E7%BC%96%E8%BE%91%E5%99%A8%E7%9A%84%E5%BB%BA%E8%AE%AE/</url>
      <content type="text"><![CDATA[							Vim作者对高效使用编辑器的建议（附原文地址）	原文链接：《Seven Habits of Effective Text Editing》前言本文摘自Vim主要作者Bram Moolennar的2000年11月在其个人网站发布的提高文本编辑效率的7个方法，个人认为从工具作者那里学习如何使用工具是最好的学习方式。本篇文章重点介绍了，达到高效使用编辑器的方法。第一部分、编辑文本1、快速在文本间移动	通过搜索的方式快速定位					使用\pattern的方式搜索			使用*直接对所在单词搜索			%在匹配括号间移动，[{返回之前的{，gd本地变量定义的位置。				注：应对搜索结果配置高亮	同样的内容不要打2遍					使用宏来记录你的重复性的操作（之后的文章会介绍）			活用.来重复上一步的文本操作。			自动纠错		对于经常犯的拼写错误，我们使用以下几种方式来避免。					自动补全：使用自动补全而不是自己手动输入变量名			自动纠错：对于自己经常犯的拼写错误可以使用如下配置来让vim自动纠错。:abbr Lunix Linux:abbr pn penguin # 当然也可以实现快速输入		第二部分、多文本操作	使用grep，ack，ag来对工程中所有的文件进行搜索		注：我们使用了ctrlp，CtrlSF来进行辅助	在一个终端窗口中进行分割来方便编辑：如:sp，:vs	让VIM和其他工具整合在一起使用：如:sh能进入bash模式等。第三部分、迭代优化自己的编辑器这部分个人觉得是最重要的，很多都认为使用VIM是先背快捷键然后熟练使用VIM，但是实际上和键盘盲打相似，是在使用中慢慢的逐渐提高使用VIM技巧，这里大佬给出了如下3个步骤：Step 1While you are editing, keep an eye out for actions you repeat and/or spend quite a bit of time on.观察自己在哪些步骤进行了很多的重复性输入工作。Step 2Find out if there is an editor command that will do this action quicker. Read the documentation, ask a friend, or look at how others do this.查找文档，询问朋友看有没有能让这些操作变得更快的方案。（去VIM的Wiki或者看我的专栏（笑））或者自己写一个宏或者脚本来自动化这些输入。Step3Train using the command. Do this until your fingers type it without thinking.不断的使用快捷命令，直到你的指头形成肌肉记忆。大佬的建议这里大佬说了一个很有意思的事情，就是希望让我们能养成一个习惯。首先，我们不需要去记住一个编辑器的所有的命令，这是完全浪费时间的 ( a complete waste of time )，每个人只需要知道其中 20% 左右的命令就够用了。其次，不要去优化只用一到两次的操作，把时间花在大量重复的操作上，写一个宏或者去互联网上查看别人的即决方案。最后，也是最重要的，将自己的解决方案和查到的命令记录下来。很多指令，在一段时间内经常使用，我们会熟记于心，但是随着一些原因停止了使用，之后再想回忆起来就需要花更多的时间。简单的来说，就是不做无用功，让我们的每一次努力达到可叠加的效果。最后的话关于为什么使用VIM，大佬给出了如下的话：Learning to drive a car takes effort. Is that a reason to keep driving your bicycle? No, you realize you need to invest time to learn a skill. Text editing isn&#39;t different. You need to learn new commands and turn them into a habit.好的编辑器是值得我们花时间学的，最后感谢大家订阅我的专栏。]]></content>
      <categories>
        
          <category> Vim </category>
        
      </categories>
      <tags>
        
          <tag> Vim </tag>
        
          <tag> 软件使用经验 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[赫拉利三部曲]]></title>
      <url>/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/30/%E8%B5%AB%E6%8B%89%E5%88%A9%E4%B8%89%E9%83%A8%E6%9B%B2/</url>
      <content type="text"><![CDATA[							赫拉利三部曲	1、作者简介尤瓦尔·赫拉利是以色列历史学家，出生于1976年，牛津大学历史学博士，耶路撒冷希伯来大学历史系教授，代表作为《人类简史》和《未来简史》。尤瓦尔·赫拉利擅长世界历史和宏观历史进程研究，是学界公认的“青年怪才”。2、主要内容这三步曲中，赫拉利分别讨论了三个话题，人的过去是什么，现在的人处在发展的什么位置，以及人的未来的发展方向是什么样子的。在人类简史中，赫拉利说明我们人类的有三次决定性的“发明”让我们变成现在的样子。第一次是出现了谈论抽象的能力，这使的我们拓展了表达的能力，使人类的合作范围扩大了许多。第二次则是出现了通过想象力来构建一种虚拟的政治秩序，来继续扩大可合作的范围。最终，通过金钱秩序，帝国秩序和信仰秩序这三种秩序，让全球人类进行合作成为了可能。但是这前两种秩序依旧有它的天花板，于是就带来了一种特殊的革命，“科技革命”。在这场革命中，人类首先承认了自己的无知，立根与观察和数学，这种科学是可以运用已有理论取得新能力，即进步是可以叠加的，就好比我们的瓷器曾今是世界最优秀的技术，但是由于仅仅是口口相传，没有留下准确的实验记录导致了烧制瓷器的技术不断的被“再发明”，最终被日本和欧洲超过。而在未来简史中，赫拉利讨论的是我们一直以来促进社会变革的到底是什么？他提出了一个非常惊世骇俗的观点：推动社会变革的不是我们对真实现实的认识，而是我们头脑中虚构的现实，也就是宗教的力量。他认为人文主义也是一种宗教，而如今这种宗教遇到了它的瓶颈。人文三件套，自我不可分割，我有自由意志，我最了解自己。如今分别通过“左右脑实验”，“老鼠实验”，“算法预测”得到了否定。他认为，人文这个宗教触碰到了上限急需变革，而接下来他认为的宗教是“数据教”，算法是数据驱动的，它的宗教三定义则是，价值观为信息要流动。需要遵守的戒律为不断生产和消化信息，最大化自己的信息流。当你遵守了戒律之后得到的许诺是，如果你允许信息自由流动，让万物之网越来越完善，它就能造福每一个人。好了我们知道了赫拉利说的我们未来可能的样子，过去我们是怎么发展的，现在再来说下《今日简史》中我们正在进行什么变革。总体有如下几个：	AI正在蚕食我们的很多岗位，被替代的岗位后，新创造的岗位出现在相对高层级的位置。被替代工作的人很难获得这些新出现的工作，而变成了无用之人。	教育方面，上百年来我们社会的教育都是阶段性的教育模式，现在随着社会的快速变革，我们不得不成为终身学习者已不被淘汰。	自由和平等将会受到前所未有的冲击。自由和平等本身就是一对相互对立的概念，鱼和熊掌不可兼得，你在追求平等的时候自然会有一部分失去自由，当追求自由的时候也必然会导致部分不平等的现象。而这次的技术革命很有可能进一步的撕裂这种不平等，甚至突破生物的边界，即有钱人可以通过技术改造自己，用技术让自己远超普通人。	恐怖主义横行。恐怖主义其实在当今社会造成的伤亡并不多，甚至没有每年车祸去世的人多。但是它造成的恐慌由于互联网的扩大变得具有非常大的恐慌效应。赫拉利做了这样一个比方，恐怖分子就像一只想摧毁瓷器店的苍蝇，但它自身没那么大的力量，于是它就钻进公牛的耳朵里，让公牛发疯，然后冲进瓷器店。	警惕战争。虽然目前的世界和平的原因，赫拉利认为是战争带来的收益已经远远小于战争带来的损耗，所以再次爆发大规模战争的可能性不高，不过凡事有例外。因为宗教的冲突带来的发动战争的动机依旧充分，同时由于武器的进步下一次的战争很可能是毁灭性的，所以我们要谨慎对待他。3、个人理解这三本书读完之后，和隔壁KK的《必然》这本书相比，有一种头皮发麻的感觉，内容甚至更像带着严密推理逻辑的恐怖片。但是个人整体还是乐观的，首先正如赫拉利所说，整个推进的过程是缓慢的，我们在中途是有大量的机会可以调整历史前进的方向。对于三本书中提到的大部分观点我是持赞同的态度，对于《人类简史》，我认同科技革命特殊的原因在于它的可叠加的特性，也认同人类相比其他物种，包括其他人科物种优秀的部分在于能够合作完成个体不能完成的大型工程这种看法。甚至部分赞同，每次革命都会有部分个体陷入更加悲惨的境地这种结论。对于《未来简史》，我也接受人文主义，带有一定的宗教的色彩，是科学和宗教互相作用的结果。也认同我们发展的未来是“数据教”这种方向。但是这种未来真的是一个“危机”么？我是不认同的，首先在三本书中多次提到的，革命之后带来的个体的苦难。这个逻辑看似很合理，像农业革命之后，人的脑容量没有之前采集时期的人类的脑容量大。法老奴役着个体去修建毫无意义的金字塔，现在我们看来这些都是一些少数人奴役集体的暴行。但是这种偏见是否是因为我们站在上帝视角才会这么觉得呢，在当时的历史条件下，这种社会结构完全是一种创举，而且也因为有这种社会结构他们才能在文明的碰撞中幸存下来。而雅典这种超前的民主体制，却因为自身的局限性，（沟通技术手段不够发达）无法扩张，并被罗马取代。也许我们穿越回古代，去询问这些古人，可能他们的幸福指数并不比现代低，因为每个人都有着信仰，有着自己人生的意义。同时，如果一个文明获得了进步，比如获得了人文主义，人类的生活质量得到了提高，你再希望它再退回到之前相对落后的制度中是不会成功的，这是一种必然的规矩，像楚门的世界中，走出了那个精心设计的小天地之后，再想回去就不那么容易了，这是一种不可逆的变化。而我们现在正在经历这种不可逆的变化，即人工智能的发展。历史上，人类的进步均是，朝着能让更多的人进行合作的方向进展的。尤其是互联网的出现，地球上的所有人都能轻易的进行沟通，交流，科技的思想从欧洲开始遍布全球。而人类也开始意识到自身的局限性，好比人对大数不敏感，在很多机械性的劳动中容易出错，人由于生理的原因不能进行24小时连轴的工作等等。这些是可以交给人工智能的，就像当年工业革命一样，人应该做自己擅长的事情，而不是全部都想雨露均沾，我们应该要做减法，而这是趋势。是的，不可否认，它会带来很多负面的事情，很多人失业不得不继续学习，也许会造成更大的贫富差距，造成许多不平等。但是整个过程是缓慢的，当今时代下的我们有充足的时间去学习新的知识，以让我们赶上新的科技发展的浪潮。而对于人工智能替代人类的这件事上，我的看法是，人工智能会成为我们的好帮手，但不会替代我们，至少短期内不会。总结一下，我个人是不太追求绝对的平等或者绝对的自由的，只要社会是像着规范化的发展，不同的阶级间仍保留充足的上升通道，我觉得便是一个非常nice的未来。至于人工智能，物联网Iot等新技术的出现，个人觉得是一种非常棒的未来，机会和风险并存，相比我们的父辈生活在这个时代是非常幸运的。至于特别的危机，就个体而言，我们只需做好自己手头的事情，顺浪潮而行，少一些抱怨和怨天忧人，多一份好奇的探索。]]></content>
      <categories>
        
          <category> 读书笔记 </category>
        
      </categories>
      <tags>
        
          <tag> Reading </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[LaunchBar小插件]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/26/LaunchBar%E5%B0%8F%E6%8F%92%E4%BB%B6/</url>
      <content type="text"><![CDATA[WeChatHelper简介—-WeCharHelper是基于Tkkk-iOSer的WeChatPlugin-MacOS微信插件的LaunchBar第三方插件支持。主要功能有以下两个：  在不打开微信的情况下，通过搜索快速定位到要聊天的对象，并打开相应的聊天窗口  通过搜索快速定位到聊天对象，并发送信息。全程不打开微信窗口。补充说明：支持使用拼音进行汉字的模糊搜索。依赖库—–  python &gt;= 3.6  requests  TKkk-iOSer/WeChatPlugin-MacOS（支持防撤回，微信免扫码认证，微信多开，自动回复）安装指南  HomeBrew安装 在终端中执行如下指令即可：      /usr/bin/ruby -e "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/master/install)"        通过HomeBrew安装python3      brew install python3        安装python3的Requests库      pip3 install requests            安装WeChatPlugin-MacOS WeChatPlugin-MacOS的Github地址，这里有详细的下载说明。    下载目录中的WeChatHelper.lbaction，双击即可安装使用说明  呼出WeChatHelper通过LaunchBar搜索WeChatHelper叫出WeChatHelper之后键入空格进入输入模式  发送模式 输入内容格式为“要搜索的微信名/发送的内容”时，下方会出现下拉菜单，选择你要发送的对象发送信息。  打开聊天窗口模式输入内容格式为“要搜索的微信名”，在下拉菜单中选择要打开窗口的对象即可注：由于launchBar的自身原因，在发送内容的时候，以下拉菜单中显示的消息为准，有时会延迟大约2ms左右。]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> LaunchBar </tag>
        
          <tag> Mac插件开发 </tag>
        
          <tag> python </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[集成学习]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/24/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
      <content type="text"><![CDATA[									集成学习	集成学习一句话版本集成学习的思想是将若干个学习器（分类器&amp;回归器）组合之后产生新的学习器。在学习这一章节中，老师提到了这个说法，我觉得非常言简意赅就直接引用了过来。集成学习算法的成功在于保证若分类器（错误率略小于0.5，即勉强比瞎猜好一点）的多样性，且集成不稳定的算法也能得到一种比较明显的提升。注：深度学习其实也可以看作是一种集成学习集成学习的作用采用集成学习的原因有以下四点：	分类器间存在一定的差异性，这会导致分类的边界不同，也就是说分类器是一个比较专精的专家，它有它自己一定的适用范围和特长。那么通过一定的策略将多个弱分类器合并后，就可以拓展模型的适用范围，减少整体的错误率，实现更好的效果。		注：不严谨的类比的话，就像弹性网络模型就可以看作是由LASSO回归和Ridge回归组成的集成学习。	对于数据集过大或者过小，过大会导致训练一个模型太慢，过小则会导致训练不充分，在这种情况下可以分别对数据集进行划分和有放回的操作产生不同的数据子集，然后使用数据子集训练不同的分类器，最终再将不同的分类器合并成为一个大的分类器。		注：这种方案的优势就在于，提高了准确度和训练速度，使得之前很难利用的数据得到了充分的利用	如果数据的划分边界过于复杂，使用线性模型很难描述情况，那么可以训练多个模型，然后再进行模型的融合。		注：这种特性就好比当初素描老师教我们画圆一样，画一个正方形，再用一堆小直线一点一点切成圆形。	对于多个异构的特征集的时候，很难进行融合，那么可以考虑每个数据集构建一个分类模型，然后将多个模型融合。		注：简单的来说就是公司有两个人都很厉害，但是偏偏不凑巧两个人打架，就不能把他们放一个部门里，得放不同部门一样。集成学习的三种思想BaggingBagging算法思想Bagging，这个名字就是从袋子里取的意思，本身便很形象的说明了这个算法的核心思想，即在原始数据集上通过有放回的抽样的方式，重新选择出S个新数据集来分别训练S个分类器，随后在预测的时候采用多数投票或者求均值的方式来判断预测结果。Bagging适用弱学习器的范围基本的弱学习器都能用，如Linear、Ridge、Lasso、 Logistic、Softmax、ID3、C4.5、CART、SVM、KNN。BoostingBoosting算法思想提升学习（Boosting），这个名字也很形象，在赛车游戏中氮气加速有时候界面就描述是boost，也就是越加越快，每次都比上一次更快，也就是说同Bagging是不一样的，Boosting是会根据其他的弱分类器的结果来更改数据集再喂给下一个弱分类器。准确的描述为，Boosting算法每一步产生弱预测模型(如决策树)，并加权累加到总模型中。它的意义在于如果一个问题存在弱预测模型，那么可以通过提升技术的办法得到一个强预测模型。注1: 如果每一步的弱预测模型的生成都是依据损失函数的梯度方式的，那么就称为梯度提升(Gradient boosting)注2：Boosting这个集成学习的思想就有点深度网络的意思了。Boosting适用范围提升学习适用于回归和分类的问题。Stacking之前提到了Bagging是把训练集拆成不同的子集训练多个学习器投票，而Boosting是根据学习器学习的结果来改动数据集，经过多层改动后试图获得一个更好的预测效果。Bagging和Boosting这两个集成学习其实并没有通过训练结果来改变弱分类器的参数。相对比而言，Stacking就激进许多，当然也复杂和困难许多，它首先训练出多个不同的模型，然后再以之前训练的各个模型的输出作为输入来新训练一个新的模型，换句话说，Stacking算法根据模型的输出是允许改其他分类器的参数甚至结构的，也正是因为这点sklearn中很少有stacking的内置的算法。1、Bagging算法随机森林(Random Forest)随机森林的思路很简单如下：	从样本集中用Bootstrap采样选出n个样本;	从所有属性中随机选择K个属性，选择出最佳分割属性作为节点创建决策树	重复以上两步m次，即建立m棵决策树	这m个决策树形成随机森林，通过投票表决结果决定数据属于那一类注：RF算法在实际应用中具有比较好的特性，应用也比较广泛，主要应用在分类、 回归、特征转换、异常点检测等。RF算法分析RF的主要优点：	训练可以并行化，对于大规模样本的训练具有速度的优势。	由于进行随机选择决策树划分特征列表，这样在样本维度比较高的时候，仍然具有比较高的训练性能。	给以给出各个特征的重要性列表。	由于存在随机抽样，训练出来的模型方差小，泛化能力强;。	RF实现简单。	对于部分特征的缺失不敏感。RF的主要缺点：	在某些噪音比较大的特征上，RF模型容易陷入过拟合。	取值比较多的划分特征对RF的决策会产生更大的影响，从而有可能影响模型的效果。RF的变种Extra TreeExtra Tree是RF的一个相当激进的变种，原理基本和RF一样，区别如下:	RF会随机采样来作为子决策树的训练集，而Extra Tree每个子决策树采用原始数据集训练;	RF在选择划分特征点的时候会和传统决策树一样，会基于信息增益、信息增益率、 基尼系数、均方差等原则来选择最优特征值。而Extra Tree会随机的选择一个特征值来划分决策树。Extra Tree因为是随机选择特征值的划分点，这样会导致决策树的规模一般大于RF所生成的决策树。也就是说Extra Tree模型的方差相对于RF进一步减少。在某些情况下，Extra Tree的泛化能力比RF的强。Totally Random Trees EmbeddingTRTE算法主要进行了两部分操作，第一部分是对数据进行操作，第二部分是对生成的决策树的位置信息转换成向量信息以供之后构建特征编码使用。抛开数据集上的操作，TRTE算法对RF的变种在于如何参考最终生成的多个决策树来给出预测结果。RF是采用投票的方式，而TRTE算法中，每个决策树会生成一个编码来对应叶子结点的位置信息，那么把所有的决策树对应相同的分类的编码合并起来，就可以用这一合并后的编码来代表它的特征了，预测时待预测样本经过这些决策树的预测也会得到这样一个合并后的编码，通过同训练好的类别的编码之间的差距的大小来预测这个样本应该属于哪一个类别。详细的说明说下：	TRTE是一种非监督的数据转化方式。将低维的数据集映射到高维，从而让映射 到高维的数据更好的应用于分类回归模型。	TRTE算法的转换过程类似RF算法的方法，建立T个决策树来拟合数据。当决策树构建完成后，数据集里的每个数据在T个决策树中叶子节点的位置就定下来了， 将位置信息转换为向量就完成了特征转换操作，这个转换过程有点像霍夫曼编码的过程。Isolation Forest这个算法是用来异常点检测的，正如isolation这个名字，是找出非正常的点，而这些非正常的点显然是特征比较明确的，故不需要太多的数据，也不需要太大规模的决策树。它和RF算法有以下几个差别：	在随机采样的过程中，一般只需要少量数据即可。	在进行决策树构建过程中，IForest算法会随机选择一个划分特征，并对划分特征随机选择一个划分阈值。	IForest算法构建的决策树一般深度max_depth是比较小的。算法思路如下：对于异常点的判断，则是将测试样本x拟合到T棵决策树上。计算在每棵树上该样本的叶子节点的深度$h_t(x)$ 。从而计算出平均深度 $h(x) $ 。然后就可以使用下列公式计算样本点x的异常概率值，$p(x,m)$的取值范围为$[0,1]$ ，越接近于1，则是异常点的概率越大。$$p(x,m) = 2^{-\frac{h(x)}{c(m)}}  $$$$c(m) = 2\ln(m-1)+\xi - 2\frac{m-1}{m}\ \ \ \ m为样本个数，\xi为欧拉常数$$注：这个公式可以简单的理解为越是出现在越深的层数，这个事件越不可能发生，足够深的情况基本上就可以判断为不可能发生是异常点2、Boosting算法Adaboost总览Adaboost全名为Adaptive Boosting，每轮迭代中会在训练集上产生一个新的学习器，然后使用该学习器对所有样本进行预测，以评估每个样本的重要性 (Informative)。换句话来讲就是，算法会为每个样本赋予一个权重，每次用训练好的学习器标注/预测各个样本，如果某个样本点被预测的越正确，则将其权重降低，否则提高样本的权重。权重越高的样本在下一个迭代训练中所占的比重就越大，也就是说越难区分的样本在训练过程中会变得越重要。整个算法的迭代的结束条件就是错误率足够小或者达到一定的迭代次数为止。注：整体的过程很像，分豆子，先把我们直接能看出来的区别的豆子分开，留下不太能区分开来的豆子，然后交给母上大人帮忙再分这种感觉。细节描述首先再重新强调下，从线性回归开始的两种思想，第一种是，设计出一个损失函数来代表预测结果，之后根据其应该为极小值和凸函数的特性，求原公式中的参数，一般是用导数等于0这种方式。第二种思想则是，当有多个变量共同作用结果的时候，我们给每个变量前加参数，也就是权值来控制变量的影响结果的能力。这两种贯穿了几乎所有机器学习的思想，当然在Adaboost中也不会例外，整体的步骤分两部分：	每次迭代都为新的弱学习器加权重，并根据损失函数计算得到这个权重。	根据这个新的学习器的预测结果，对每个样本特征的权重进行调整。算法构建之权重系数假设我们打算用的最终分类器为$G(x)$，第m次迭代用的弱分类器为$G_m(x)$，并给分类器前加权重$\alpha_m$已保证分类准的分类器得到足够的重视。于是得到下面公式3.1，公式3.2。$$f(x) = \sum_{m=1}^M \alpha_m G_m(x)\ \ \ 公式3.1 $$$$G(x) = sign(f(x)) = sign[\sum_{m=1}^M \alpha_m G_m(x)]\ \ \ 公式3.2$$有了一个对算法整体的数学表达以后，我们就可以根据它写出AdaBoost的损失函数如下公式3.3：注：到现在为止大家应该对$e^{(-y_i f(x))}$这种公式的函数图不陌生了，就不赘述了$$loss = \frac{1}{n}\sum_{i=1}^n I(G(x_i) \neq y_i) \leq \frac{1}{n}\sum_{i=1}^n e^{(-y_i f(x_i))}\ \ \ 公式3.3 $$有了损失函数了，那么$\alpha$在哪里呢，是要像SVM一样找个算法一起求么，显然不是了，如果那样子的话估计就不是集成算法了，它是一步一步求的，它只关心当前最好结果，类比算法中的贪心算法。于是，我们讨论第k-1轮和第k论迭代的关系：$$f_{k-1}(x) = \ sum_{j=1}^{k-1}\alpha_j G_j(x) \ \ \ 第k-1轮的函数  $$$$f_k(x) = \sum_{j=1}^k \alpha_j G_j(x) = f_{k-1}(x) + \alpha_k G_k(x) \ \ \ 第k轮函数$$根据loss函数的构成方法，我们很容易写出第k轮含有$\alpha_k$的公式如下公式3.4，之后对其进行求导就可以得到$\alpha_k$的公式3.5：$$loss(\alpha_k,G_k(x)) = \frac{1}{n} \sum_{i=1}^n e^{(-y_i(f_{m-1}(x)+\alpha_k G_k(x)))} \ \ \ 公式3.4$$$$\begin{split}&\alpha_k^* = \frac{1}{x}\ln(\frac{1-\varepsilon_k}{\varepsilon_k})\ \ \ 公式3.5 \\&\overline w_{ki} = e^{(-y_i f_{k-1}(x))} \\&\varepsilon_k = \frac{1}{n}\sum_{i=1}^n \overline w_{ki}I(y_i \neq G_m(x_i))\end{split}$$于是至此，我们就将弱分类器简单的连接在一起了，做好了下一步对数据样本特征值的权重调整的准备。算法构建之样本特征权重调整首先我们设定第k轮的数据集的权重分布为$D_k = (w_{k,1}, w_{k,2},... ,w_{k,n},)$。同时每次的$D_k$都是由$D_{k-1}$通过某种规律计算得到的，这种计算公式如下公式3.6：$$\begin{split} &w_k = \frac{w_{k-1,i}}{Z_{k-1}}e^{-\alpha_{k-1}y_i G_{k-1}(x_i)}\ \ \ 公式3.6 \\&Z_k = \sum_{i=1}^n w_{k,i}e^{-\alpha_k y_i G_k(x_i)} \end{split}$$可以看到这种第k次迭代开始前的数据的权重的调整是在根据第k-1次迭代中预测结果来进行调整的，换句话说，第k次迭代的数据集被第k-1次的训练修正了。算法构建之总览我们现在知道了每个弱分类器的权值是怎么够建的，也知道了Boosting算法中调整数据的部分是怎么调整的，算法的零件已经齐全，现在拼接起来如下：	初始化数据集权重为$\frac{1}{n}$，n为特征的个数。	加入弱分类器，并根据数据集feed进模型确定这个新加入的弱分类的权重。	根据最终训练的结果，调整数据集中不同特征的权值	重复2，3步骤直到符合结束条件，一般为达到预计准确度，或者为达到规定迭代次数。Gradient Boosting首先值得注意的是，GBDT算法，它有很多别名如GBT，GTB，BGRT，GBDT，MART，初学者很容易把它们当作是多个算法，比如我（笑。言归正传GBDT全名为Gradient Boosting Decision Tree。它也是Boosting算法的一种，它的算法推导相比之前算法的较为复杂，详细公式推导参考这篇文章，这里就不赘述了。算法大体的步骤如下：	算法每次迭代生成一颗新的决策树 		注：GBDT的核心其实是找出一堆决策树，然后让他们的结果累加得到最终的预测值	在每次迭代开始之前，计算损失函数在每个训练样本点的一阶导数和二阶导数	通过贪心策略生成新的决策树，通过等式计算每个叶节点对应的预测值		注：这步是通过目标函数求导得到的，需要利用第二步中的二阶导数和一阶导数，同时等式的推导中用到了泰勒公式	把新生成的决策树 $f_t(x) $ 添加到模型中：$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+f_t(x_i)$		注：这里我们会将模型替换为$\widehat{y}_i^{t} = \widehat{y}_i^{t-1}+\xi f_t(x_i)$，这里的$\xi$ 称之为步长或者学习率。增加ϵ因子的目的是为了避免模型过拟合。*		GBDT总结GBDT优点如下：	可以处理连续值和离散值	在相对少的调参情况下，模型的预测效果也会不错	模型的鲁棒性比较强。缺点如下：	由于弱学习器之间存在关联关系，难以并行训练模型Bagging和Boosting的总结	样本选择：					Bagging算法是有放回的随机采样			Boosting算法是每一轮训练集不变，只是训练集中的每个样例在分类器中的权重发生变化，而权重根据上一轮的分类结果进行调整			样例权重：					Bagging使用随机抽样，样例的权重			Boosting根据错误率不断的调整样例的权重值， 错误率越大则权重越大			预测函数：					Bagging所有预测模型的权重相等			Boosting算法对于误差小的分类器具有更大的权重			并行计算：					Bagging算法可以并行生成各个基模型			Boosting理论上只能顺序生产，因为后一个模型需要前一个模型的结果			Bagging是减少模型的variance(方差)，Boosting是减少模型的Bias(偏度)。	Bagging里每个分类模型都是强分类器，因为降低的是方差，方差过高需要降低是过拟合。Boosting里每个分类模型都是弱分类器，因为降低的是偏度，偏度过高是欠拟合。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[贝叶斯算法]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%B4%9D%E5%8F%B6%E6%96%AF%E7%AE%97%E6%B3%95/</url>
      <content type="text"><![CDATA[									贝叶斯算法	知识前置这个章节的机器学习，其实更像是一种概率论的学习，同时这也是机器学习和数据分析中非常重要的一环。如果学习遇到了困难非常推荐参考张宇考研概率论部分的内容。同时这一章的算法，也是在文本分类中使用的比较多的。名词解释：	先验概率：$P(A)$	条件概率：$P(A|B)$	后验概率：$P(B|A)$	全概率：$P(B) = \sum_{i=1}^n P(A_i)*P(B|A_i)$	贝叶斯公式：$P(A|B) = \frac{P(A)*P(B|A)}{\sum_{i=1}^n P(B|A_i)*P(A_i)}$概率分布：	高斯分布：简单的来说它的分布呈现的是正态分布的样子。参考链接	伯努利分布：伯努利分布是0-1分布，简单的来说就是那种仍硬币的概率分布。参考链接	多项式分布：是伯努利分布的推广，不再是只有两种情况，有多种情况的概率分布。参考链接贝叶斯核心思想：找出在特征出现时，各个标签出现的概率，选择概率最大的作为其分类。朴素贝叶斯我们来“望文生义”的理解这个算法，贝叶斯指的就是上面的贝叶斯公式，而朴素则指的是“特征之间是独立的”这个朴素假设。假设有给定样本X，其特征向量为$(x_1,x_2,...,x_m)$，同时类别为$y$。算法中使用公式2.1表达在当前特征下将类别y预测正确的概率。由于特征属性之间是假定独立的，所以$P(x_1,x_2,...x_m)$是可以直接拆开的，故根据这个特性优化，得到公式2.2。由于样本给定的情况下，$P(x_1,x_2,...,x_m)$的值不变，故研究概率最大的问题只需要研究公式2.2等号右侧上面的部分，最终写出预测函数公式2.3。$$P(y|x_1,x_2,...,x_m) = \frac{P(y)P(x_1,x_2,...,x_m|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.1  $$$$P(y|x_1,x_2,...,x_m) = \frac{P(y)\prod_{i=1}^m P(x_i|y)}{P(x_1,x_2,...,x_m)}\ \ \ 公式2.2$$$$\hat{y} = arg\ max_y P(y) \prod_{i=1}^m P(x_i|y) \ \ \ 公式2.3$$到这里，算法的流程就很显而易见了，和softmax算法类似，让预测正确的概率最大即可，具体计算流程如下：设$x = {a_1,a_2,...a_m}$为带分类项，其中a为x的一个特征属性，类别集合$C={y_1,y_2,...y_n}$	分别计算所有的$P(y_i|x)$，使用上述公式2.3	选择$P(y_i|x)$最大的$y_i$作为x的类型其他朴素贝叶斯高斯朴素贝叶斯在上述贝叶斯算法中的特征是离散的，那么考虑特征属虚连续值时，且分布服从高斯分布的情况下。用高斯公式（公式3.1）代替原来计算概率的公式。那么根据训练集中，对应的类别下的属性的均值和标准差，对比待分类数据中的特征项划分的各个均值和标准差，即可得到预测类型。$$p(x_k|y_k) = g(x_k,\eta_{y_k},\sigma_{y_k}) = \frac{1}{\sqrt{2 \pi}\sigma}e^{-\frac{(x-\eta_{y_k})^2}{2\sigma_{y_k}^2}}\ \ \ 公式3.1$$伯努利朴素贝叶斯特征值的取值是布尔型的，是有true和false，符合伯努利分布，那么其$P（x_i|y_k）$的表达式如下公式3.3。$$P（x_i|y_k）= P(x_i = 1 | y_k)*x_i + (1-P(x_i=1|y_k))(1-x_k)\ \ \ 公式3.2$$注：这意味着没有某个特征也可以是一个特征，其中公式3.2其实是把两个不同条件的概率公式融合在一起了，这种方法也在逻辑回归中使用过多项式朴素贝叶斯特征属性分布服从多项分布时，得到如下公式3.3，公式的来源简单的来说就是已知盒子中红球和所有球的总个数，求从盒中摸到红球的概率差不多。其中$N_{y_k x_i} $为类别$y_k$下，特征$x_i$出现的次数，$N_{y_k}$ 指的是类别 $y_k$ 下，所有特征出现的次数。$$P(x_i|y_k) = \frac{N_{y_k x_i} + \alpha}{N_{y_k} + \alpha n}  $$注：待预测样本中的特征xi在训练时可能没有出现，如果没有出现，则$N_{y_k x_i} $ 值为0，如果直接拿来计算该样本属于某个分类的概率，结果都将是0。所以在分子中加入α，在分母中加入αn可以解决这个问题。贝叶斯网络由于之前朴素贝叶斯，前提条件是假定特征值之间没有关系，这显然是不现实的而贝叶斯网络正是解决这个问题的。其关键方法是图模型，我们构建一个图模型，把具有因果联系的各个变量联系在一起。贝叶斯网络的有向无换图中的节点表示随机变量，连接节点的箭头表示因果关系。简单的来说贝叶斯网络就是模拟人的认知思维推理模式的，用一组条件概率以及有向无换图对不确定关系推理关系建模。而这种方式在深度学习之前是很受欢迎的，它和之后的隐马尔可夫被使用作为提取特征的工具，而现在渐渐的过度到了深度学习。贝叶斯网络工作原理首先贝叶斯网络的实质就是建立一个有向无环图，其中方向代表因果关系。仔细思考一下，为什么是有向无环图，是因为如果是有环的话，就会有节点是自己依赖于自己，显然这样是有问题的。具体贝叶斯工作的核心原理可以理解为，根据人已知的经验或者其他手段，规定一些完全没有依赖于其他事件的事件发生的概率，随后根据制作的贝叶斯网络（因果关系图）推算出不同事件发生的概率。这个过程有点像是在做一个概率论的期末考试题，已知A，B，C的概率和ABCD之间转换的关系，问在发生了BC条件下，发生D的概率。大体就是这样一种感觉。事例如下图：其中$x_1,x_2,x_3$独立，则$x_6,x_7$独立，$x_1,x_2,x_3,...,x_7$的联合概率分布如下：$$p(x_1,x_2,...,x_7) = p(x_1)p(x_2)p(x_3)p(x_4|x_1,x_2,x_3)p(x_5|x_1,x_3)p(x_6|x_4)p(x_7|x_4,X_5)$$实际上这部分的概率计算，其实就是根据初始条件和转移方式，求的目标的概率这样的过程。和之前常用的最大似然估计算法对比，贝叶斯的这一系列算法考虑了先验概率，而最大似然估计算法没有，在最大似然估计算法中其实相当于默认了先验概率是相同的。注：最大后验概率MAP其实可以看作是贝叶斯算法和最大似然估计算法结合的应用]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[聚类算法（下）]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8B/</url>
      <content type="text"><![CDATA[							聚类算法（下）	聚类算法上中讲了大名鼎鼎的K-Means算法及其优化变种，在这篇中几种讲述两位两种不同思路的聚类算法。层聚类算法传统层聚类算法—AGNES和DIANA算法层次聚类和K-Means的思路不太一样，它的思路有点像是决策树，按照层次进行分解，知道满足某种条件为止，传统的层次聚类分为自底而上，和自上而下两类：	凝聚的层次聚类:		这类算法是采用采用自底向上的策略，其中的代表便是AGNES算法(AGglomerative Nesting)，它的核心思想是：最初将每个对象作为一个簇，然后这些簇根据某些准则被一步一步合并，两个簇间的距离可以由这两个不同簇中距离最近的数据点的相似度来确定。聚类的合并过程反复进行直到所有的对象满足簇数目。	分裂的层次聚类：		和凝聚的层次聚类相反，这种是采用自顶向下的策略，代表算法为DIANA算法(DIvisive Analysis)。其核心思想是：首先将所有对象置于一个簇中，然后按照某种既定的规则逐渐细分为越来越小的簇(比如最大的欧式 距离)，直到达到某个终结条件(簇数目或者簇距离达到阈值)。在AGNES算法中都提到了，簇是根据某些原则进行分裂或者合并的，而这个原则就是簇间距离。计算簇间距离的方法有最小距离（SL聚类），最大距离（CL聚类）以及平均距离（AL聚类），具体的说明如下：	最小距离（SL聚类）		选择两个聚簇中最近的两个样本之间的距离（Single/Word-Linkage）		注：得到的模型容易形成链式结构	最大距离（CL聚类）		选择两个聚簇中最圆的两个眼本的距离（Complete-Linkage）		注：如果出现了异常值的话，那他们的构建很容易受这个异常值的影响。	平均距离（AL聚类）		选择两个聚类中的平均值（Average-Linkage聚类算法）或者中值（Median-Linkage聚类法）AGNES和DIANA算法优缺点如下：	简单，理解容易。	合并点/分裂点选择不太容易。	合并/分类的操作不能进行撤销。	由于执行效率较低$O(t*n^2)$，$t$为迭代次数，$n$为样本点数。层次聚类优化算法之前我们看到了传统的层次聚类算法，由于其执行效率太低，且不能动构建的的特点，显然不适合大数据集。于是我们在此基础上引入了BIRCH算法和CURE算法。BIRCH算法BIRCH (balanced iterative reducing and clustering using hierarchies) 算法，英文的全称翻译过来以后是平衡迭代削减聚类算法，其构成和我们考研数据结构中学过的B+树非常的类似，甚至很多特性都是相同的，具体的说它构建的树叫做CF（Cluster Feature）-Tree。	节点，即簇的结构：		既然是树，那么就不得不提它的节点的结构了。在BIRCH构建CF树的过程中，每个节点等于说是存放了它之下所有节点的特征，于是他在节点中存放了如下的三部分数据。					N，指在这个节点中有多少个样本点。			LS，指的是这个节点中的样本相应特征的和。			LS，指的是这个节点中的样本相应特征的特征的平方和。			节点之间，节点和子节点，以及叶子结点之间的关系		节点和其子节点是包含的关系，也就是父节点中的N，LS以及SS是其所有子节点的和。而相应的样本点的具体信息指包含在底层节点中（叶子结点的子节点），同时叶子结点构成一个单项链表，同时有一个指针指向其表头。这点的特性是同B+树高度一致的。	最多子女个数，以及分裂判定		和B+树一样，对于树构建中的分叉个数是有限制的，这个限制需要提前给出，即分支因子。同时，值得注意的是，一般而言在构建节点簇的中心点的时候，一般选用第一个进入这个节点的样本点作为中心点，然后根据指定的该簇和中心点限定的距离，即类直径，其往往通过LS和SS算出。判断新入的点是否可以划入该簇，而分裂节点的时候，往往以这个初始点进行分割。综上我们可以看出，BIRCH算法的本质其实就是动态的插入样本点，然后动态的根据规则构建一个类B+树。它的优点是动态建树且效率高是线性效率，即每个样本点都是一次性插入的，同时也节省内存，所以非常适合大数据集。不过遗憾的是它也是采用距离作为分类标准，故只适合分布呈凸形或者球形的数据集、且需要给定聚类个数和簇之间的相关参数，而这些对节点CF的限制可能导致簇类结果和真实不太一致。注1：BIRCH不依赖给定的待分类簇数量K，但是给定了K值最好，若不一定K值，最终CF-Tree的叶子结点树木就是最终分类的簇的数目。注2:BIRCH算法在训练大规模数据集的时候，和mini-batch K-Means相比，BIRCH算法更加适合类别数量K比较多的情况。注3：由于类直径是通过LS和SS算出的，所以当特征维度超过20～30左右的时候，不建议使用该算法。CURE算法（使用代表点的聚类法）CURE（Clustering Using REpresentatives），该算法先把每个数据点看成一类，然后合并距离最近的类直至类个数为所要求的个数为止。但是和AGNES算法的区别是：取消了使用所有点，或用中心点+距离来表示一个类，而是从每个类中抽取固定数量、 分布较好的点作为此类的代表点，并将这些代表点乘以一个适当的收缩因子，使它们更加靠近类中心点。代表点的收缩特性可以调整模型可以匹配那些非球形的场景，而且收缩因子的使用可以减少噪音对聚类的影响。CURE算法的优点是能够处理非球形分布的应用场景，同时彩娱乐随机抽样和分区的方式可以提高算法的执行效率。密度聚类算法密度聚类方法的指导思想是：只要样本点的密度大于某个阀值，则将该样本添加到最近的簇中。这类算法可以克服基于距离的算法只能发现凸聚类的缺点，可以发现任意形状的聚类，而且对噪声数据不敏感。不过这种计算的复杂度高，计算量大。密度聚类算法的常用算法有DBSCAN和密度最大值算法。DBSCAN算法DBSCAN（Density-Based Spatial Clustering of Applications with Noise），将簇定义为密度相连的点的最大集合，能够将足够高密度的区域划分为簇，并且在具有噪声的空间数据上能够发现任意形状的簇。其核心思路是用一个点的ε邻域内的邻居点数衡量该点所在空间的密度，该算法可以找出形状不规则的cluster，而且聚类的时候事先不需要给定cluster的数量。DBSCAN算法流程它的算法流程如下：	如果一个点$x$的$\varepsilon$领域内包含m个对象，则创建一个x作为核心对象的新簇。	寻找并合并核心对象直接密度可达的对象	没有新点可以更新簇的时候，算法结束注：1. 每个簇至少包含一个核心对象；2. 非核心对象可以是簇的一部分，构成簇的边缘；3. 包含过少对象的簇被认为是噪声；4. 最大的密度相连对象的集合C为密度聚类中的一个簇，它满足两个属性，Maximality和Connectivity，Maximality指的是若$x$属于C，$y$从$x$密度可达，那么$y$也属于C，Connectivity指的是，若$x$和$y$都属于C，那么$x$和$y$是密度相连的。DBSCAN相关名词解释其中提到的定义有$\varepsilon$领域，密度，MinPts，核心点，边界点，噪音点，直接密度可达，密度可达，密度相连。他们的解释如下：	$\varepsilon$邻域($\varepsilon$ neighborhood）：给定对象在半径$\varepsilon$的区域。	密度(density)：在$\varepsilon$领域中的$x$的密度，是一个整数依赖于半径$\varepsilon$，$N_{\varepsilon}(X) $指的是半径内的点的个数。		$$		p(x) = |N_{\varepsilon}(X)|  		$$	MinPts：指得是判定该点是不是核心点的时候使用的阀值，记为M	核心点（core point）：如果$p(x) \geq M$ ,那么称$x$为$X$的核心点，记由$X$中所有核心点构成的集合为$X_c$，并记$X_nc$ 表示由$X$中所有非核心点构成的集合。通俗的来说， 核心点是密度达到一定阀值的的点。	边界点（border point）：如果非核心点$x$的$\varepsilon$邻域中存在核心点，那么认为$x$为$X$的边界点。通俗来讲就是密度特别稠密的边缘地带，也就是簇的边缘部分。	噪音点（noise point）：集合中除了边界点和核心点之外的点都是噪音点，所有噪音点组成的集合叫做$X_noi$，显然这些点就是对应稀疏区域的点。	直接密度可达：这个是密度聚类中最重要的概念，它指的是给定一个对象集合 $X$，如果$y$是在$x$的$\varepsilon$邻域内，而且$x$是一个核心对象，可以说对象y从对象$x$出发是直接密度可达的	密度可达：如果存在一个对象链$p_1, p_2,...,p_m $ ，如果满足$p_{i+1}$是从$p_i$直接密度可达的，那么称$p_m$是从$p1$密度可达的，简单的来说就像铁链环环相扣差不多。	密度相连：在集合$X$中，如果存在一个对象$o$，使得对象$x$和$y$是从$o$关于$\varepsilon$和$m$密度可达的，那么对象$x$和$y$是关于$\varepsilon$和$m$密度相连的。DBSCAN算法优缺点优点: 	不需要事先给定cluster的数目	可以发现任意形状的cluster	能够找出数据中的噪音，且对噪音不敏感 	算法只需要两个输入参数 	聚类结果几乎不依赖节点的遍历顺序缺点:	DBSCAN算法聚类效果依赖距离公式的选取，最常用的距离公式为欧几里得距离。但是对于高维数据，由于维数太多，距离的度量已变得不是那么重要	DBSCAN算法不适合数据集中密度差异很小的情况MDCA密度最大值聚类算法MDCA(Maximum Density Clustering Application)算法基于密度的思想引入划分聚类中，能够自动确定簇数量并发现任意形状的簇。另外MDCA一般不保留噪声，因此也避免了阈值选择不当情况下造成的对象丢弃情况。注：MDCA的算法和AGNES非常相像，不同的是最初的初始簇确定是通过密度来确定的。MDCA算法思路MDCA算法核心一共分三步，划分、合并簇以及处理剩余节点三部分。	将数据集划分为基本簇：					对数据集X选取最大密度点$P_{max}$ ，形成以最大密度点为核心的新簇$C_i$，按照距离排序计算出序列$S_{p_max}$,对序列的前M个样本数据进行循环判断，如果节点的密度大于等于$density_0$ ，那么将当前节点添加$C_i$中。			循环处理剩下的数据集X，选择最大密度点$P_{max}$，并构建基本簇$C_{i+1}$，直到X中剩余的样本数据的密度均小于$deansity_0$。			使用凝聚层次聚类的思想，合并较近的基本簇，得到最终的簇划分：					在所有簇中选择距离最近的两个簇进行合并，合并要求是：簇间距小于等于$dist_0$，如果所有簇中没有簇间距小于$dist_0$的时候，结束合并操作			处理剩余节点，归入最近的簇					最常用、最简单的方式是：将剩余样本对象归入到最近的簇。		MDCA算法名词解释最大密度点：如字面意思，就是密度最大的点，密度计算公式一般取DBSCAN算法中的密度计算公式。有序序列$S_{p_{max}}$：根据所有对象与最大密度点的距离进行排序。密度阈值$density_0$：当节点的密度值大于密度阈值的时候，认为该节点属于一个 比较固定的簇，在第一次构建基本簇的时候，就将这些节点添加到对应簇中，如果小于这个值的时候，暂时认为该节点为噪声节点。簇间距离：对于两个簇C1和C2之间的距离，采用两个簇中最近两个节点之间的距离作为簇间距离。M值：初始簇中最多数据样本个数]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[聚类算法（上）]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/19/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95-%E4%B8%8A/</url>
      <content type="text"><![CDATA[							聚类算法（上）	聚类算法很多，所以和讲回归算法一样，分成了上下，上中主要讲了传统的K-Means算法以及其相应的优化算法入K-Means++，K-Means||和Canopy等。下中主要讲了另外两种的思路的聚类算法，即层次聚类和密度聚类。什么是聚类聚类算就是怼大量未知标注的数据集，按照数据内部存在的数据特征将数据集划分为多个不同的类别，使类别内的数据比较相似，类别之间的数据相似度比较小，属于无监督学习。从定义就可以看出，聚类算法的关键在于计算样本之间的相似度，也称为样本间的距离。相似度/距离计算公式说到聚类算法，那肯定核心就是计算距离的公式了，目前常用的有以下几种。闵可夫斯基距离（Minkowski）：公式2.1	当p为1的时候是曼哈顿距离（Manhattan）：公式2.2	当p为2的时候是欧式距离（Euclidean）：公式2.3					标准化欧式距离：				这个距离的计算方式如同其字面意思，标准化欧式距离就是对欧式距离的标准化。标准化的正常定义为，$X^* = \frac{X - \overline X}{s}$，这个$s$指的就是方差，而方差的计算公式为$s = \sqrt{\frac{\sum_{i=1}^n(x_i - \overline X)^2}{n}}$，所以其标准化公式如下公式2.5。			当p为无穷大的以后是切比雪夫距离（Chebyshev）：公式2.4$$dist(X,Y)= \sqrt[p]{\sum_{i=1}^{n} |x_i - y_i|^p}\ \ \ 公式2.1$$$$M\_dist=\sum_{i=1}^n|x_i-y_i| \ \ \ 公式2.2$$$$E\_dist = \sqrt{\sum_{i=1}^n|x_i-y_i|^2} \ \ \ 公式2.3$$$$C\_dist = max_i(|x_i-y_i|)\ \ \ 公式2.4$$$$S\_E\_D = \sqrt{\sum_{i=1}^n(\frac{x_i-y_i}{s_i})^2}\ \ \ 公式2.5 $$夹角余弦相似度（Cosine）：使用这个公式的时候，需要注意的是，这里的相似之的是同一个方向上的，而同一个方向上的两个点可能距离是非常远的。比如一个吻张灏总分别出现单词A 10次，单词B 20次，另一个文章中出现单词A 100次，单词B 200次，这时候如果使用欧几里得距离的话，这两个文章是不相似的，然而显然这两个单词的比例相似很能说明这两个文章其实是有关系的，所以在文章的相似度的判别中使用夹角余弦相似度比较合适，公式如下2.6。个人理解为，其是从距离以外的衡量相似度的另一个维度的指标。$$\cos(\theta)  = \frac{\sum_{k=1}^n x_{1k}x_{2k}}{\sqrt{\sum_{k=1}^n x_{1k}^2} * \sqrt{\sum_{k=1}^n x_{2k}^2}} = \frac{a^T · b}{|a||b|}\ \ \ 公式2.6$$KL距离（相对熵）：思考下条件熵的定义，简单的来说就是在放生一件事情的时候，发生另一件事的概率。公式如下公式2.7.注：这里书的概率不是实指概率，而是熵表达的含义。这个公式其实就是条件熵的公式。$$D(P|Q)=\sum_x P(x)\log(\frac{P(x)}{Q(x)})\ \ \ 公式2.7$$杰卡德相似系数(Jaccard)：这个很好理解，它的核心就是使用两个集合的交集和并集的比率来代表两者的相似度，也就是说重合的越多越相似。公式如下，公式2.8.$$J(A,B) = \frac{|A\bigcap B|}{|A \bigcup B|}  $$$$dist(A,B) = 1-J(A,B) \ \ \ 公式2.8 $$Pearson相关系数：这个就是考研数学中的相关系数，表达就是两者之间的想关系，所以直接拿来用就好了，公式如下公式2.9。$$\rho_{XY} = \frac{Cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} = \frac{E[(X-E(X))(Y-E(Y))]}{\sqrt{D(X)}\sqrt{D(Y)}} = \frac{\sum_{i=1}^n(X_i - \mu_x)(Y_i - \mu_Y)}{\sqrt{\sum_{i=1}^n(X_i - \mu_X)^2}*\sqrt{\sum_{i=1}^n(Y_i - \mu_Y)^2}}  $$$$dist(X,Y) = 1 - \rho_{XY}\ \ \ 公式2.9$$聚类的思想给定一个有M个对象的数据集，构建一个具有k个簇的模型，其中k&lt;=M。满足 以下条件:	每个簇至少包含一个对象	每个对象属于且仅属于一个簇 	将满足上述条件的k个簇成为一个合理的聚类划分基本思想:对于给定的类别数目k，首先给定初始划分，通过迭代改变样本和簇的隶属关系，使的每次处理后得到的划分方式比上一次的好，即总的数据集之间的距离和变小了K-Means 系列K-Means算法K-means的核心算法如下：# 假设输入样本T为x1,x2,x3,...,xm初始化k个类别的中心点a1,a2,a3,...,akwhile not EndCondition :	1.根据每个样本和中心点的欧几里得距离，选择最近的中心点作为自己的类别	2.更新每个类别的中心点aj，为隶属该类别的所有的样本的均值# EndCondition有迭代次数，最小平方误差MSE，簇中心点变化率。再循环中的第二步，我们移动了中心点的位置，把中心点移到了隶属于该中心点类别的所有样本的中间，并使用样本的均值作为位置。这样子看似是拍脑袋想的移动策略，其实是可以推导出来的。正如聚类算法思想所指出的，我们要让所有的点到自己的分类的中心点的欧几里得距离最小，所以我们设置目标放称为公式4.1，公式中的1/2是为了之后求导运算方便。我们为了让目标函数尽可能的小，所以使用了之前一直在使用的思考方式，对其使用梯度下降算法，求导后得到公式4.2，之后令其等于0，就得到了公式4.3。$$J(a_1,a_2,a_3,...,a_k) = \frac{1}{2}\sum_{j=1}^K \sum_{i=1}^n (x_i - a_j)^2 \ \ \ 公式4.1$$$$\frac{\partial J}{\partial a_j} = \sum_{i=1}^{N_j}(x_i-a_j)\ \ \ 公式4.2$$$$a_j = \frac{1}{N}\sum_{i=1}^{N_j} x_i \ \ \ 公式4.3 $$最后这个看似不错的算法，其实有着不小的缺点，那就是初值敏感。我们来仔细想一想，如果两个不小心随机生成的初值落到了一个类别中，两者的距离还特别近，这中情况下就很难正确分类了。除此之外，由于移动策略中使用的是均值，也就是说如果集合中含有非常大的误差点的话，这样子会是中心点的设置偏离正确点很远，所以很多时候我们改用中值来更新中心点，这就是我们说的K-Mediods聚类，即K中值聚类。总结下K-means算法优点：	理解容易，聚类效果不错	处理大数据集的时候，该算法可以保证较好的伸缩性和高效率	当簇近似高斯分布的时候，效果非常不错缺点：	 K值是用户给定的，在进行数据处理前，K值是未知的，不同的K值得到的结果也不一样	对初始簇中心点是敏感的 	不适合发现非凸形状的簇或者大小差别较大的簇 	特殊值(离群值)对模型的影响比较大二分K-Means算法由于K-Means对初始中心点非常敏感，我们这里就尝试着通过二分法弱化初始中心点。这种算法的具体步骤如下：# 把所有样本数据作为一个簇放到队列中while not EndCondition:	1.从队列中选择一个簇，使用K-means划分为两个簇	2.将划分好的两个簇放回队列# EndCondition 为簇的数量，最小平方误差，迭代次数等# 选择簇的手段有两种1.使用SSE 2.选择数据量最多的簇我们在这个算法中提到了SSE，这个可以是簇内所有样本点，到其中心点的距离的总和，代表着簇内的点是不是高度相关。计算公式如下公式4.4。$$SSE = \sum_{i=1}^n w_i(y_i - \hat y_i)^2\ \ \ 公式4.4$$可以看出在这种算法下，很好的避开了，两个中心点都在一起的情况。K-Means++和K-Means||K-Means++做的改善，是直接对初始点的生成位置的选择进行优化的，他的初始点生成策略如下：	从数据集中任选一个节点作为第一个聚类中心	对数据集中的每个点x，计算x到所有已有聚类中心点的距离和D(X)，基于D(X)采用线性概 率选择出下一个聚类中心点(距离较远的一个点成为新增的一个聚类中心点)	重复步骤2直到找到k个聚类中心点可以看出，K-Means++企图生成相聚距离较远的几个中心点。但是缺点也是显而易见的，由于聚类中心点选择过程中的内在有序性，在扩展方面存在着性能方面的问题，即第k个聚类中心点的选择依赖前k-1个聚类中心点的值。而K-Means||就是针对K-Means++缺点作出了的优化，主要思路是改变每次遍历时候的取样规则，并非按照K-Means++算法每次遍历只获取一个样本，而是每次获取 K个样本，重复该取样操作$O(\log{n}w	z)$ 次，然后再将这些抽样出来的样本聚类出K个点，最后使用这K个点作为K-Means算法的初始聚簇中心点。注：一般5次重复采用就可以保证一个比较好的聚簇中心点。Canopy算法Canopy属于一种“粗略地”聚类算法，简单的来说就是，不那么追求自动获得最优解，而是引入了一种人为规定的先验值进行聚类，具体步骤如下：# 给定样本列表L=x1,x,2...,xm以及先验值r1和r2(r1 &gt; r2)for P in L:	计算P到所有聚簇中心点的距离(如果不存在聚簇中心，那么此时点P形成一个新的聚簇)，并选择出最小距离D(P,aj)	if D &lt; r1:		# 表示该节点属于该聚簇		添加到该聚簇列表中	if D &lt; r2:		# 表示该节点不仅仅属于该聚簇，还表示和当前聚簇中心点非常近，		将该聚簇的中心点设置为P，并将P从列表L中删除	if D &gt; r1:		节点P形成一个新的聚簇	if EndCondition:		# 结束条件为直到列表L中的元素数据不再有变化或者元素数量为0的时候		break		注：Canopy算法得到的最终结果的值，聚簇之间是可能存在重叠的，但是不会存在 某个对象不属于任何聚簇的情况显然，这种算法虽然快，但是很难生成满足我们应用的模型，所以通常我们将它作为解决K-Means初值敏感的方案，他们合在一起就是Canopy+K-Means算法。顺序就是先使用Canopy算法获得K个聚类中心，然后用这K个聚类中心作为K-Means算法。这样子就很好的解决了K-Means初值敏感的问题。Mini Batch K-Means算法Mini Batch K-Means算法是K-Means算法的一种优化变种，采用小规模的数据子集，来减少计算时间。其中采用小规模的数据子集指的是每次训练使用的数据集是在训练算法的时候随机抽取的数据子集。Mini Batch K-Means算法可以减少K-Means算法的收敛时间，而且产生的结果效果只是略差于标准K-Means算法。它的算法步骤如下：# 首先抽取部分数据集，使用K-Means算法构建出K个聚簇点的模型while not EndCondition:	1.抽取训练数据集中的部分数据集样本数据，并将其添加到模型中，分配给距离最近的聚簇中心点	2.更新聚簇的中心点值# EndCondtion同K-Means一样，可以理解为不停的进行K-Means算法。聚类算法衡量标准聚类算法的衡量标准有很多，包括均一性、完整性、V-measure、调整兰德系数（ARI ，Adjusted Rnd Index）、调整互信息(AMI，Adjusted Mutual Information)以及轮廓系数等等。均一性、完整性以及V-measure均一性：一个簇中只包含一个类别的样本，则满足均一性。其实也可以认为就是正确率，即每个聚簇中正确分类的样本数占该聚簇总样本数的比例和。其公式如下公式5.1。$$p = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(K_i)}\ \ \ 公式5.1  $$完整性：同类别样本被归类到相同簇中，则满足完整性。每个聚簇中正确分类的样本数占该类型的总样本数比例的和，通俗的来说就是，我们已分类类别中，分类正确的个数。其公式如下，公式5.2：$$r = \frac{1}{k}\sum_{i=1}^k \frac{N(C_i == K_i)}{N(C_i)}\ \ \ 公式5.2 $$在实际的情况中，均一性和完整性是往往不能兼得的，就好像抓特务时的矛盾一样，到底是保证每个抓的人都是特务，还是宁可错抓也不放过一个特务，之间的取舍很难把握。所以再一次贯彻，鱼和熊掌不可兼得，我们就加权，于是得到的就是V-measure，其公式如下公式5.3：$$V_\beta = \frac{(1+\beta^2)·pr}{\beta^2·p + r}\ \ \ 公式5.3  $$调整蓝德系数ARI兰德系数（RI，Rand index），我用中文看了不少讲兰德系数的博客，其中的文字说明几乎都是相同的，对个人的理解帮助不是特别大，于是用英文查的。最终理解了这个系数的参数的意思，想看英文说明的，个人觉得还挺好懂的参考这里。以下是我个人的讲解。首先，将原数据集中的元素进行两两配对形成一个新的数据集，我们称之为S数据集。这时候，我们将原数据集，根据两种不同的策略分别划分成r份和s份，并对这两个数据集命名为X和Y。在这里我们可以看出，X和Y的元素是相同的，只是他们的划分方式不同。接下来我们来思考，S数据集中，每个元素中的两个样本，在X和Y中只有两种可能，就是两个样本都在一个子集中，或者不在一个子集中，那么对于S中的一个元素，只有四种可能性。	两个样本都在X的一个子集中，也同时在Y的一个子集中，这些元素的个数是a	两个样本横跨X的不同子集，也同时在Y中横跨Y的不同子集，这些元素的个数是b	两个样本都在X的一个子集中，但在Y中横跨Y的不同子集，同理数量为c	两个样本横跨X的不同子集，但在Y的的一个子集中，同理数量为d有了上述的理解，我们再看蓝得系数公式，公式5.4，我们就不难理解了。RI的取值在$[0,1]$之间，越靠近1代表越相似。$$RI = \frac{a+b}{a+b+c+d} = \frac{a+b}{C_2^n} \ \ \ 公式5.4  $$接下来引入，调整兰德系数(ARI，Adjusted Rnd Index)，ARI取值范围$[-1,1]$，值越大，表示聚类结果和真实情况越吻合。从广义的角度来将，ARI是衡量两个数据分布的吻合程度的，公式5.5如下：$$ARI = \frac{RI - E[RI]}{max(RI) - E[RI]}\ \ \ 公式5.5$$调整互信息(AMI，Adjusted Mutual Information)调整互信息，整体的流程很像ARI，AMI则是对MI进行调整。而MI是使用信息熵来描述的。那么互信息表示了什么呢，首先先看下维基百科的定义：独立的(H(X),H(Y)), 联合的(H(X,Y)), 以及一对带有互信息 I(X; Y) 的相互关联的子系统 X,Y 的条件熵。在概率论和信息论中，两个随机变量的互信息（Mutual Information，简称MI）或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布p(X,Y) 和分解的边缘分布的乘积 p(X)p(Y) 的相似程度。互信息是点间互信息（PMI）的期望值。互信息最常用的单位是bit。简单的来说，这个公式代表着两个子系统X和Y的相似度，但是这里的相似度是从信息熵的角度出发的，它越大代表着两者的差异越大，其计算公式以及相关的公式如下公式5.6，公式5.7所示。$$MI(X;Y) = \sum_{y \in Y}\sum_{x \in X}p(x,y)\log{\frac{p(x,y)}{p(x)p(y)}}\ \ \ 公式5.6$$$$\begin{split}MI(X;Y) &= H(X) - H(X|Y)\\&=H(Y) - H(Y|X)\\&=H(X) + H(Y) - H(X,Y)\\&=H(X,Y) - H(X|Y) - H(Y|X) \ \ \ 公式5.7\end{split}$$轮廓系数之前我们说到的衡量指标都是有标签的，这里的轮廓系数则是不包含标签的评价指标。	簇内不相似度：		计算样本i到同簇其它样本的平均距离为$a_i$ 。$a_i$越小，表示样本$i$越应该被聚类到该簇，而簇C中的所有样本的$a_i$的均值被称为簇C的簇不相似度。	簇间不相似度：		计算样本i到其它簇$C_j$ 的所有样本的平均距离bij， $b_i=min{bi1,bi2,...,bik}$ ，$b_i$ 越大，表示该样本$i$越不属于其它簇。	轮廓系数：		$s_i$值越接近1表示样本i聚类越合理，越接近-1，表示样本i应该分类到另外的簇中，近似为0，表示样本i应该在边界上。所有样本的si的均值被成为聚类结果的轮廓系数。		$$		s_i = \frac{b_i - a_i}{max\{a_i,b_i\}}		$$]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[BaiduPCS-Go的使用]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/17/BaiduPCS-Go%E7%9A%84%E4%BD%BF%E7%94%A8/</url>
      <content type="text"><![CDATA[Why BaiduPCS-GoBaiduPCS-Go是一个用Go语言编的命令行版的百度网盘，我们可以类比mas和Appstore的关系。那么为什么要用这样一个安装比较麻烦，还要记命令行的百度网盘的替代品，直接用百度网盘客户端不好么？这还真的是不好，百度网盘在mac下是一个十足的阉割版，最常用的功能中，Mac版缺失了以下几种功能：  没有分享功能：mac下的客户端的分享功能居然是需要通过浏览器打开，太不优雅了。  没有离线下载任务：直接导致不能下载磁力链接。如果你和我一样平时一样习惯终端操作，这个工具的学习成本超级低，同时它还有一定的提升下载速度的功效。使用指南安装Mac一般是预装了go的，如果没有的话，使用brew install go来安装。除了go我们还需要安装git，同样使用brew install git。在拥有了git和go以后，执行下面的指令即可。go get -u -v github.com/iikira/BaiduPCS-Go注：在安装途中，有提示说其安装到了一个~/go/bin的目录，也就是说这个工具的执行文件在~/go/bin这个目录。为了之后我们能够全局使用这个指令，于是我们将export PATH="/Users/deamov/go/bin:$PATH"添加到配置环境变量的文件中，如果没有使用zsh的话在~/.bashrc中，如果用的是zsh的话在~/.zshrc中。注：deamov是我的电脑的用户名，至此安装便结束了。常用操作说明登陆BaiduPCS-Go简单一行指令就可以登录了，如果之前已经登陆过账号的话，现在就已经可以开始进行下载等操作了，如下效果图。 第一次使用需要有登陆的操作，输入login即可登陆，尊许提示依次输入账户和密码即可，如果需要验证码，则会输出一个链接，打开就可以看到验证码了。基本操作基本的移动目录的方式和linux的操作一样，ls是现实当前目录的文件，rm是删除命令，cd是切换目录，创建目录是mkdir，拷贝是cp，值得一提的是它支持Tab补全。和平时使用的终端命令不同的有如下几个指令。  搜索：平时我们使用的grep在这里是不能使用的，我们用search关键词来搜索。      search 关键字 # 搜索当前工作目录的文件  search -path=/ 关键字 # 搜索根目录的文件  search -r 关键字	# 递归搜索当前工作目录的文件         下载：记住是download就好啦      BaiduPCS-Go download &lt;网盘文件或目录的路径1&gt;   BaiduPCS-Go d &lt;网盘文件或目录的路径1&gt; &lt;文件或目录2&gt; &lt;文件或目录3&gt; ...  # 当然支持多文件下载咯，下载目录默认在~/Download文件夹中        离线下载: 支持http/https/ftp/电驴/磁力链协议      # 将百度和腾讯主页, 离线下载到根目录 /  offlinedl add -path=/ http://baidu.com http://qq.com  # 添加磁力链接任务  offlinedl add magnet:?xt=urn:btih:xxx  # 查询任务ID为 12345 的离线下载任务状态  offlinedl query 12345  # 取消任务ID为 12345 的离线下载任务  offlinedl cancel 12345         分享share    查看分享内容      share list  share l        取消分享      share cancel &lt;shareid_1&gt;  share c &lt;shareid_1&gt;  # 遗憾的是只能支持通过shareid来取消分享        上传：同名文件会被覆盖    注：需要退出BaiduPCS-Go使用，否则本地文件目录不能自动补全    $BaiduPCS-Go upload &lt;本地文件/目录的路径1&gt; &lt;文件/目录2&gt; &lt;文件/目录3&gt; ... &lt;目标目录&gt;$BaiduPCS-Go u &lt;本地文件/目录的路径1&gt; &lt;文件/目录2&gt; &lt;文件/目录3&gt; ... &lt;目标目录&gt;# Example$BaiduPCS-Go upload ~/Downloads/1.mp4 /Video        其他    这个工具很强大，还可以通过设置下载线程数等等操作来提升下载速度，更多详细的操作请参考它的官网。  ]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> 百度网盘 </tag>
        
          <tag> 下载 </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Tobias的小粉丝在此]]></title>
      <url>/%E6%9D%82%E9%A1%B9/2018/08/16/Tobias%E5%B0%8F%E8%BF%B7%E5%BC%9F%E5%9C%A8%E6%AD%A4/</url>
      <content type="text"><![CDATA[									Tobias的小粉丝在此	序言最近迷上了吉他，当然不是指那种一周速成的把妹弹唱啦。为了防止大家对吉他有一种特别简单，把妹专用道具的奇怪印象。特别提一个小知识，古典吉他在世界公认的十大难学的乐器中排第三，顺便一提，钢琴排第五。正文不过，非常遗憾最近喜欢上的是并不是尼龙弦的古典吉他，而是它的另一个兄弟指弹吉他中的Percussive Fingerstyle。Percussive FingerStyle起源于80到90年代，它通过快速击打琴弦，琴身，以及琴的边缘制造装饰音。现在比较有名的几个这个风格的吉他手有Andy McKee和Tommy Emmanuel，而Tobias Rauscher也是这个领域的新秀，也是目前我最喜欢的吉他手之一。值得一提的是，他是14岁开始自学的吉他，开始的时候主要弹奏摇滚和重金属，知道2010年终于开始了Solo Acoustic Guitar的道路。和相似的同时用多个吉他，甚至奇怪形状吉他的Luca Stricagnoli不同，他只使用一把吉他，视觉效果上没有了那份笨重，同时Tobias的笑容也能让人感受到他对于吉他的热爱。有兴趣的小伙伴可以在网易云搜他的曲子。惭愧的说，第一次听他的曲子也是在网易云里面听到了，结果整整单曲循环了好几天（笑。其他Tobias的官网在此]]></content>
      <categories>
        
          <category> 杂项 </category>
        
      </categories>
      <tags>
        
          <tag> FingerStyle </tag>
        
          <tag> Guitar </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[支持向量机SVM]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/13/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BASVM/</url>
      <content type="text"><![CDATA[							支持向量机（Support Vector Machine）	支持向量机（Support Vector Machine）1、前言在之前我们介绍了线性回归算法以及其变种，LASSO回归、Ridge回归。他们是从减少过拟合的角度出发而得到的算法，而 SVM（支持向量机）则是优化原本线性回归算法中选择“分割线”，或者说选择分割超平面这样一个过程。TAG：# 拉格朗日数乘子算法 # KKT条件2、SVM的优点和缺点	优点：泛化错误率低，计算开销不大，结果易解释	缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。	数据类型：数值型和标称型数据。3、SVM算法原理首先我们都知道，为了划分二维的数据需要一根线，划分三维数据需要一个面。这里线是一维，面是二维，同理继续推出对 N 维的数据需要使用 N-1 维的对象进行分割，线性回归和 SVM 本质都是通过找这个超平面来达到分类的效果。具体的来说SVM是在优化线性回归中的 $kx+b$ 模型。在线性回归中只需要考虑有一个分割超平面能进行分类即可，而SVM则想找出所有能分类的分割超平面中最优的超平面，即所有点都到分割超平面的距离最大，而支持向量指的就是离超平面最近的那些点。超平面的公式为公式 2.1。所以点 A 到分割超平面的距离为公式 2.2。这里我们为了方便计算引入类别标签为-1和+1。所以保证所有的最小间隔的点最大化的公式为公式 2.3。注1：-1和+1是为了保证预测正确的时候，$y(x_i)*label_i$都是一个很大的正值。注2：$arg\ max_{w,b}$的含义是，得到w和b使得后面式子取最大值$$y(x) = w^TX+b\ \ \ 公式2.1$$$$\frac{|w^TX+b|}{||w||}\ \ \ 公式2.2$$$$arg\ max_{w,b}\{min_i(label_i*(w^Tx_i+b)*\frac{1}{||w||})\}\ \ \ 公式2.3$$显然我们不能直接求解上面的式子。需要化简下它。首先由于公式2.1在正确预测时，同 $label$ 的乘积大于 1。所以我们可以拆分公式2.3为公式2.4和约束条件公式2.5。注：这里的约束条件公式2.5中，要对每一个式子前都要加系数，即拉格朗日数乘子$\alpha_i$。$$arg\ min_{w,b}\ ||w||\ \ \ 公式2.4$$$$st.\ \ label_i*(w^Tx_i+b) \geq 1 \ \ \ 公式2.5$$对为了方便求导计算在公式 2.4 前加上$\frac{1}{2}$这个系数。之后使用拉格朗日乘子法得到公式2.6，并进行计算。根据KKT条件，让偏导数等于0得到公式2.7和公式2.8。注：这里需要注意的是拉格朗日数乘子的正负号，这个同不等式的符号有关$$L(w,b,\alpha)= \frac{1}{2}||w||^2-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1]\ \ \ 公式2.6$$$$\sum_{i=1}^{n}\alpha_i label_i x_i = w\ \ \ 公式2.7$$$$\sum_{i=1}^{n}\alpha_i label_i = 0\ \ \ 公式2.8$$将公式2.7，公式2.8代入公式2.6化简，再根据对偶问题得到最终公式2.9，根据KKT，其约束条件为公式2.10。注1：KKT条件在SMO算法中统一进行讲解。注2：b是由公式2.8消掉的。注3：在拉格朗日乘子法应用在这里，我们可以把$||w||$，写作$max_\alpha\ L(w,b,\alpha)$，所以原式可以写作$min_{w,b}\ max_\alpha\ L(w,b,\alpha)$，根据对偶问题，就可以变成$max_\alpha\ min_{w,b}\ L(w,b,\alpha)$，这也是能把公式2.7和公式2.8代入公式2.6的原因，也是公式2.9种是$max_\alpha$的原因。具体证明在KKT中的附上的博客中。$$max_\alpha\ \sum_{i=1}^{n}\alpha_i - \frac{1}{2}\sum_{i,j=1}^{m}label_i*label_j*a_i*a_j\langle x_i·x_j\rangle\ \ \ 公式2.9$$$$\alpha_i \geq 0\ \  且\ \sum_{i=1}^{m}\alpha_i*label_i = 0\ \ \ 公式2.10$$注：这里$\langle x_i·x_j\rangle$是两者向量积的运算，是从$x_i^T*x_j$得到的。这么看来我们算出了$\alpha$就能算出超平面，所以SVM的工作就是算出这些$\alpha$，而SMO算法就是求$\alpha$的典型算法。4、对SVM引入线性不可分由于数据都不那么干净，所以我们不应该假设数据能够100%的线性可分。我们通过对判定的公式，公式2.5，引入松弛变量$\xi_i\geq 0$，得到其引入了松弛因子的形式，如下公式3.1。$$y_i(w*x_i+b)\geq1-\xi_i\ \ \ 公式3.1$$同时对于目标函数公式2.4也需要调整，我们将$\xi$引入目标函数并对其设置权值，得到公式3.2，也因此其约束条件变为公式3.1，公式3.2。$$min_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i\ \ \ 公式3.2$$$$\begin{split}st.\ \ \ &y_i(w*x_i+b)\geq 1 - \xi_i\\&\xi \geq 0\end{split}$$故拉格朗日函数$L(w,b,\xi,\alpha,\mu)$为如下公式3.3，其中$\alpha$，$\mu$为拉格朗日数乘子。$$L(w,b,\xi,\alpha,\mu)=\frac{1}{2}||w||^2+C\sum_{i=1}^N\xi_i-\sum_{i=1}^n\alpha_i*[label_i*(w^Tx_i+b)-1+\xi_i]-\sum_{i=1}^n\mu_i\xi_i\ \ \ 公式3.3$$和之前的操作一样，对其进行求偏导操作后，类似的得到了相同的公式2.7，公式2.8，不同的是这里对$\xi$的求到后对$\alpha$有了限制，得到了公式3.4，由于$\mu\geq0$所以有$\alpha_i$的取值范围$0 \leq \alpha_i \leq C$。注：注意这里的$\alpha$取值，之后SMO会用$$C-\alpha_i-\mu_i = 0\ \ \ 公式3.4  $$最终目标函数还是同之前推导的相同，即公式2.9。变化的只有，约束条件中$\alpha$的取值变为了$0 \leq \alpha_i \leq C$。这样有了目标函数了以后，之后可以根据梯度下降算法求得最终的$\alpha$5、SMO（Sequential Minimal Optimization）5.1、KKT条件 由于使用了拉格朗日数乘法，其中KKT条件便是SMO算法的精髓，所以我觉得有必要在这里提到KKT条件。首先我们求$f(x)$ 极值的时候，需要讨论三种情况。	没有约束条件：	有一个等式$h(x)$的约束条件：		使用拉格朗日乘子法（Lagrange Multiplier），也就是我们在高数中求极值常用的。设置一个拉格朗日系数$\alpha_1$，得到如下公式，之后对$x$和$\alpha_1$用求导的方式求极值即可。		$$		L(x, \alpha) = f(x) + \alpha*h(x)		$$	含有不等式的约束条件：		当约束条件中有不等式时，就需要用到KKT条件。同样地，把所有的不等式约束$g(x)\leq0$、等式约束$h(x)=0$和目标函数$f(x)$全部写为一个式子如下公式。		$$		L(x,\alpha_1, \alpha_2)= f(x) + \alpha_1*g(x)+\alpha_2*h(x)		$$		KKT条件是说最优值必须满足以下条件：					$L(x, \alpha) = f(x) + \alpha(x)$ 对$x$，$\alpha_1$，$\alpha_2$求导为零。			$h(x)=0$ 。			$g(x)*\alpha_1=0$。				其中第三个式子非常有趣，因为$g(x)\leq$ 0 ，如果要满足这个等式，必须有$a = 0$或者$g(x) = 0$。这是SVM的很多重要性质的来源。同时$f(x)$也可以写作$max_{\alpha_1,\alpha_2}\ L(x,\alpha_1,\alpha_2)$，这个则是SMO求解中的一个关键性质。详细的论述参考这篇博客。5.2、SMO算法细节SMO算法综述由于原来直接通过梯度下降进行求解速度太慢，所以1996年，John Platt依靠KKT的特性，将大优化问题变成了多个小优化问题来求解，成为了SVM中最常用的求解思路。其思路如下：	Loop：					选取一对 $\alpha_i$，$\alpha_j$作为变量，其余看为常数			如果这对$\alpha$满足以下两个条件，使用梯度下降算法改变他们的值。									两者都在间隔边界外					两者都没有在进行过区间化处理，或者不在边界上							当满足了KKT条件，即$\sum_{i=1}^N\alpha_iy_i=0$和$0\leq \alpha_i \leq C$，退出循环。		注：这里可以这么理解，$\alpha_i$从之前的公式中我们可以大致理解为是每一个样本的权值，我们这些操作可以理解为通过操作$\alpha$把所有的样本点尽量的放在间隔边界上。算法推导我们接下来要做的是，通过类似梯度下降的方式来求的最优的$\alpha$值。正如上一节所说的，SMO的本质是大优化问题画小优化问题。所以从目标函数公式2.9中，随意取出两个$\alpha$，为了表达方便，不妨直接取$\alpha_1$和$\alpha_2$，同时对公式2.9前加负号取反之后，化简如下式4.1，其中$\kappa_{ij}$代表$\langle x_i·x_j\rangle$。$$\begin{split}min_{\alpha_1, \alpha_2}W(\alpha_1,\alpha_2) &=\frac{1}{2}\kappa_{11}\alpha_1^2+\frac{1}{2}\kappa_{22}\alpha_2^2+y_1y_2\alpha_1\alpha_2\kappa_{12}-(\alpha_1+\alpha_2)\\&+y_1\alpha_1\sum_{i=3}^Ny_i\alpha_i\kappa_{i1}+y_2\alpha_2\sum_{i=3}^Ny_i\alpha_i\kappa_{i2}\ \ \ 公式4.1\\\end{split} $$$$\begin{split}st. \ \ &\alpha_1y_1+\alpha_2y_2=-\sum_{i=3}^Ny_i\alpha_i\\&0\leq\alpha_i\leq C\end{split}$$由于我们已经把不是$\alpha_1$和$\alpha_2$的参数看作常量，所以在进行求偏导进行梯度下降算法的时候就不需要考虑公式4.1中第二行的式子。通过这个式子中约束条件的等式就可以得到仅含$\alpha_j$的式子，对其进行梯度下降算法，得到如下公式4.2：$$g(x)=\sum_{i=1}^Ny_i\alpha_i\kappa(x_i,x)+b  $$$$\eta = \kappa_{11}+\kappa_{22}-2\kappa_{12} = ||x_1-x_2||^2$$$$E_i = g(x_i)-y_i = (\sum_{j=1}^Ny_j\alpha_j\kappa_{ji}+b)-y_i$$$$\alpha_i = \frac{\xi-\alpha_j y_j}{y_i}$$$$\alpha_j^{new}=\alpha_j^{old}+\frac{y_j(E_i-E_j)}{\eta}\ \ \ 公式4.2$$这时候我们已经找到了两个的$\alpha$的新值了，不过我们不能确定这两个新值是否还满足KKT条件。所以我们根据KKT条件中$\alpha$的取值，设置了 L 和 H 来防止新值不满足 KKT，即$L\leq\alpha_i,\alpha_j \leq H$，其中L，H的公式如下公式4.3和公式4.4得到：$$if\ y_i \neq  y_j\ \ \ L=max(0,\alpha_j-\alpha_i),\ H=min(C,C+\alpha_j-\alpha_i)$$$$if\ y_i = y_j\ \ \ L=max(0,\alpha_j+\alpha_i-C),\ H=min(C,\alpha_j+\alpha_i)$$L H的细节推导在这个博客中详细的说明了LH是怎么推出来的。简单的来说就是之前提到的由线性不可分中得到的一个 $\alpha$ 的取值范围划定了一个正方形的框，之后公式 4.1 中第一个条件我们可以画一条直线，然后试图得到一个更精确的$alpha$范围，而这个 L 和 H 分别代表的是 High 和 Length（这点没有考证，是我个人的理解）。而为什么要讨论 $y_i$ 和 $y_j$ 是否同号，则是因为他们是否同号决定了画的直线的斜率。至此，SMO算法的整体就讲完了，它最厉害的地方在于把一个很复杂的大的运算量很大的运算变成了一个一个小的优化问题，个人觉得这种以提升效率为目的，化大为小，带点贪心的算法的算法是非常优雅。6、结语这一章节中涉及的算法看似很复杂，但是实际的推导当你理解了拉格朗日数乘法和KKT条件以后，剩余的就是一些基本的求导操作，和一些减少运算的小技巧，如在判断 $\alpha$ 的变化方向的时候就不考虑公式 4.1 中第二行的式子等。建议大家都手推推，毕竟这个算法是目前面试中出现概率比较高的算法。到这里为止 SVM 算法还是作为一种二分类算法，当咱们的专栏进行到接近尾声的时候，我会给大家介绍几种把二分类变成多分类的集成算法思路。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[使用Github创建自己的小博客]]></title>
      <url>/%E8%BD%AF%E4%BB%B6%E4%BD%BF%E7%94%A8/2018/08/12/%E4%BD%BF%E7%94%A8Github%E5%88%9B%E5%BB%BA%E8%87%AA%E5%B7%B1%E7%9A%84%E5%B0%8F%E5%8D%9A%E5%AE%A2/</url>
      <content type="text"><![CDATA[							使用Github创建自己的小博客	懒人攻略只有四步：	找到自己喜欢的别人的博客的Github地址，一般为username.github.io结尾。	Fork一份对方的源码，之后把仓库名改为YourGithubName.github.io	在_config.yaml中更改个人信息，同时把_posts中的文章都删了，注意别人的文章格式，之后仿照对方的格式写即可。	给你Fork的原作者写封邮件表达感谢！说不定就这么勾搭了一个大佬也不一定呢。完成了四步后，浏览器输入YourGithubName.github.io就能在晚上看到自己的博客啦。折腾攻略本这不重新造轮子的原则，附上我参考的大佬们的文章。	搭建篇：		简书上chaosinmotion 的 Github Pages + Jekyll 独立博客一小时快速搭建&amp;上线指南 	添加评论系统：		Github上knightcai的 为博客添加 Gitalk 评论插件 		特别一提，如果出现Validation Error是因为博客标题的名字编码后太长了，参考这个Issue中mr-wind的使用 id: decodeURI(location.pathname) 解决方案。		注：md5的方案可能更好，偷懒起见我没有用。	阅读量统计：		wanghao的 为NexT主题添加文章阅读量统计功能 ，这个文章用的是leandCloud。	搜索服务：		使用Algolia，不过自带的LocalSearch比较简单。文章有配置说明。	主题：		Next系列。官网有安装手册。	CopyRight:		在目录下搜索copyright，找到那个html文件进行修改就好了。效果是文章下面的红竖杠中的内容。	小彩蛋：		史蒂芬小恐龙，他的js文件在这里！之后就任君发挥啦，Happy Coding。	最后题外话		所有的配置基本上都可以在_config.yaml中设置，同时在博客中\代表的就是根目录，这样子你自己在配置其他的功能的时候就可以轻松愉悦的配置。值得一提的是css文件和js文件都在assets文件夹中，自己DIY的时候最好不要打乱目录结构。]]></content>
      <categories>
        
          <category> 软件使用 </category>
        
      </categories>
      <tags>
        
          <tag> GithubPage </tag>
        
          <tag> 博客 </tag>
        
          <tag> 教程 </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[决策树优化策略]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/10/%E5%86%B3%E7%AD%96%E6%A0%91%E4%BC%98%E5%8C%96%E7%AD%96%E7%95%A5/</url>
      <content type="text"><![CDATA[							决策树优化策略	1、剪枝优化是什么？决策树的剪枝是决策树算法中最基本、最有用的一种优化方案，分为以下两类：	前置剪枝：在构建决策树的过程中，提前停止。这种策略无法得到比较好的结果	后置剪枝：在决策树构建好后，然后开始剪裁，一般使用两种方案。a）用单一叶子结点代替整个子树，也节点的分类采用子树中最主要的分类。b）将一个子树完全替代另一个子树。后置剪枝的主要问题是存在计算效率问题，存在一定的浪费情况。后置剪枝后置剪枝的核心思想其实就是交叉验证，其通过对完全树进行剪枝，一直剪到只剩下树根，这样子便得到许多树，随后通过使用数据集分别对他们验证，然后根据结果选择最优树。2、决策树剪枝过程while 生成的决策树不为1个节点:	计算所有内部非叶子节点的剪枝系数;	选择最小剪枝系数的节点:		if 有多个最小剪枝系数节点:			选择包含数据项多的节点删除		else:			删除节点		将剪枝后的树存入之后用的决策树集for 决策树 in 决策树集:	用数据集验证决策树，得到最优剪枝后的决策树其中用于验证决策树的损失函数如下公式 1.1：$$loss = \sum_{t=1}^{leaf} \frac{D_t}{D}H(t)\ \ \  公式1.1$$那么我们剪枝需要把所有的可能都剪一边么，显然不能。这里就引入了剪枝系数来判别每次剪枝选择哪个节点：首先我们明确，剪枝系数的目的为，平衡准确度和树的节点数量之间的关系。所以很自然的想到我们常用的处理手法，在损失函数中引入叶子结点的变量，得到公式1.2。注：这种思路我们在LR算法中也用了，生成了Ridge和LASSO$$loss_{\alpha} = loss + \alpha*leaf\ \ \ \ 公式1.2$$假定剪枝前的损失函数为$loss(R)$，剪枝后的损失函数为$loss(r)$，由于我们是想让剪枝前后的准确率尽量不变，所以让剪枝前后的损失函数相等，化简得公式1.3，即剪枝系数。注：多次剪枝后为根节点，所以$r=1$。$$\alpha = \frac{loss(r)-loss(R)}{R_{leaf}-1}\ \ \ \ 	公式1.3$$那么这个系数怎么用呢，答案就是由于我们想尽量减去的叶子结点多点，又同时保持准确度，故剪枝系数越小越好。3、结语我们可以看到，到了这里算法开始就有了集成学习的特点了，算法开始从简单的单一的算法进行进化和融合，最终像搭积木一样慢慢完成了后期特别复杂的算法。BTW，这里使用的算法其实大部分代码实现（不是使用别人写好的函数），可以在 《机器学习实战》 中找到。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[决策树]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/09/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
      <content type="text"><![CDATA[							决策树&amp;ID3,C4.6,CART	1、 前言之前我们已经说了，机器学习的从线性回归，概率这个出发点发展的算法。这次我们讲从第三个出发点，使用信息熵的算法，决策树。2、 信息熵首先我们来介绍什么是信息熵，信息熵是1948年，香农引入信息熵。一个系统越是有序， 信息熵就越低，一个系统越是混乱，信息熵就越高，所以信息熵被认为是一个系统有序程度的度量。举个例子，太阳从东边升起这个规律人人都知道，所以信息熵低，而GAN算法是怎么推导的知道的人就不多，所以它的信息熵高。注：一个事件发生的概率大，那么它含有的信息少。一言已概之就是：信息熵就是用来描述系统信息量的不确定度。其公式如下：$$H(x) = - \sum_{i=1}^m p_i log_2(p_i)$$条件熵的定义为： 给定条件X的情况下，所有不同x值情况下Y的信息熵的平均值叫做条件熵。$$H(Y|X) = \sum_{j=1}P(X = v_j)H(Y|X = v_j)$$$$H(Y|X) = H(X,Y)-H(X)$$3、 纵览决策树算法在了解了这个算法的关键评判标准后，我们来看下决策树是什么，决策树 ( Decision Tree ) 是在已知各种情况发生概率的基础上，通过构建决策树来进行分析的一种方式，是一种直观应用概率分析的一种图解法。决策树是一种树形结构， 其中每个内部节点表示一个属性的测试，每个分支表示一个测试输出，每个叶节点代表一种类别。换句话说，决策树的使用是一种模拟了简单的人类思考过程的思路。通过许多if...else...进行判断最后得到一个结论。3.1、 决策树构建过程构建步骤如下:	将所有的特征看成一个一个的节点;	遍历每个特征的每一种分割方式，找到最好的分割点;将数据划分为不同的子节点 $N_1, N_2,...,N_m$。计算划分之后所有子节点的纯度信息。	对第二步产生的分割，选择出最优的特征以及最优的划分方式。得出最终的子节点: $N_1,N_2,...,N_m $ 	对子节点分别继续执行2-3步，直到每个最终的子节点都足够纯。注：这里的纯度指的是每个字节点的类别尽量相同。注意构建过程中的选择最优特征的步骤决定了后期算法的不同3.2、划分方式的选择在上述 3.1 中提到了两个关键性的事情，叫做划分方式和选择最优特征，这里我们就要讨论的是划分方式。众所周知，数据分为离散的和连续的，显然我们对他们的处理有些不同。如果属性是离散值，且要求是二叉树，我们就可以按照一般的逻辑方式，按照属于此子集和不属于此子集分成两个分支。如果没有要求是二叉树则一个属性就是一个分支。如果是属性为连续值，则可以确定一个值作为分裂点split_point，按照&gt;split_point和&lt;=split_point生成两个分支。3.3、 决策树分割属性选择之前我们说了划分方式，那么我们如何选择最优的特征呢。答案是比较纯度。首先决策树算法是一种“贪心”算法策略，只考虑在当前数据特征情况下的最好分割方式，且不能进行回溯操作。而对于整体的数据集而言，通过查看每个特征属性划分后，纯度的变化进行比较，选择能让数据集变得更纯的特征属性进行分割。之后重复上述步骤直到满足条件。注：题外话，虽然我们很少需要自己造轮子，但是还是需要知道树的结构是符合递归规律的，一般而言树的构建都可以使用递归算法那么纯度是什么，纯度其实就是一种判断决策树是否向着正确方向前进的判断。粗鄙的类比，就是母猪配种，选择最合适的方式将不同种的猪分开，尽量保证每波猪都是纯种的。纯度的判断标准不同也决定了之后的算法的名称不同，其标准如下三种：	基尼系数：$Gini = 1 - \sum_{i=1}^nP(i)^2$					信息熵：$-\sum_{i=1}^n P(i)log_2(P(i))$			误差率：$Error = 1 - max_{i=1}^n \{P(i)\}$		注 1：这里我说的是算法名称不同，其实更多的想指代他们是一个系列的算法，和LR与Lasso和Ridge的关系一样注 2：如上公式都体现了纯度值越小信息量越大这个理念上面是基础的一些系数，在剪枝和判断的时候需要一个种体现变化的系数，于是出现了如下公式来作为C4.5和ID3的判别标准。简单的理解为，这个公式表达了以A属性划分后，信息量增加的量，或者说指代的是纯度变纯了多少。$$Gain = \Delta = H(D)-H(D|A)$$3.4、 停止条件决策树构建的过程是不断递归的过程，就像 (是) while 循环一样，必须有跳出循环的条件。以下是两个停止条件。	每个字节点只有一种类型时停止条件。（容易过拟合）	节点中记录数小于某个阀值的时候，或者迭代次数达到给定值的时候停止构建。3.3、 决策树的评估$$loss = \sum_{t=1}^{leaf} \frac{|D_t|}{D}H(t)$$其中$H(t)$前的参数$\frac{D_t}{D}$主要的目的其实是给信息熵加权值，代表着节点中的样本点越多它越重要。4、 算法对比其实到这里决策树的核心就介绍完了，现在来看实际中的算法应用，也是面试种可能（不太可能）问到的算法，即ID3，C4.5，CART。其中CART最重要，会在之后的GBDT算法中，代表着那个DecisionTree。4.1 、ID3算法ID3算法是决策树的一个经典的构造算法，每次迭代选择分割属性的方式为，使用信息增益作为评判标准。优点为：决策树构建速度快，实现简单。缺点为：首先，计算依赖于特征数目较多的特征，而属性值最多的属性并不一定最优。其次，ID3算法不是递增算法，是单变量决策树，对于特征属性之间的关系不会考虑，抗噪性差。最后由于它的数据要扔到内存中，所以只适合小规模数据集。4.2、C4.5算法在ID3算法的基础上，进行算法优化提出的一种算法 ( C4.5 ) 。其使用信息增益率来取代ID3算法中的信息增益，同时在树的构造过程中会进行剪枝操作进行优化。能够自动完成对连续属性的离散化处理。C4.5算法在选中分割属性的时候选择信息增益率最大的属性。信息增益率公式如下：$$Gain\_ratio(A) = \frac{Gain(A)}{H(A)}$$优点：产生的规则易于理解，同时准确率较高且实现简单缺点：由于采用了剪枝优化，所以对数据集需要进行多次顺序扫描和排序，所以效率较低 ，和ID3一样同样需要将数据放在内存，故只适合小规模数据集。注：这里的能够处理连续值和剪枝优化都算是后期加的，只是Sklearn中没有给ID3赋予这个功能。他们的核心差别就在是增益率还是增益。剪枝优化会在决策树优化中讲CART好了，轮到了这个决策树中最重要的算法了，它使用基尼系数作为数据纯度的量化指标来构建的决策树。CART拆开是，Classification And Regression Tree，中文是分类回归树。这么叫的原因是它可以用来做分类和回归两类问题。值得注意的是：CART构建是二叉树其GINI增益公式如下：$$Gain = \Delta = Gini(D) - Gini(D|A)$$注：GINI系数的计算不牵扯信息熵运算中的对数运算，故速度比较快总结	ID3和C4.5基本上是一回事，所以他们都是单变量决策树，都只使用在小规模数据集	C4.5算是ID3的优化，所以当属性取值较多的时候，可以考虑C4.5而不是ID3	决策树的树是存在内存中的，所以一般只适用于小数据量。		注：一般你要是看到了类似B+树的结构，一般不是完全在内存中	CART是最常用的，尤其是后期集成算法GBDT中用的就是CART。CART是二叉树，ID3和C4.5不一定是。最后这掌的重点在于知道决策树的生成规律，而三种算法的区别仅仅只是对于树形成节点的规则不同而已，ID3使用信息增益、 C4.5使用信息增益率、CART使用基尼系数。这一章依旧是一个轻松愉悦的章节，内容非常easy。之后我们会开始进入基础算法之后的升级版就困难不少。BTW，没想到有这么多人关注专栏，再次感谢大家。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[《滚雪球》- 爱丽丝·查理芒格]]></title>
      <url>/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/2018/08/08/%E6%BB%9A%E9%9B%AA%E7%90%83-%E7%88%B1%E4%B8%BD%E4%B8%9D-%E6%9F%A5%E7%90%86%E8%8A%92%E6%A0%BC/</url>
      <content type="text"><![CDATA[							《滚雪球》 - 爱丽丝·查理芒格	读书笔记在巴菲特看来，真正富有的人生应该是这样的：做一份自己喜欢的工作，找到兴趣相投的朋友。只要能做到这些，你的人生也一样是成功的。正如巴菲特的总结，做一个自己能感到自己价值的工作，找到能互相认可的人也许才是人生的最大意义。在努力的做自己的喜欢的工作的时候才能更容因进入”专注“的境界。而兴趣相投的朋友则会让你的生活变得丰富，人说到底还是群居的动物，社交是必要的也是维持正常心理所必须要的要素。同时他们之间也是互通的，一个好的社交保持了好的积极的心态，保持了良好的且多元的信息社区，促进了工作的进步，工作的进步获得了回报，在财力和心理上形成正向激励，达成”飞轮效应“，进步越来越开实现复利增长。巴菲特有三宝，“内部积分卡”，“专注”，“复利”内部积分卡指的是，自己的行为有自己的评价标准，自己来定义自己是什么样的人，而不是由外部来定义自己，这样做到之后就会达到一种非常好的内心自洽。正如熊太行老师讲的那样，其实有时候内心自洽是一个强大者的开端，能让我们不去盲目的做一些违背自我的决定。“异于常人”的专注，其实很多时候是别人不能复制你成功的最大的壁垒。有着异于常人的专注的巴菲特能够通过专注找到“地上的烟蒂”，而缺了专注的行业其他人即便知道了方法也不能发现“烟蒂”。复利，在滚雪球中，复利的魅力用于体现在投资上，每次都能在之前的基础上以一个固定的比例进行增长，之后会越涨越快一发不可收拾。这点在吴军的《Google方法论》和《硅谷来信》，以及吴伯凡老师也多次提及，不过名词略有不同，吴军老师称之为指数增长，吴伯凡老师称之为飞轮效应。都在强调可叠加的魅力。如果放在生活中，我们应该常常反思，我们现在做的工作是不是可以叠加的，是不是在以大利益行动，是不是能成为未来的自己工作学习的基础。人生就像滚雪球，最重要的就是发现湿雪和长长的山坡这句话特别经典，也被很多人视为人生格言。我的感悟是，湿雪不难找到，但是有耐心和勇气去用人生这一个有限的时间，去选择一个长而不急的山坡不是每个人都能做到的。很多人看到别人在急坡中一时的成功，便忍不住放弃自己的计划，跟随别人，最终摔得人仰马翻。随后重复上述过程，直到到达山底最终一无所获。最佳的策略也许是认准一个“湿雪”较多的路径，随着不陡不缓的山坡，不急不躁，把自己的雪球越滚越大。个人总结今年开始，随着读的书越来越多，发现很多时候抛开大佬们成功的表象背后，总有一些惊人的相似的地方。首先是兴趣和专注，吴军说“努力只能让你成为行业的前20%，而兴趣决定了你是否能走完最后的10%”，自己不知道从什么时候起养成了一种选择困难症，虽然确定了计算机机器学习这个大方向，但是却没有一个具体的目标，导致近一个月来学习重心的摇摆不定，在机器学习，爬虫和区块链中摇摆不定。虽然他们学习的进度都还不错，爬虫接了北京高教所得项目，区块链得到了Hackathon竞赛的奖，机器学习算是初步入门正在踏入深度学习的领域，但是真真切切的感受到了自己能力的天花板，即“我没有能力去同时专精所有的方向”，方向仍需做减法。其次是“复利”，可叠加性的增长最有意思的是，往往这种复利型的进步开始的时候很困难，很累很苦，需要一个很强的内心去坚持，但随着飞轮开始转动一切就会变得理所当然。这点有一点小小的体会吧，这点体现在VIM的配置上面，开始的时候很艰难，各种快捷键都不知道是什么，觉得无比难用。不过，在了解到真的很多大佬是真的使用VIM或者类似的我认为超级难用的编辑器作为生产工具的时候，觉得也许应该坚持下来，不过没想到真的VIM在日后成为了我主要的生产工具。开始的时候只是配置了代码高亮，后来渐渐加入了代码补全，再后来补充了各种各样插件并学习了更多的快捷键，最后甚至自己能编一些脚本来实现自己想要的功能。那么它给我带来了什么“复利增长呢”，减少了不必要的分心，所有的学习需要的工具大多都能通过统一的途径获得，操作上不需要太多的改动，从论文的书写到代码的规范，同时由于打开很快的特性，能让我在有想法的时候就快速的写代码来测试，这些收获让我觉得之前的小努力都是值得的。同样的，我也相信算法，统计学和离散数学，包括博弈论，这些很多难啃又不会有立竿见影成效的东西最终会成为自己职业生涯复利增长的第一步。最后是内心自洽，个人自身的感受是，如果内心不能够自洽的话，很多精力都在花费在平复情绪上进行了没必要的经历开支，甚至“因为别人都这么做所以我也要这么做的”作出欠思考的行动，或者因为“情谊”做出了完全违背自己大利益的行动。行事内心要一把尺子，这把尺子要拿稳了。气场和魄力的起点便是内心自洽，如果自己都不能坚信自己的决定，又谈何让别人相信你的决定呢？]]></content>
      <categories>
        
          <category> 读书笔记 </category>
        
      </categories>
      <tags>
        
          <tag> Reading </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[Logistic回归&SoftMax回归]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/07/Logistic%E5%9B%9E%E5%BD%92_SoftMax%E5%9B%9E%E5%BD%92/</url>
      <content type="text"><![CDATA[							Logistic回归&SoftMax回归	序言 之前的文章中我们介绍了普通最小二乘线性回归算法，并进行了较为详细的推导，并通过分析其过拟合的问题，推导出了另外三个算法，Ridge回归算法，LASSO回归算法，以及弹性网络。并简要的分析了他们的优缺点。今天我们来接着介绍算法。为什么说是算法而不是回归算法呢，是因为在研究了逻辑回归和Softmax回归算法以后，惊讶的发现这两个算法是分类算法，所以这个回归算法下的说法就不是很严谨了。Logistic回归在回归算法上中我们介绍了，线性回归算法。我个人是这么拆分的，线性-回归-算法，回归指的整体的算法是回归算法而不是分类算法，线性指的是在算法回归中，假定输入输出之间关系函数是$y = kx + b$这个线性方程。那么再看看Logistic回归，显然这里肯定就是换了另一个种类的方程咯。对的，不过在逻辑回归中这里有一点不太一样，它的输出只有0和1，它假定输出 $y = 1$的概率$P$，自然 $y = 0$的概率就是 $1-P$，而逻辑回归中假定的方程就是这个$P$。现在我们来看看这个方程是什么，如下公式1.1所示。这个公式没有接触过的朋友可能不太好想这是一个什么函数，但是结合着我们高数的极限知识，我们肯定知道这个函数在$+\infty $的时候去趋近于1，在$-\infty$的时候趋近于0，而且是快速收敛的。建议百度sigmoid函数看看这个函数的图（Logistic函数就是sigmoid函数）。	$p=h_\theta(x) = g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}} $		公式1.1之前在线性回归算法中我们知道了求最佳$\theta$的流程就是先写出他的似然函数（这个公式表示了预测正确的概率），然后求最大似然估计（想办法让这个预测正确的概率最大），最终通过求最大似然估计的结果，得到一种调整$\theta$的最佳方案。这个逻辑回归的似然函数为公式1.3(由公式1.2易得)。	$L(y|x;\theta)=(h_\theta(x))^y(1-h_\theta(x))^{1-y}$	公式1.2	$L(\vec y|x;\theta)=\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}$					公式1.3之后对这个似然函数进行求导，我们都学过导数，在学导数的时候都知道，导数代表着值变化的趋势和速度。这里我们通过公式1.3对求对数，然后再对 $\theta_j $求导得到导数（公式1.4），这个导数就代表了针对$\theta_j $的正确预测的概率的变化趋势。如果让 $\theta$加上这个导数就可以保证让这个公式变大，直至最大为止。所以逻辑回归的调参如公式1.5，公式1.6所示，那么为什么是两个，在调参章节中会说明。	$\frac{\partial \ell(\theta)} {\partial \theta_j} = \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$ 公式1.4	$\theta_j = \theta_j + \alpha \sum_{i=1}^m(y^{(i)} - h_\theta(x^{(i)}))x_j$	公式1.5		$\theta_j = \theta_j + \alpha (y^{(i)} - h_\theta(x^{(i)}))x_j $公式1.6好的，到这里逻辑回归的调参我们就知道了，特别的说明的是，这里的 $alpha$就是传说中的学习速率，而这个参数我们要注意的是，它不能太大（会错过最大值而不能收敛），也不能太小（会收敛到局部最小值）。那么我们有了调参，这里肯定也有一个损失函数来表示现在预测的怎么样吧，这里的思路很简单，你不是似然函数是越大越好么，那么我把你来个倒数不就好了么，最简单的做法就是给求对数之后的公式1.3，添加一个负号。所以它的损失函数如公式1.7。$loss(y^{(i)},\hat y^{(i)}) = -\ell(\theta) = \sum_{i=1}^m ln(1+e^{(1-2y^{(i)})\theta^T x^{(i)}}) $	公式1.7最后，想说从那个 只能为1或者0就可以看出逻辑回归虽然名字带了回归两个字，但是其实是一个分类算法。而接下来的SoftMax回归算法则是逻辑回归算法的一种拓展，这个在下一节里说。SoftMax回归在逻辑回归中，最终的分类结果，只有两类。这显然不是适用于很多其他的情况。所以SoftMax对逻辑回归进行了一般化，适用于K分类的问题。针对K分类的问题，我们的小伙伴 $\theta$参数就不在是一个向量了，我们设第K类的参数为向量$\theta_k$， 则有n个属性的参数就成了一个二维矩阵$\theta_{k*n}$。在Softmax回归中，我们设预测对第k类的概率为公式2.1。$p(y=k|x;\theta)=\frac{e^{\theta_k^T x}}{\sum_{i=1}^K e^{\theta_i^T x}} $, $k=1,2 ...,K $公式2.1剩下的分析同逻辑回归一样就不赘述了。机器学习调参我们在之前的学习中，了解到机器学习在迭代的过程就是不断的调整$/theta$参数的过程。有些是算法自己就调整了，有些是需要我们的人工的来调整的，这里就要引入超参这个概念了。什么事超参呢，超参就是不能通过算法自动调整的参数，比如Ridige回归和LASSO回归的 $\lambda$，弹性网络中的 $\alpha$。除了这些，在Logistic回归这一章节中，我们发现了调参函数有两个，但是并没有说明为什么。在这里将进行详细介绍。在Logistic和Softmax回归中我们用到的调参方式，只要你稍微了解过深度学习就一定听过，这种调参方式就是大名鼎鼎的梯度下降算法。而这两个公式，公式1.5是批量梯度下降算法（BGD），公式1.6是随机梯度下降算法（SGD）。通过公式1.6我们可以看出，每次迭代SGD调整一个$\theta_j$只需要和其中一条属性的比，而BGD每次迭代每调整一个$\theta_j$需要和所有属性的比较，所以SGD迭代速度快。那么SGD每次迭代考虑的属性少会不会没有BGD准呢，这是不一定的。梯度下降算法就像大雾天气下山，我们只能看清眼前的一小部分，并以此为依据下山，我们很有可能最后在山的上的一个小坑里出不来，陷入局部最优解的问题。SGD在全局餐在多个相对最优解的情况下，SGD很有可能跳出某些局部最优解，所以不一定会比BGD坏。而BGD一定能够得到一个局部最优解（在线性回归中一定是得到一个全局最优解）。不过由于SGD少考虑一些情况，所以有随机性，因而最终结果可能会比BGD差，一般情况下我们会优先使用SGD。注：个人的简单理解是，SGD每次只会往某一个坐标轴方向走一步，而BGD则是结合所有坐标轴的情况，往一个空间内的一个方向走一步。所以理论上BGD很“理性”。SGD和BGD出现的优缺点的情况是不是很熟悉，是不是很像Ridge回归和LASSSO回归的抉择，最终出现了弹性网络。那么是不是也有一个类似梯度下降算法的“弹性网络”呢，答案显然是有的，那就是MBGD，我不全不考虑完不就好了嘛。MBGD中每次拿b个样本的平均梯度作为更新方向，这里的b一般为10。这样子就既保证了速度，也一定程度上保证了准确度。模型效果判断最后，我们模型也训练好了，那么怎么来判定我的模型是不是合乎标准的，就像吴军老师在Google方法论说的一样，在工程中不是只有对错，只有相对的好和相对的不好，这个模型是否符合项目的需要也是有几个标准的，在训练的时候需要注意最终目标需要达到的标准是什么。常用的有MSE（公式4.1）、RMSE（公式4.2）、$R^2$（公式4.3）。在看公式前先说明TSS和RSS的概念，用在算$R^2$上。TSS(Total SUm of Squares)：总平方和TTS，样本之间的差异性。是伪方差的m倍。RSS：是预测值和样本值之间的差异，是MSE的m倍。$$MSE=\frac{1}{m}\sum_{i=1}^m(y_i - \hat y_i)^2\ \ \ 公式4.1$$$$RMSE=\sqrt{MSE} \ \ \ 公式4.2$$$$R^2 = 1 - \frac{RSS}{TSS} = 1-\frac{\sum_{i=1}^m(y_i-\hat y_i)^2}{\sum_{i=1}^m(y_i-\overline y)^2}\ \ \ 公式4.3$$				通过看公式我们可以得到以下的结论。MSE：误差平方和，越趋近于0越拟合。RMSE：就是对MSE开根号，所以和MSE的判别一样。$R^2$：值域为，最优解是1，若预测值恒为样本期望则为0除了这些，还有一个叫做混淆矩阵的东西，也是评价模型的一种手段，可以之后搜索看看。结语这些算法算是机器学习早期的故事，我们可以看到，算法的出现非常符合人类思维发展模式。算法是由具象到抽象的，开始的时候，说到分类我们很容易想到一刀切的方式，于是出现了线性回归，与此同时我们又想出了用概率表示的方式于是出现了逻辑回归。当香农提出了信息熵之后，又出现了信息熵来表示分类是否合适的算法思路。算法也是又简单到复杂的，之后随着线性回归算法的使用，我们发现出现了过拟合，于是我们用正则项来解决，于是出现了LASSO和Ridge算法。之后为了让这个“一刀切”更好，出现了SVM。而概率的那条路就走的更远一点，在考虑了特征之间的关系后，出现了贝叶斯网络，更进一步在考虑到了隐变量之后出现了HMM，慢慢的也能看见现在的神经网络的雏形。最后，个人觉得算法是一种工具，如果不能灵活使用的话，充其量只能算一种思辨游戏。祝大家HappyCoding。]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[线性回归算法]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/07/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%AE%97%E6%B3%95/</url>
      <content type="text"><![CDATA[							回归算法上	​在序章中我们提到了，机器学习的本质就是一个分类器，对给出的数据进行有价值的分类。​具体的机器学习算法的分类分为，监督学习和无监督学习两种。而在监督渡学习中，我们以分类的类别是否是离散的，分为两种分类方式，分别是分类和回归。即，分类后是有一定的，像水果的分类，苹果，梨，橘子等等这样确定的分类的是分类，而分类后的预测结果是一个连续的数值则是回归。在这篇文章中，我们说的回归算法便是监督算法中的回归算法。最小二乘参数&amp;损失函数​还记得我们最开始说的，机器学习的实质就是分类么，即得到输入$x$后，通过一个训练好的关系$f(x)$，输出一个$\hat y$ ，如果你的模型好的话，$\hat y$ 和真实值$y$会非常接近，有着非常好的预测结果。在这一个章节中，将会从数学层面来讲一讲这个是如何做到的。首先我们用大白话，整体的了解下这是一个什么样子过程。首先，我们需要先确定个目标，打比方说我要做一锅好吃的菜，我刚开始拿到一个方子，买来了菜切好了菜，按照方子做了，即选了算法，拿到训练数据后，机器学习第一次迭代。之后，尝了一尝，觉得这个菜不和我胃口，即和已有数据标签对比，然后调整盐量，菜的切法之后，即发现结果不理想，以某种方式调整参数，再做一次，做完之后再尝，再调整，即再次迭代，最终研究出了自己喜欢的菜适合自己的配方，即符合要求的准确率停止训练。​看这个过程是不是简单的说就是，不断的调整方法，直到完成目标任务，就和小婴儿学习一样。那么过程现在知道了，那如何让我们的小婴儿计算机来学习呢。首先，我们想，最简单的方程是什么呢，很小我们就知道了$y = kx + b$ 这个直线公式，我们现在就用它来作为输入$x$和输出$y$之间的关系，所以引入下面的公式2.1。我是这么理解的，每一个输入 $x$ 都对输出 $y$ 有不同的作用效果，所以我们给输入 $x$ 前面写一个系数来代表它的作用强度，这个系数就是$k$ ，也就是下面公式中的 $\theta$ ， 那么后面的$\epsilon$ 是什么呢。我的理解是，你看，我们现在是做一个$y$ 的预测，从实际出发，我们很难找到完美的$\hat y$ 可以完全等于训练集中$x$ 所对应的 $y$ ，这时候误差是必定存在的，所以引入误差$\epsilon$，为我们通过模型预测的值$\hat y$ 和已知值$y$的差，如公式2.2所示。除此之外，我们可以把$\epsilon$ 看作现实中众多随机现象引起的误差，而这个误差一般符合高斯分布（参考中心极限定理），这个在之后的推导中也会用到。实际问题中，很多随机现象可以看做众多因素的独立影响的综合反应，往往服从正态分布。​$$y^{(i)} = \theta^T*x^{(i)}\ \ \ 公式2.1$$$$y^{(i)}- \theta^T*x^{(i)}= \epsilon^{(i)}\ \ \ 公式2.2$$​误差、误差，顾名思义，我们要让误差最小，由于$\epsilon$符合高斯分布（最大似然估计），所以$\epsilon$ 的概率应该符合公式2.3。然后用公式2.2进行等式代换得到公式2.4。那么这个公式2.4怎么理解呢，这里输入$x$是固定的，我们只能调整$\theta$来使输出$\hat y$来接近样本给出的$y$。而公式2.4的意思就是，在输入$x$的情况下，输出正确$y$的概率。$$p(\epsilon^{(i)}) = \frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(\epsilon)^2}{2 \sigma^2}}\ \ \ 公式2.3$$$$p(y^{(i)}|x^{(i)};\theta) =\frac{1}{\sigma \sqrt{2\pi}}e^{-\frac{(y^{(i)}- \theta^T*x^{(i)})^2}{2 \sigma^2}}\ \ \ 公式2.4$$	​接下来我们要引入最大似然估计，小伙伴你们没有听错，就是考研数学中概率论的那个最大似然估计（居然真的用上了）。由于每个输入都是相互独立的，所以整体预测对的概率密度，可以通过每个样本预测对的概率密度相乘得到。于是我们得到了下面的公式2.5。是不是很熟悉，我们现在做的是改变$theta$使得概率最大，就和考研的时候学的一样，我们对公式2.5先求对数，然后求导就可以算出来我们要的$theta$的。$$L(\theta) = \prod_{i = 1}^mp(y^{(i)}|x^{(i)};\theta)\ \ \ 公式2.5$$​在这里有一点小小的变化，为了方便计算，我们对公式2.5求对数以后，我们得到了公式2.6。观察公式2.6我们很快的发现。$m \log \frac{1}{\sigma \sqrt{2\pi}}$和$m \log \frac{1}{\sigma \sqrt{2\pi}}$都是常数，和$\theta$相关的只有后面那部分。所以我们求导光求导后面的那块就好了。然后就可以得到一个$\theta$和输入和输出的关系公式2.7，机器就是这么求出最优解的，但是矩阵的求逆是很复杂的，这里会耗费大量的计算量。$$\ell(\theta) = m \log \frac{1}{\sigma \sqrt{2\pi}} - \frac{1}{\sigma^2}*\frac{1}{2}\sum^m_{i =1}(y^{(i)}-\theta ^{T}x^{(i)})^2 \ \ \ 公式2.6$$$$\theta = (X^TX)^{-1}X^TY \ \ \ 公式2.7$$​最后，我们在公式2.6中看到一个很有意思的公式，这个只要是了解过损失函数的人都是眼熟的。这个公式就是$-\frac{1}{2}\sum^m_{i =1}(y^{(i)}-\theta ^{T}x^{(i)})^2$，回想下损失函数的作用是什么，是一个求$\theta$的一个关键函数，它代表着预测的是不是准确，然后根据这个函数来调整我们通过输入$x$生成$y$的函数$h(x)$。所以我们可以写出如下公式2.8，而这个正是常用损失函数中的平方和损失函数。$$loss = J(\theta) = \frac{1}{2}\sum^m_{i =1}(h_{\theta}(x^{(i)})-y^{(i)})^2 \ \ \ 公式2.8$$​实际中我们有很多损失函数，如0-1损失函数，感知损失函数，平方和损失函数，绝对值损失函数，对数损失函数。3、多项式扩展​现在我们对整体的流程也有了了解，这里讨论下，在之前讨论中输入输出关系$f(x)$的一些遗留问题。之前我们说我们的目的就是找到这么一个$f(x)$，但是这个关系不会凭空出现，所以我们预测了一个关系$h(x)$，在预测的时候我们给每个样本属性$x$前都乘上了一个系数$\theta$，但是这么做有一个前提就是，这些属性之间是相互没有关联的，而这恰恰与实际完全不符。解决这个问题的正是，对函数$h(x)$进行多项式扩展，多项式扩展后出现了属性之间相乘的形式，自然就表示了属性间的相关性，预测的准确率也就大大提升了。4、什么是过拟合&amp;过拟合问题的解决​在我的理解中，即便是机器学习也是按照人指的方向进行数学问题的求解操作。在这个机器学习中，我们一定会找到一个通过样本来看预测效果非常非常好的结果，但是这个效果真的好么。如下图，我们可以看到，每一个样本都是符合的，但是很显然这个曲线是不对的。​这是什么原因导致的呢，在通过输出$\theta​$值，我们发现，这是由于某些$\theta​$值过大导致的。所以我们由此可以想到解决方案就是，用一个添加项来迫使$theta​$不至于过大。在这里我们引入正则项（norm），即L1-norm，L2-norm。通过公式我们可以看出，我们要让损失函数很小，加入了正则想项后，势必$\theta​$不会变得太大。​L2-norm：$J(\theta) = \frac{1}{2}\sum^m_{i=1}(h_{\theta}(x^{i}) - y^{(i)})^2 + \lambda\sum^n_{j=1} \theta_j^2 $$\ \ \ \lambda > 0$L1-norm：$J(\theta) = \frac{1}{2}\sum^m_{i=1}(h_{\theta}(x^{i}) - y^{(i)})^2 + \lambda\sum^n_{j=1} |\theta_j| \ \ \ \lambda > 0$​当我们使用L2-norm的线性回归模型就是Ridge回归（岭回归模型），而我们使用了L1-norm 的模型则是LASSO回归。接下来我们分析下这两个的性能问题。​L2-norm中，由于对于各个维度的参数缩放是在一个圆内缩放的，几乎不可能导致有维度参数变为0的情况，那么也就不会产生稀疏解；而L1-norm是在一个方形内的，则很容易产生稀疏解。实际应用中，数据的维度中是存在噪音和冗余的，稀疏的解可以找到有用的维度并且减少冗余，提高回归预测的准确性和鲁棒性（减少了过度拟合），而L1-norm则可以达到最终解的稀疏性的要求。​所以，Ridge模型有较高的准确性和鲁棒性，而LASSO模型更快，更能晒出稀疏解。那么如果我们两个属性都要兼备怎么办呢。接下来就引入了弹性网络ElasitcNet算法。其实就是很暴力的同时引入了L1-norm和L2-norm，然后用$p$来代表哪个多点，具体公式如下。$$J(\theta) = \frac{1}{2}\sum^m_{i=1}(h_{\theta}(x^{i} - y^{(i)}))^2 + \lambda(p\sum^n_{j=1} \theta_j^2 +(1-p)\sum^n_{j=1} |\theta_j| )\ \ \ \lambda > 0 \&\& p \in [0,1]$$]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
    <entry>
      <title><![CDATA[机器学习序章]]></title>
      <url>/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/2018/08/05/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%BA%8F%E7%AB%A0/</url>
      <content type="text"><![CDATA[							机器学习系列（序章）	序言机器学习&amp;人工智能&amp;深度学习，这三个是现在经常听到的词语。一旦提到了这些都会给人一种高大上的感觉，感觉会是一种很难学会的技术。表示在下血本（突然脑抽）的情况下，剁手买了1w多的数据挖掘的网课，目前正在学习它，希望能在学习完成后揭开机器学习的面纱，争取让每个读我的博客的人都能对机器学习有一个较为全面的概念。目前的更新顺序为课程的顺序，在整体学完之后，会按自己的理解进行一个汇总。什么是机器学习首先先上官方的卡内基梅隆大学的教授TomMitchell的定义。A program can be said to learn from experience E with respect to some class of tasks T and performance measure P , If its performance at tasks in T, as measured by P, improves with experience E.对于某给定的任务T，在合理的性能度量方案P的前提下，某计算机程序可以自主学习任务T的经验E；随着提供合适、优质、大量的经验E，该程序对于任务T的性能逐步提高。看起来很官方的说法对吧，接下来粗略的说明是怎么回事。一句话版本：抓了一把混着豆子的米（数据），根据你对豆子和米的特征的认识（已有经验），把豆子和米分开分别装在两个袋子里（分类），随后验收的人看你是否真的把米和豆子分开了（性能度量）。数学版本：X*P=Y，Y是分类的类别，X是一个数据，我们找的是矩阵P能使所有的数据X都能对应到相迎的分类Y。简单来说，机器学习就是分类器，通过学习已有的数据，得到一个数据和类别的关系，再用这个关系来对未来未分类的数据进行预测，这就是我理解的机器学习。 机器学习&amp;人工智能&amp;深度学习有什么区别说到这里，有些学过深度学习的人肯定就会疑惑这个和机器学习好像一样啊，深度学习也是把图片分类啊。是的，深度学习准确的来说算是机器学习的一部分，而机器学习和深度学习又可以被人工智能所包含。只不过深度学习在图像识别和语音识别的方便有着突出的优势，而机器学习在数据挖掘，统计学习和自然语言处理方面已经有了很大的发展。它的工作流程是什么样子数据收集=&gt;数据预处理(数据清洗)=&gt;特征提取=&gt;模型构建=&gt;模型测试评估=&gt;上线=&gt;迭代数据收集和数据清洗：可以理解为，做饭前的买菜（为模型提供训练用的有效数据，去除显而易见的无效数据）特征提取：可以理解为，炒菜前的切菜，切的越好，炒完越好吃（即从数据中选出可能能代表数据特征的属性）模型构建：可以理解为炒菜，用切好的菜，以一定的顺序进行翻炒（选择合适的算法来训练模型）。模型测试评估：试吃，如果不好吃，则反思是不是切的不好，菜买的不对，或者炒的顺序不对（测试用例看是否符合标准，如果不对责重复前面的步骤）。特别的说，训练的部分，其实就是以当前的权值运算出来的结果和已知结果对比，然后根据差距来修改权值，如此往复，使预测结果和已知结果无限接近。算法的分类和选择机器学习分为如下几个分类：	有监督学习：也就是训练用的数据是有标签的，在训练前是人工分好类的。再用训练过后的模型，对未来收到的数据进行分类，来达到预测的目的。	无监督学习：和有监督学习相比，训练的数据是没有分类的，在无监督学习中，就是通过学习，把这些为分类的数据进行分类，来推断出数据的一些内在结构。	半监督学习：训练的数据包含少量的含有标签的数据，通过这些数据来训练和分类。顾名思义就是无监督和有监督的结合。然后从算法的角度来看，又可以分为如下三种：	分类：标签是整形的，是一个一个独立的离散的。分类标识的时候使用int型。	回归：标签是浮点型，分类是连续的而不是离散的。分类表示用float的型。	聚类：1，2都是有监督学习，而3则是无监督学习。最后附上一个算法的选择图：（图很清楚只需要一点的英文水平就能看懂）classification:分类 regression:回归 clustering:聚类 dimensionality reduction：降维度公开数据获取渠道	UCI Machine Learning Repository （新手推荐这个，有标签）	Amazon Public-datasets	Kaggle	KDnuggets	Sougou语料库	天池	DC竞赛]]></content>
      <categories>
        
          <category> 机器学习 </category>
        
      </categories>
      <tags>
        
          <tag> MachineLearning </tag>
        
      </tags>
      <tags></tags>
    </entry>
  
</search>
